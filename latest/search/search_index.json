{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Tutorials</p><p>In-depth tutorials on using nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. See the user guides (TODO: Add guides) at the bottom of this page for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark()\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, we report the resulting accuracy metric by printing it to the terminal in a table.</p> <p><pre><code>from nnbench import runner\nfrom training import model, X_test, y_test\n\n\nr = runner.BenchmarkRunner()\n\ny_pred = model.predict(X_test)\n\n# To collect in the current file, pass \"__main__\" as module name.\nresult = r.run(\"__main__\", params={\"model\": model, \"y_pred\": y_pred, \"y_test\": y_test})\n\nr.report(to='console', result=result)\n</code></pre> The resulting output might look like this:</p> <pre><code>python benchmarks.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>context</li> <li>core</li> <li>reporter</li> <li>runner</li> <li>types</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/#nnbench.add_reporters","title":"add_reporters","text":"<pre><code>add_reporters()\n</code></pre> Source code in <code>src/nnbench/__init__.py</code> <pre><code>def add_reporters():\n    eps = entry_points()\n\n    if hasattr(eps, \"select\"):  # Python 3.10+ / importlib.metadata &gt;= 3.9.0\n        reporters = eps.select(group=\"nnbench.reporters\")\n    else:\n        reporters = eps.get(\"nnbench.reporters\", [])  # type: ignore\n\n    for rep in reporters:\n        key, clsname = rep.name.split(\"=\", 1)\n        register_reporter(key, clsname)\n</code></pre>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], dict[str, Any]]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/core.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    func: Callable[..., Any] | None = None,\n    parameters: Iterable[dict[str, Any]] = (),\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; list[Benchmark] | Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> DEFAULT: <code>()</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>list[Benchmark] | Callable[[Callable], list[Benchmark]]</code> <p>The resulting benchmark family (if no arguments were given), or a parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    func: Callable[..., Any] | None = None,\n    parameters: Iterable[dict[str, Any]] = (),\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; list[Benchmark] | Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    list[Benchmark] | Callable[[Callable], list[Benchmark]]\n        The resulting benchmark family (if no arguments were given), or a parametrized decorator\n        returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        for params in parameters:\n            _check_against_interface(params, fn)\n            name = fn.__name__ + \"_\" + \"_\".join(f\"{k}={v}\" for k, v in params.items())\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    func: Callable[..., Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; list[Benchmark] | Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[Benchmark] | Callable[[Callable], list[Benchmark]]</code> <p>The resulting benchmark family (if no arguments were given), or a parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    func: Callable[..., Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; list[Benchmark] | Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    list[Benchmark] | Callable[[Callable], list[Benchmark]]\n        The resulting benchmark family (if no arguments were given), or a parametrized decorator\n        returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        varnames = iterables.keys()\n        for values in itertools.product(*iterables.values()):\n            d = dict(zip(varnames, values))\n            _check_against_interface(d, fn)\n            name = fn.__name__ + \"_\" + \"_\".join(f\"{k}={v}\" for k, v in d.items())\n            wrapper = update_wrapper(partial(fn, **d), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.reporter_registry","title":"reporter_registry  <code>module-attribute</code>","text":"<pre><code>reporter_registry: MappingProxyType[str, type[BaseReporter]] = MappingProxyType(_reporter_registry)\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BaseReporter","title":"BaseReporter","text":"<p>The base interface for a benchmark reporter class.</p> <p>A benchmark reporter consumes benchmark results from a run, and subsequently reports them in the way specified by the respective implementation's <code>report()</code> method.</p> <p>For example, to write benchmark results to a database, you could save the credentials for authentication in the class constructor, and then stream the results directly to the database in <code>report()</code>, with preprocessing if necessary.</p> PARAMETER  DESCRIPTION <code>**kwargs</code> <p>Additional keyword arguments, for compatibility with subclass interfaces.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>class BaseReporter:\n    \"\"\"\n    The base interface for a benchmark reporter class.\n\n    A benchmark reporter consumes benchmark results from a run, and subsequently\n    reports them in the way specified by the respective implementation's `report()`\n    method.\n\n    For example, to write benchmark results to a database, you could save the credentials\n    for authentication in the class constructor, and then stream the results directly to\n    the database in `report()`, with preprocessing if necessary.\n\n    Parameters\n    ----------\n    **kwargs: Any\n        Additional keyword arguments, for compatibility with subclass interfaces.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any):\n        pass\n\n    def report(self, result: BenchmarkResult) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BaseReporter.report","title":"report","text":"<pre><code>report(result: BenchmarkResult) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def report(self, result: BenchmarkResult) -&gt; None:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.ConsoleReporter","title":"ConsoleReporter","text":"<p>             Bases: <code>BaseReporter</code></p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>class ConsoleReporter(BaseReporter):\n    # TODO: Implement regex filters, context values, display options, ... (__init__)\n    def report(self, result: BenchmarkResult) -&gt; None:\n        try:\n            from tabulate import tabulate\n        except ModuleNotFoundError:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires `tabulate` to be installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n            )\n\n        benchmarks = result[\"benchmarks\"]\n        print(tabulate(benchmarks, headers=\"keys\"))\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.ConsoleReporter.report","title":"report","text":"<pre><code>report(result: BenchmarkResult) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def report(self, result: BenchmarkResult) -&gt; None:\n    try:\n        from tabulate import tabulate\n    except ModuleNotFoundError:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires `tabulate` to be installed. \"\n            f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n        )\n\n    benchmarks = result[\"benchmarks\"]\n    print(tabulate(benchmarks, headers=\"keys\"))\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.register_reporter","title":"register_reporter","text":"<pre><code>register_reporter(key: str, cls_or_name: str | type[BaseReporter]) -&gt; None\n</code></pre> <p>Register a reporter class by its fully qualified module path.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to register the reporter under. Subsequently, this key can be used in place of reporter classes in code.</p> <p> TYPE: <code>str</code> </p> <code>cls_or_name</code> <p>Name of or full module path to the reporter class. For example, when registering a class <code>MyReporter</code> located in <code>my_module</code>, <code>name</code> should be <code>my_module.MyReporter</code>.</p> <p> TYPE: <code>str | type[BaseReporter]</code> </p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def register_reporter(key: str, cls_or_name: str | type[BaseReporter]) -&gt; None:\n    \"\"\"\n    Register a reporter class by its fully qualified module path.\n\n    Parameters\n    ----------\n    key: str\n        The key to register the reporter under. Subsequently, this key can be used in place\n        of reporter classes in code.\n    cls_or_name: str | type[BaseReporter]\n        Name of or full module path to the reporter class. For example, when registering a class\n        ``MyReporter`` located in ``my_module``, ``name`` should be ``my_module.MyReporter``.\n    \"\"\"\n\n    if isinstance(cls_or_name, str):\n        name = cls_or_name\n        modname, clsname = name.rsplit(\".\", 1)\n        mod = importlib.import_module(modname)\n        cls = getattr(mod, clsname)\n        _reporter_registry[key] = cls\n    else:\n        # name = cls_or_name.__module__ + \".\" + cls_or_name.__qualname__\n        _reporter_registry[key] = cls_or_name\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner","title":"BenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"An abstract benchmark runner class.\"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self):\n        self.benchmarks: list[Benchmark] = list()\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        if ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            ppath = Path(path_or_module)\n            if ppath.is_dir():\n                pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n                for py in pythonpaths:\n                    logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                    self.collect(py, tags)\n                return\n            elif ppath.is_file():\n                module = import_file_as_module(path_or_module)\n            else:\n                raise ValueError(\n                    f\"expected a module name, Python file, or directory, \"\n                    f\"got {str(path_or_module)!r}\"\n                )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if isdunder(k):\n                continue\n            elif isinstance(v, self.benchmark_type):\n                if not tags or set(tags) &amp; set(v.tags):\n                    self.benchmarks.append(v)\n            elif iscontainer(v):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        if not tags or set(tags) &amp; set(bm.tags):\n                            self.benchmarks.append(bm)\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str],\n        params: dict[str, Any] | Params,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n    ) -&gt; BenchmarkResult | None:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | Params\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n\n        Returns\n        -------\n        BenchmarkResult | None\n            A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n            holding the context information, and \"benchmarks\", holding an array with the\n            benchmark results.\n\n        Raises\n        ------\n        ValueError\n            If any context key-value pair is provided more than once.\n        \"\"\"\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        # if we still have no benchmarks after collection, warn and return early.\n        if not self.benchmarks:\n            logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n            return None  # TODO: Return empty result to preserve strong typing\n\n        if isinstance(params, Params):\n            dparams = asdict(params)\n        else:\n            dparams = params\n\n        _check(dparams, self.benchmarks)\n\n        ctx: dict[str, Any] = dict()\n        ctxkeys = set(ctx.keys())\n\n        for provider in context:\n            ctxval = provider()\n            valkeys = set(ctxval.keys())\n            # we do not allow multiple values for a context key.\n            duplicates = ctxkeys &amp; valkeys\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx |= ctxval\n            ctxkeys |= valkeys\n\n        results: list[dict[str, Any]] = []\n        for benchmark in self.benchmarks:\n            bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n            res: dict[str, Any] = {}\n            try:\n                benchmark.setUp(**bmparams)\n                # Todo: check params\n                res[\"name\"] = benchmark.name\n                res[\"value\"] = benchmark.fn(**bmparams)\n            except Exception as e:\n                # TODO: This needs work\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(**bmparams)\n                results.append(res)\n\n        return BenchmarkResult(\n            context=ctx,\n            benchmarks=results,\n        )\n\n    def report(\n        self, to: str | BaseReporter | Sequence[str | BaseReporter], result: BenchmarkResult\n    ) -&gt; None:\n        \"\"\"\n        Report collected results from a previous run.\n\n        Parameters\n        ----------\n        to: str | BaseReporter | Sequence[str | BaseReporter]\n            The reporter to use when reporting / streaming results. Can be either a string\n            (which prompts a lookup of all nnbench native reporters), a reporter instance,\n            or a sequence thereof, which enables streaming result data to multiple sinks.\n        result: BenchmarkResult\n            The benchmark result to report.\n        \"\"\"\n\n        def load_reporter(r: str | BaseReporter) -&gt; BaseReporter:\n            if isinstance(r, str):\n                try:\n                    return reporter_registry[r]()\n                except KeyError:\n                    # TODO: Add a note on nnbench reporter entry point once supported\n                    raise KeyError(f\"unknown reporter class {r!r}\")\n            else:\n                return r\n\n        dests: tuple[BaseReporter, ...] = ()\n\n        if isinstance(to, (str, BaseReporter)):\n            dests += (load_reporter(to),)\n        else:\n            dests += tuple(load_reporter(t) for t in to)\n\n        for reporter in dests:\n            reporter.report(result)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    if ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py, tags)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if isdunder(k):\n            continue\n        elif isinstance(v, self.benchmark_type):\n            if not tags or set(tags) &amp; set(v.tags):\n                self.benchmarks.append(v)\n        elif iscontainer(v):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        self.benchmarks.append(bm)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str],\n    params: dict[str, Any] | Params,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkResult | None\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Params</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>BenchmarkResult | None</code> <p>A JSON output representing the benchmark results. Has two top-level keys, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If any context key-value pair is provided more than once.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str],\n    params: dict[str, Any] | Params,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkResult | None:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | Params\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n\n    Returns\n    -------\n    BenchmarkResult | None\n        A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n        holding the context information, and \"benchmarks\", holding an array with the\n        benchmark results.\n\n    Raises\n    ------\n    ValueError\n        If any context key-value pair is provided more than once.\n    \"\"\"\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    # if we still have no benchmarks after collection, warn and return early.\n    if not self.benchmarks:\n        logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n        return None  # TODO: Return empty result to preserve strong typing\n\n    if isinstance(params, Params):\n        dparams = asdict(params)\n    else:\n        dparams = params\n\n    _check(dparams, self.benchmarks)\n\n    ctx: dict[str, Any] = dict()\n    ctxkeys = set(ctx.keys())\n\n    for provider in context:\n        ctxval = provider()\n        valkeys = set(ctxval.keys())\n        # we do not allow multiple values for a context key.\n        duplicates = ctxkeys &amp; valkeys\n        if duplicates:\n            dupe, *_ = duplicates\n            raise ValueError(f\"got multiple values for context key {dupe!r}\")\n        ctx |= ctxval\n        ctxkeys |= valkeys\n\n    results: list[dict[str, Any]] = []\n    for benchmark in self.benchmarks:\n        bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n        res: dict[str, Any] = {}\n        try:\n            benchmark.setUp(**bmparams)\n            # Todo: check params\n            res[\"name\"] = benchmark.name\n            res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            # TODO: This needs work\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(**bmparams)\n            results.append(res)\n\n    return BenchmarkResult(\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.report","title":"report","text":"<pre><code>report(to: str | BaseReporter | Sequence[str | BaseReporter], result: BenchmarkResult) -&gt; None\n</code></pre> <p>Report collected results from a previous run.</p> PARAMETER  DESCRIPTION <code>to</code> <p>The reporter to use when reporting / streaming results. Can be either a string (which prompts a lookup of all nnbench native reporters), a reporter instance, or a sequence thereof, which enables streaming result data to multiple sinks.</p> <p> TYPE: <code>str | BaseReporter | Sequence[str | BaseReporter]</code> </p> <code>result</code> <p>The benchmark result to report.</p> <p> TYPE: <code>BenchmarkResult</code> </p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def report(\n    self, to: str | BaseReporter | Sequence[str | BaseReporter], result: BenchmarkResult\n) -&gt; None:\n    \"\"\"\n    Report collected results from a previous run.\n\n    Parameters\n    ----------\n    to: str | BaseReporter | Sequence[str | BaseReporter]\n        The reporter to use when reporting / streaming results. Can be either a string\n        (which prompts a lookup of all nnbench native reporters), a reporter instance,\n        or a sequence thereof, which enables streaming result data to multiple sinks.\n    result: BenchmarkResult\n        The benchmark result to report.\n    \"\"\"\n\n    def load_reporter(r: str | BaseReporter) -&gt; BaseReporter:\n        if isinstance(r, str):\n            try:\n                return reporter_registry[r]()\n            except KeyError:\n                # TODO: Add a note on nnbench reporter entry point once supported\n                raise KeyError(f\"unknown reporter class {r!r}\")\n        else:\n            return r\n\n    dests: tuple[BaseReporter, ...] = ()\n\n    if isinstance(to, (str, BaseReporter)):\n        dests += (load_reporter(to),)\n    else:\n        dests += tuple(load_reporter(t) for t in to)\n\n    for reporter in dests:\n        reporter.report(result)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.iscontainer","title":"iscontainer","text":"<pre><code>iscontainer(s: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def iscontainer(s: Any) -&gt; bool:\n    return isinstance(s, (tuple, list))\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.isdunder","title":"isdunder","text":"<pre><code>isdunder(s: str) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def isdunder(s: str) -&gt; bool:\n    return s.startswith(\"__\") and s.endswith(\"__\")\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":"<p>Useful type interfaces to override/subclass in benchmarking workflows.</p>"},{"location":"reference/nnbench/types/#nnbench.types.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkResult","title":"BenchmarkResult","text":"<p>             Bases: <code>TypedDict</code></p> Source code in <code>src/nnbench/types.py</code> <pre><code>class BenchmarkResult(TypedDict):\n    context: dict[str, Any]\n    benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkResult.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkResult.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact","title":"Artifact","text":"<p>             Bases: <code>Generic[T]</code></p> <p>A base artifact class for loading (materializing) artifacts from disk or from remote storage.</p> <p>This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way. It is most useful when running models on already saved data or models, e.g. when comparing a newly trained model against a baseline in storage.</p> <p>Subclasses need to implement the <code>Artifact.materialize()</code> API, telling nnbench how to load the desired artifact from a path.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the artifact files.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>class Artifact(Generic[T]):\n    \"\"\"\n    A base artifact class for loading (materializing) artifacts from disk or from remote storage.\n\n    This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way.\n    It is most useful when running models on already saved data or models, e.g. when\n    comparing a newly trained model against a baseline in storage.\n\n    Subclasses need to implement the `Artifact.materialize()` API, telling nnbench how to\n    load the desired artifact from a path.\n\n    Parameters\n    ----------\n    path: str | os.PathLike[str]\n        Path to the artifact files.\n    \"\"\"\n\n    def __init__(self, path: str | os.PathLike[str]) -&gt; None:\n        # Save the path for later just-in-time materialization.\n        self.path = path\n        self._value: T | None = None\n\n    @classmethod\n    def materialize(cls) -&gt; \"Artifact\":\n        \"\"\"Load the artifact from storage.\"\"\"\n        raise NotImplementedError\n\n    def value(self) -&gt; T:\n        if self._value is None:\n            raise ValueError(\n                f\"artifact has not been instantiated yet, \"\n                f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n            )\n        return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = path\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.materialize","title":"materialize  <code>classmethod</code>","text":"<pre><code>materialize() -&gt; 'Artifact'\n</code></pre> <p>Load the artifact from storage.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef materialize(cls) -&gt; \"Artifact\":\n    \"\"\"Load the artifact from storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.value","title":"value","text":"<pre><code>value() -&gt; T\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def value(self) -&gt; T:\n    if self._value is None:\n        raise ValueError(\n            f\"artifact has not been instantiated yet, \"\n            f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n        )\n    return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Params","title":"Params  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Params:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>field(default=None)</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>field(repr=False, default=())</code> </p> <code>interface</code> <p>Interface of the benchmark function</p> <p> TYPE: <code>Interface</code> DEFAULT: <code>field(init=False, repr=False)</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    interface: Interface\n        Interface of the benchmark function\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str | None = field(default=None)\n    setUp: Callable[..., None] = field(repr=False, default=NoOp)\n    tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n    interface: Interface = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if not self.name:\n            name = self.fn.__name__\n\n            super().__setattr__(\"name\", name)\n            super().__setattr__(\"interface\", Interface.from_callable(self.fn))\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = field(default=None)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface. An instance of this class is created using the <code>from_callable</code> class method.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface--parameters","title":"Parameters:","text":"<p>names : tuple[str, ...]     Names of the function parameters. types : tuple[type, ...]     Types of the function parameters. defaults : tuple     A tuple of the function parameters' default values. variables : tuple[Variable, ...]     A tuple of tuples, where each inner tuple contains the parameter name and type. returntype: type     The function's return type annotation, or NoneType if left untyped.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface. An instance of this class\n    is created using the `from_callable` class method.\n\n    Parameters:\n    ----------\n    names : tuple[str, ...]\n        Names of the function parameters.\n    types : tuple[type, ...]\n        Types of the function parameters.\n    defaults : tuple\n        A tuple of the function parameters' default values.\n    variables : tuple[Variable, ...]\n        A tuple of tuples, where each inner tuple contains the parameter name and type.\n    returntype: type\n        The function's return type annotation, or NoneType if left untyped.\n    \"\"\"\n\n    names: tuple[str, ...]\n    types: tuple[type, ...]\n    defaults: tuple\n    variables: tuple[Variable, ...]\n    returntype: type\n\n    @classmethod\n    def from_callable(cls, fn: Callable) -&gt; Interface:\n        \"\"\"\n        Creates an interface instance from the given callable.\n        \"\"\"\n        sig = inspect.signature(fn)\n        ret = sig.return_annotation\n        return cls(\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(p.default for p in sig.parameters.values()),\n            tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable) -&gt; Interface\n</code></pre> <p>Creates an interface instance from the given callable.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable) -&gt; Interface:\n    \"\"\"\n    Creates an interface instance from the given callable.\n    \"\"\"\n    sig = inspect.signature(fn)\n    ret = sig.return_annotation\n    return cls(\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(p.default for p in sig.parameters.values()),\n        tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file)\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.with_suffix(\"\").as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: For absolute paths, the resulting module name will be horrifying\n    #  -&gt; find a sensible cutoff point (project root)\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>We aim to provide additional tutorials in the future - contributions are welcome!</p>"}]}