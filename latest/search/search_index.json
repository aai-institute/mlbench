{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Examples</p><p>Examples on how to use nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. See the user guides (TODO: Add guides) at the bottom of this page for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark()\ndef accuracy(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, using the <code>ConsoleReporter</code> we report the resulting accuracy metric by printing it to the terminal in a table.</p> <p><pre><code>from nnbench import runner\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = runner.BenchmarkRunner()\nreporter = ConsoleReporter()\n\n# To collect in the current file, pass \"__main__\" as module name.\nresult = r.run(\"__main__\", params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test})\n\nreporter.report(result)\n</code></pre> The resulting output might look like this:</p> <pre><code>python benchmarks.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"guides/benchmarks/","title":"Defining benchmarks with decorators","text":"<p>To benchmark your machine learning code in nnbench, define your key metrics in Python functions and apply one of the provided decorators. The available decorators are  - <code>@nnbench.benchmark</code>, which runs a benchmark with supplied parameters, - <code>@nnbench.parametrize</code>, which runs several benchmarks with the supplied parameter configurations, - <code>@nnbench.product</code>, which runs benchmarks with all parameter combinations that arise from the supplied values. </p> <p>First we introduce a small machine learning example which we will subsequently use to motivate the use of the three benchmark decorators.</p> <p>We recommend to split the model training, benchmark definition, and benchmark running into different files. In this guide, these are called <code>training.py</code>, <code>benchmarks.py</code>, and <code>main.py</code>.</p>"},{"location":"guides/benchmarks/#example","title":"Example","text":"<p>Let us consider an example where we want to evaluate a <code>scikit-learn</code> random forest classifier on the Iris dataset. For this purpose, we will define several helper functions inside a file, <code>training.py</code>. We use <code>prepare_data()</code>, to load the dataset,  <code>train_rf()</code> to train a random forest model with the specified parameters, and <code>accuracy()</code> to calculate the accuracy of the supplied model on the given dataset.</p> <pre><code># training.py\nimport numpy as np\nfrom sklearn import base, metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_data() -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    data = load_iris()\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    return X_train, X_test, y_train, y_test\n\n\ndef train_rf(X_train: np.ndarray, y_train: np.ndarray, n_estimators: int, max_depth: int, random_state: int = 42) -&gt; RandomForestClassifier:\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre>"},{"location":"guides/benchmarks/#benchmark-for-single-benchmarks","title":"<code>@benchmark</code> for single benchmarks","text":"<p>Now, we define our benchmarks in a new file called <code>benchmarks.py</code>. We first encapsulate the benchmark logic into a function, <code>benchmark_accuracy()</code> which prepares the data, trains a classifier, and lastly, obtains the accuracy. To mark such a function as a benchmark, we apply the <code>@benchmark</code> decorator.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.benchmark()\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Warning</p> <p>This training benchmark is designed as a local, simple, and self-contained example to showcase nnbench.  In a real world scenario, to follow best practices, you may want to separate the data preparation and model training steps from the benchmarking logic and pass the corresponding artifacts as a parameter to the benchmark. See the user guide for more information.</p> <p>Lastly, we set up a benchmark runner in the <code>main.py</code>. There, we supply the parameters (<code>n_estimators</code>, <code>max_depth</code>, <code>random_state</code>) necessary in the function definition as a dictionary to the <code>params</code> keyword argument. </p> <pre><code># main.py\nfrom nnbench import runner\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = runner.BenchmarkRunner()\nreporter = ConsoleReporter()\n\nresult = r.run(\"./benchmarks.py\", params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42})\nreporter.report(result)\n</code></pre> <p>When we execute the <code>main.py</code> we get the following output:</p> <pre><code>python main.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchparametrize-for-multiple-configuration-benchmarks","title":"<code>@nnbench.parametrize</code> for multiple configuration benchmarks","text":"<p>Sometimes, we are not only interested in the performance of a model for given parameters but want to compare the performance for different configurations.  To achieve this, we can turn our single accuracy benchmark in the <code>benchmarks.py</code> file into a parametrized benchmark. To do this, replace the decorator with <code>@nnbench.parametrize</code> and supply the parameter combinations of choice as dictionaries in the first argument.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.parametrize(\n    ({\"n_estimators\": 10, \"max_depth\": 2},\n    {\"n_estimators\": 50, \"max_depth\": 5},\n    {\"n_estimators\": 100, \"max_depth\": 10})\n)\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Notice that the parametrization is still incomplete, as we did not supply a <code>random_state</code> argument. The unfilled arguments are given in <code>BenchmarkRunner.run()</code> via a dictionary passed as the <code>params</code> keyword argument.</p> <pre><code># main.py\nfrom nnbench import runner\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = runner.BenchmarkRunner()\nreporter = ConsoleReporter()\n\nr = runner.BenchmarkRunner()\nresult = r.run(\"./benchmarks.py\", params={\"random_state\": 42})\nreporter.report(result)\n</code></pre> <p>Executing the parametrized benchmark, we get an output similar to this:</p> <pre><code>python main.py  \n\n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.955556\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.866667\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.911111\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchproduct-for-benchmarks-over-parameter-configuration-grids","title":"<code>@nnbench.product</code> for benchmarks over parameter configuration grids","text":"<p>In case we want to run a benchmark scan for all possible combinations of a set of parameters, we can use the <code>@nnbench.product</code> decorator to supply the different values for each parameter.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.product(n_estimators=[10, 50, 100], max_depth=[2, 5, 10])\ndef benchmark_accuracy_product(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>We still provide the <code>random_state</code> parameter to the runner directly, like we did with the <code>@nnbench.parametrize</code> decorator. By executing the benchmark, we get results for all combinations of <code>n_estimators</code> and <code>max_depth</code>. It looks similar to this:</p> <pre><code>python main.py  \n\n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=10_max_depth=5    0.955556\nbenchmark_accuracy_n_estimators=10_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=50_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.911111\nbenchmark_accuracy_n_estimators=50_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=100_max_depth=2   0.933333\nbenchmark_accuracy_n_estimators=100_max_depth=5   0.955556\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.955556\n</code></pre>"},{"location":"guides/organization/","title":"How to efficiently organize benchmark code","text":"<p>To efficiently organize benchmarks and keeping your setup modular, you can follow a few guidelines.</p>"},{"location":"guides/organization/#tip-1-separate-benchmarks-from-project-code","title":"Tip 1: Separate benchmarks from project code","text":"<p>This tip is well known from other software development practices such as unit testing. To improve project organization, consider splitting off your benchmarks into their own modules or even directories, if you have multiple benchmark workloads.</p> <p>An example project layout can look like this, with benchmarks as a separate directory at the top-level:</p> <pre><code>my-project/\n\u251c\u2500\u2500 benchmarks/ # &lt;- contains all benchmarking Python files.\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre> <p>This keeps the benchmarks neatly grouped together while siloing them away from the actual project code. Since you will most likely not run your benchmarks in a production setting, this is also advantageous for packaging, as the <code>benchmarks/</code> directory does not ship by default in this configuration.</p>"},{"location":"guides/organization/#tip-2-group-benchmarks-by-common-attributes","title":"Tip 2: Group benchmarks by common attributes","text":"<p>To maintain good organization within your benchmark directory, you can group similar benchmarks into their own Python files. As an example, if you have a set of benchmarks to establish data quality, and benchmarks for scoring trained models on curated data, you could structure them as follows:</p> <pre><code>benchmarks/\n\u251c\u2500\u2500 data_quality.py\n\u251c\u2500\u2500 model_perf.py\n\u2514\u2500\u2500 ...\n</code></pre> <p>This is helpful when running multiple benchmark workloads separately, as you can just point your benchmark runner to each of these separate files:</p> <pre><code>from nnbench.runner import BenchmarkRunner\n\nrunner = BenchmarkRunner()\ndata_metrics = runner.run(\"benchmarks/data_quality.py\", params=...)\n# same for model metrics, where instead you pass benchmarks/model_perf.py.\nmodel_metrics = runner.run(\"benchmarks/model_perf.py\", params=...)\n</code></pre>"},{"location":"guides/organization/#tip-3-attach-tags-to-benchmarks-for-selective-filtering","title":"Tip 3: Attach tags to benchmarks for selective filtering","text":"<p>For structuring benchmarks within files, you can also use tags, which are tuples of strings attached to a benchmark:</p> <pre><code># benchmarks/data_quality.py\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo1(data) -&gt; float:\n    ...\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo2(data) -&gt; int:\n    ...\n\n\n@nnbench.benchmark(tags=(\"bar\",))\ndef bar(data) -&gt; int:\n    ...\n</code></pre> <p>Now, to only run data quality benchmarks marked \"foo\", pass the corresponding tag to <code>BenchmarkRunner.run()</code>:</p> <pre><code>from nnbench.runner import BenchmarkRunner\n\nrunner = BenchmarkRunner()\nfoo_data_metrics = runner.run(\"benchmarks/data_quality.py\", params=..., tags=(\"foo\",))\n</code></pre> <p>Tip</p> <p>This concept works exactly the same when creating benchmarks with the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>context</li> <li>core</li> <li>reporter</li> <li>runner</li> <li>types</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/#nnbench.add_reporters","title":"add_reporters","text":"<pre><code>add_reporters()\n</code></pre> Source code in <code>src/nnbench/__init__.py</code> <pre><code>def add_reporters():\n    eps = entry_points()\n\n    if hasattr(eps, \"select\"):  # Python 3.10+ / importlib.metadata &gt;= 3.9.0\n        reporters = eps.select(group=\"nnbench.reporters\")\n    else:\n        reporters = eps.get(\"nnbench.reporters\", [])  # type: ignore\n\n    for rep in reporters:\n        key, clsname = rep.name.split(\"=\", 1)\n        register_reporter(key, clsname)\n</code></pre>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], dict[str, Any]]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/core.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>A display name to give to the benchmark. Useful in summaries and reports.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    name: str | None\n        A display name to give to the benchmark. Useful in summaries and reports.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        for params in parameters:\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        varnames = iterables.keys()\n        for values in itertools.product(*iterables.values()):\n            params = dict(zip(varnames, values))\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.reporter_registry","title":"reporter_registry  <code>module-attribute</code>","text":"<pre><code>reporter_registry: MappingProxyType[str, type[BenchmarkReporter]] = MappingProxyType(\n    _reporter_registry\n)\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BenchmarkReporter","title":"BenchmarkReporter","text":"<p>The base interface for a benchmark reporter class.</p> <p>A benchmark reporter consumes benchmark results from a previous run, and subsequently reports them in the way specified by the respective implementation's <code>report_result()</code> method.</p> <p>For example, to write benchmark results to a database, you could save the credentials for authentication on the class, and then stream the results directly to the database in <code>report_result()</code>, with preprocessing if necessary.</p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>class BenchmarkReporter:\n    \"\"\"\n    The base interface for a benchmark reporter class.\n\n    A benchmark reporter consumes benchmark results from a previous run, and subsequently\n    reports them in the way specified by the respective implementation's `report_result()`\n    method.\n\n    For example, to write benchmark results to a database, you could save the credentials\n    for authentication on the class, and then stream the results directly to\n    the database in `report_result()`, with preprocessing if necessary.\n    \"\"\"\n\n    merge: bool = False\n    \"\"\"Whether to merge multiple BenchmarkRecords before reporting.\"\"\"\n\n    def report_result(self, record: BenchmarkRecord) -&gt; None:\n        raise NotImplementedError\n\n    def report(self, *records: BenchmarkRecord) -&gt; None:\n        if self.merge:\n            raise NotImplementedError\n        for record in records:\n            self.report_result(record)\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BenchmarkReporter.merge","title":"merge  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>merge: bool = False\n</code></pre> <p>Whether to merge multiple BenchmarkRecords before reporting.</p>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BenchmarkReporter.report_result","title":"report_result","text":"<pre><code>report_result(record: BenchmarkRecord) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def report_result(self, record: BenchmarkRecord) -&gt; None:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.BenchmarkReporter.report","title":"report","text":"<pre><code>report(*records: BenchmarkRecord) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def report(self, *records: BenchmarkRecord) -&gt; None:\n    if self.merge:\n        raise NotImplementedError\n    for record in records:\n        self.report_result(record)\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.ConsoleReporter","title":"ConsoleReporter","text":"<p>             Bases: <code>BenchmarkReporter</code></p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>class ConsoleReporter(BenchmarkReporter):\n    # TODO: Implement regex filters, context values, display options, ... (__init__)\n    def report_result(self, record: BenchmarkRecord) -&gt; None:\n        try:\n            from tabulate import tabulate\n        except ModuleNotFoundError:\n            raise ValueError(\n                f\"{self.__class__.__name__} requires `tabulate` to be installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n            )\n\n        benchmarks = record[\"benchmarks\"]\n        print(tabulate(benchmarks, headers=\"keys\"))\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.ConsoleReporter.report_result","title":"report_result","text":"<pre><code>report_result(record: BenchmarkRecord) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def report_result(self, record: BenchmarkRecord) -&gt; None:\n    try:\n        from tabulate import tabulate\n    except ModuleNotFoundError:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires `tabulate` to be installed. \"\n            f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n        )\n\n    benchmarks = record[\"benchmarks\"]\n    print(tabulate(benchmarks, headers=\"keys\"))\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.register_reporter","title":"register_reporter","text":"<pre><code>register_reporter(key: str, cls_or_name: str | type[BenchmarkReporter]) -&gt; None\n</code></pre> <p>Register a reporter class by its fully qualified module path.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to register the reporter under. Subsequently, this key can be used in place of reporter classes in code.</p> <p> TYPE: <code>str</code> </p> <code>cls_or_name</code> <p>Name of or full module path to the reporter class. For example, when registering a class <code>MyReporter</code> located in <code>my_module</code>, <code>name</code> should be <code>my_module.MyReporter</code>.</p> <p> TYPE: <code>str | type[BenchmarkReporter]</code> </p> Source code in <code>src/nnbench/reporter.py</code> <pre><code>def register_reporter(key: str, cls_or_name: str | type[BenchmarkReporter]) -&gt; None:\n    \"\"\"\n    Register a reporter class by its fully qualified module path.\n\n    Parameters\n    ----------\n    key: str\n        The key to register the reporter under. Subsequently, this key can be used in place\n        of reporter classes in code.\n    cls_or_name: str | type[BenchmarkReporter]\n        Name of or full module path to the reporter class. For example, when registering a class\n        ``MyReporter`` located in ``my_module``, ``name`` should be ``my_module.MyReporter``.\n    \"\"\"\n\n    if isinstance(cls_or_name, str):\n        name = cls_or_name\n        modname, clsname = name.rsplit(\".\", 1)\n        mod = importlib.import_module(modname)\n        cls = getattr(mod, clsname)\n        _reporter_registry[key] = cls\n    else:\n        # name = cls_or_name.__module__ + \".\" + cls_or_name.__qualname__\n        _reporter_registry[key] = cls_or_name\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner","title":"BenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"An abstract benchmark runner class.\"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self):\n        self.benchmarks: list[Benchmark] = list()\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        if ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            ppath = Path(path_or_module)\n            if ppath.is_dir():\n                pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n                for py in pythonpaths:\n                    logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                    self.collect(py, tags)\n                return\n            elif ppath.is_file():\n                module = import_file_as_module(path_or_module)\n            else:\n                raise ValueError(\n                    f\"expected a module name, Python file, or directory, \"\n                    f\"got {str(path_or_module)!r}\"\n                )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if isdunder(k):\n                continue\n            elif isinstance(v, self.benchmark_type):\n                if not tags or set(tags) &amp; set(v.tags):\n                    self.benchmarks.append(v)\n            elif iscontainer(v):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        if not tags or set(tags) &amp; set(bm.tags):\n                            self.benchmarks.append(bm)\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str],\n        params: dict[str, Any] | Parameters,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n    ) -&gt; BenchmarkRecord | None:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | Parameters\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n\n        Returns\n        -------\n        BenchmarkRecord | None\n            A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n            holding the context information, and \"benchmarks\", holding an array with the\n            benchmark results.\n\n        Raises\n        ------\n        ValueError\n            If any context key-value pair is provided more than once.\n        \"\"\"\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        # if we still have no benchmarks after collection, warn and return early.\n        if not self.benchmarks:\n            logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n            return None  # TODO: Return empty result to preserve strong typing\n\n        if isinstance(params, Parameters):\n            dparams = asdict(params)\n        else:\n            dparams = params\n\n        _check(dparams, self.benchmarks)\n\n        ctx: dict[str, Any] = dict()\n        ctxkeys = set(ctx.keys())\n\n        for provider in context:\n            ctxval = provider()\n            valkeys = set(ctxval.keys())\n            # we do not allow multiple values for a context key.\n            duplicates = ctxkeys &amp; valkeys\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx |= ctxval\n            ctxkeys |= valkeys\n\n        results: list[dict[str, Any]] = []\n        for benchmark in self.benchmarks:\n            bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n            res: dict[str, Any] = {}\n            try:\n                benchmark.setUp(**bmparams)\n                # Todo: check params\n                res[\"name\"] = benchmark.name\n                res[\"value\"] = benchmark.fn(**bmparams)\n            except Exception as e:\n                # TODO: This needs work\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(**bmparams)\n                results.append(res)\n\n        return BenchmarkRecord(\n            context=ctx,\n            benchmarks=results,\n        )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    if ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py, tags)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if isdunder(k):\n            continue\n        elif isinstance(v, self.benchmark_type):\n            if not tags or set(tags) &amp; set(v.tags):\n                self.benchmarks.append(v)\n        elif iscontainer(v):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        self.benchmarks.append(bm)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str],\n    params: dict[str, Any] | Parameters,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord | None\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Parameters</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord | None</code> <p>A JSON output representing the benchmark results. Has two top-level keys, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If any context key-value pair is provided more than once.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str],\n    params: dict[str, Any] | Parameters,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord | None:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | Parameters\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n\n    Returns\n    -------\n    BenchmarkRecord | None\n        A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n        holding the context information, and \"benchmarks\", holding an array with the\n        benchmark results.\n\n    Raises\n    ------\n    ValueError\n        If any context key-value pair is provided more than once.\n    \"\"\"\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    # if we still have no benchmarks after collection, warn and return early.\n    if not self.benchmarks:\n        logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n        return None  # TODO: Return empty result to preserve strong typing\n\n    if isinstance(params, Parameters):\n        dparams = asdict(params)\n    else:\n        dparams = params\n\n    _check(dparams, self.benchmarks)\n\n    ctx: dict[str, Any] = dict()\n    ctxkeys = set(ctx.keys())\n\n    for provider in context:\n        ctxval = provider()\n        valkeys = set(ctxval.keys())\n        # we do not allow multiple values for a context key.\n        duplicates = ctxkeys &amp; valkeys\n        if duplicates:\n            dupe, *_ = duplicates\n            raise ValueError(f\"got multiple values for context key {dupe!r}\")\n        ctx |= ctxval\n        ctxkeys |= valkeys\n\n    results: list[dict[str, Any]] = []\n    for benchmark in self.benchmarks:\n        bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n        res: dict[str, Any] = {}\n        try:\n            benchmark.setUp(**bmparams)\n            # Todo: check params\n            res[\"name\"] = benchmark.name\n            res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            # TODO: This needs work\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(**bmparams)\n            results.append(res)\n\n    return BenchmarkRecord(\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.iscontainer","title":"iscontainer","text":"<pre><code>iscontainer(s: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def iscontainer(s: Any) -&gt; bool:\n    return isinstance(s, (tuple, list))\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.isdunder","title":"isdunder","text":"<pre><code>isdunder(s: str) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def isdunder(s: str) -&gt; bool:\n    return s.startswith(\"__\") and s.endswith(\"__\")\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":"<p>Useful type interfaces to override/subclass in benchmarking workflows.</p>"},{"location":"reference/nnbench/types/#nnbench.types.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord","title":"BenchmarkRecord","text":"<p>             Bases: <code>TypedDict</code></p> Source code in <code>src/nnbench/types.py</code> <pre><code>class BenchmarkRecord(TypedDict):\n    context: dict[str, Any]\n    benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact","title":"Artifact","text":"<p>             Bases: <code>Generic[T]</code></p> <p>A base artifact class for loading (materializing) artifacts from disk or from remote storage.</p> <p>This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way. It is most useful when running models on already saved data or models, e.g. when comparing a newly trained model against a baseline in storage.</p> <p>Subclasses need to implement the <code>Artifact.materialize()</code> API, telling nnbench how to load the desired artifact from a path.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the artifact files.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>class Artifact(Generic[T]):\n    \"\"\"\n    A base artifact class for loading (materializing) artifacts from disk or from remote storage.\n\n    This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way.\n    It is most useful when running models on already saved data or models, e.g. when\n    comparing a newly trained model against a baseline in storage.\n\n    Subclasses need to implement the `Artifact.materialize()` API, telling nnbench how to\n    load the desired artifact from a path.\n\n    Parameters\n    ----------\n    path: str | os.PathLike[str]\n        Path to the artifact files.\n    \"\"\"\n\n    def __init__(self, path: str | os.PathLike[str]) -&gt; None:\n        # Save the path for later just-in-time materialization.\n        self.path = path\n        self._value: T | None = None\n\n    @classmethod\n    def materialize(cls) -&gt; \"Artifact\":\n        \"\"\"Load the artifact from storage.\"\"\"\n        raise NotImplementedError\n\n    def value(self) -&gt; T:\n        if self._value is None:\n            raise ValueError(\n                f\"artifact has not been instantiated yet, \"\n                f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n            )\n        return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = path\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.materialize","title":"materialize  <code>classmethod</code>","text":"<pre><code>materialize() -&gt; 'Artifact'\n</code></pre> <p>Load the artifact from storage.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef materialize(cls) -&gt; \"Artifact\":\n    \"\"\"Load the artifact from storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.value","title":"value","text":"<pre><code>value() -&gt; T\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def value(self) -&gt; T:\n    if self._value is None:\n        raise ValueError(\n            f\"artifact has not been instantiated yet, \"\n            f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n        )\n    return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Parameters","title":"Parameters  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Parameters:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>field(default=None)</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>field(repr=False, default=())</code> </p> <code>interface</code> <p>Interface of the benchmark function</p> <p> TYPE: <code>Interface</code> DEFAULT: <code>field(init=False, repr=False)</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    interface: Interface\n        Interface of the benchmark function\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str | None = field(default=None)\n    setUp: Callable[..., None] = field(repr=False, default=NoOp)\n    tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n    interface: Interface = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if not self.name:\n            super().__setattr__(\"name\", self.fn.__name__)\n        super().__setattr__(\"interface\", Interface.from_callable(self.fn))\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = field(default=None)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface. An instance of this class is created using the <code>from_callable</code> class method.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface--parameters","title":"Parameters:","text":"<p>names : tuple[str, ...]     Names of the function parameters. types : tuple[type, ...]     Types of the function parameters. defaults : tuple     A tuple of the function parameters' default values. variables : tuple[Variable, ...]     A tuple of tuples, where each inner tuple contains the parameter name and type. returntype: type     The function's return type annotation, or NoneType if left untyped.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface. An instance of this class\n    is created using the `from_callable` class method.\n\n    Parameters:\n    ----------\n    names : tuple[str, ...]\n        Names of the function parameters.\n    types : tuple[type, ...]\n        Types of the function parameters.\n    defaults : tuple\n        A tuple of the function parameters' default values.\n    variables : tuple[Variable, ...]\n        A tuple of tuples, where each inner tuple contains the parameter name and type.\n    returntype: type\n        The function's return type annotation, or NoneType if left untyped.\n    \"\"\"\n\n    names: tuple[str, ...]\n    types: tuple[type, ...]\n    defaults: tuple\n    variables: tuple[Variable, ...]\n    returntype: type\n\n    @classmethod\n    def from_callable(cls, fn: Callable) -&gt; Interface:\n        \"\"\"\n        Creates an interface instance from the given callable.\n        \"\"\"\n        # Set follow_wrapped=False to get the partially filled interfaces.\n        # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n        sig = inspect.signature(fn, follow_wrapped=False)\n        ret = sig.return_annotation\n        return cls(\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(p.default for p in sig.parameters.values()),\n            tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable) -&gt; Interface\n</code></pre> <p>Creates an interface instance from the given callable.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable) -&gt; Interface:\n    \"\"\"\n    Creates an interface instance from the given callable.\n    \"\"\"\n    # Set follow_wrapped=False to get the partially filled interfaces.\n    # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n    sig = inspect.signature(fn, follow_wrapped=False)\n    ret = sig.return_annotation\n    return cls(\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(p.default for p in sig.parameters.values()),\n        tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file)\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.with_suffix(\"\").as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: For absolute paths, the resulting module name will be horrifying\n    #  -&gt; find a sensible cutoff point (project root)\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"tutorials/","title":"Examples","text":"<p>This page showcases some examples of applications for nnbench. Click any of the links below for inspiration on how to use nnbench in your projects.</p> <ul> <li>How to integrate nnbench into an existing ML pipeline</li> </ul>"},{"location":"tutorials/mnist/","title":"Integrating nnbench into an existing ML pipeline","text":"<p>Thanks to nnbench's modularity, we can easily integrate it into existing ML experiment code.</p> <p>As an example, we use an MNIST pipeline written for the popular ML framework JAX. While the actual data sourcing and training code is interesting on its own, we focus solely on the nnbench application part. You can find the full example code in the nnbench repository.</p>"},{"location":"tutorials/mnist/#defining-and-organizing-benchmarks","title":"Defining and organizing benchmarks","text":"<p>To properly structure our project, we avoid mixing training pipeline code and benchmark code by placing all benchmarks in a standalone file, similarly to how you might structure unit tests for your code.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom mnist import ArrayMapping, ConvNet\n\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(params: ArrayMapping, data: ArrayMapping) -&gt; float:\n    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n    cn = ConvNet()\n    y_pred = cn.apply({\"params\": params}, x_test)\n    return jnp.mean(jnp.argmax(y_pred, -1) == y_test).item()\n\n\n@nnbench.benchmark(name=\"Model size (MB)\")\ndef modelsize(params: ArrayMapping) -&gt; float:\n    nbytes = sum(x.size * x.dtype.itemsize for x in jax.tree_util.tree_leaves(params))\n    return nbytes / 1e6\n</code></pre> <p>This definition is short and sweet, and contains a few important details:</p> <ul> <li>Both functions are given the <code>@nnbench.benchmark</code> decorator - this enables our runner to find and collect them before starting the benchmark run.</li> <li>The <code>modelsize</code> benchmark is given a custom name (<code>\"Model size (MB)\"</code>), indicating that the resulting number is the combined size of the model weights in megabytes. This is done for display purposes, to improve interpretability when reporting results.</li> <li>The <code>params</code> argument is the same in both benchmarks, both in name and type. This is important, since it ensures that both benchmarks will be run with the same model weights.</li> </ul> <p>That's all - now we can shift over to our main pipeline code and see what is necessary to execute the benchmarks and visualize the results.</p>"},{"location":"tutorials/mnist/#setting-up-a-benchmark-runner-and-parameters","title":"Setting up a benchmark runner and parameters","text":"<p>After finishing the benchmark setup, we only need a few more lines to augment our pipeline.</p> <p>We assume that the benchmark file is located in the same folder as the training pipeline - thus, we can specify our parent directory as the place in which to search for benchmarks:</p> <p>Next, we can define a custom subclass of <code>nnbench.Parameters</code> to hold our benchmark parameters. Benchmark parameters are a set of variables used as inputs to the benchmark functions collected during the benchmark run.</p> <p>Since our benchmarks above are parametrized by the model weights (named <code>params</code> in the function signatures) and the MNIST data split (called <code>data</code>), we define our parameters to take exactly these two values.</p> <pre><code>@dataclass(frozen=True)\nclass MNISTTestParameters(nnbench.Parameters):\n    params: Mapping[str, jax.Array]\n</code></pre> <p>And that's it! After we implement all training code, we just run nnbench directly after training in our top-level pipeline function:</p> <pre><code>def mnist_jax():\n    \"\"\"Load MNIST data and train a simple ConvNet model.\"\"\"\n    mnist = load_mnist()\n    mnist = preprocess(mnist)\n    state, data = train(mnist)\n\n    # the nnbench portion.\n    runner = BenchmarkRunner()\n    reporter = ConsoleReporter()\n    params = MNISTTestParameters(params=state.params, data=data)\n</code></pre> <p>We use the <code>ConsoleReporter</code> to print the results directly to the terminal in a table. Notice how by we can reuse the training artifacts in nnbench as parameters to obtain results right after training!</p> <p>The output might look like this:</p> <pre><code>name               value\n---------------  -------\naccuracy         0.9712\nModel size (MB)  3.29783\n</code></pre> <p>This can be improved in a number of ways - for example by enriching it with metadata about the model architecture, the used GPU, etc. For more information on how to supply context to benchmarks, check the user guide section.</p>"}]}