{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Tutorials</p><p>In-depth tutorials on using nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p> <p>TODO: Add content.</p>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>context</li> <li>core</li> <li>runner</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextTuple","title":"ContextTuple  <code>module-attribute</code>","text":"<pre><code>ContextTuple = tuple[str, Any]\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], Union[ContextTuple, Sequence[ContextTuple]]]\n</code></pre> <p>A function providing a context value. Context tuple is structured as context key name and value.</p>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; tuple[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; tuple[str, str]:\n    return \"system\", platform.system()\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; tuple[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; tuple[str, str]:\n    return \"cpuarch\", platform.machine()\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; tuple[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; tuple[str, str]:\n    return \"python_version\", platform.python_version()\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.Params","title":"Params  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>@dataclass(init=False)\nclass Params:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>field(default=None)</code> </p> <code>params</code> <p>Fixed parameters to pass to the benchmark.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>field(repr=False, default_factory=dict)</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>field(repr=False, default=())</code> </p> Source code in <code>src/nnbench/core.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    params: dict[str, Any]\n        Fixed parameters to pass to the benchmark.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str | None = field(default=None)\n    params: dict[str, Any] = field(repr=False, default_factory=dict)\n    setUp: Callable[..., None] = field(repr=False, default=NoOp)\n    tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n\n    def __post_init__(self):\n        if not self.name:\n            name = self.fn.__name__\n            if self.params:\n                name += \"_\" + \"_\".join(f\"{k}={v}\" for k, v in self.params.items())\n\n            super().__setattr__(\"name\", name)\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = field(default=None)\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: dict[str, Any] = field(repr=False, default_factory=dict)\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/core.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    params: dict[str, Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>AbstractBenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The parameters (or a subset thereof) defining the benchmark.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A decorated callable yielding the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    params: dict[str, Any] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `AbstractBenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark.\n    params: dict[str, Any] | None\n        The parameters (or a subset thereof) defining the benchmark.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable\n        A decorated callable yielding the benchmark.\n    \"\"\"\n\n    # TODO: The above return typing is incorrect\n    #  (needs a func is None vs. func is not None overload)\n    def inner(fn: Callable) -&gt; Benchmark:\n        name = fn.__name__\n        if params:\n            name += \"_\" + \"_\".join(f\"{k}={v}\" for k, v in params.items())\n        return Benchmark(fn, name=name, params=params, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func:\n        return inner(func)  # type: ignore\n    else:\n        return inner\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    func: Callable[..., Any] | None = None,\n    parameters: Iterable[dict] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>AbstractBenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict] | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A decorated callable yielding the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    func: Callable[..., Any] | None = None,\n    parameters: Iterable[dict] | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `AbstractBenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark.\n    parameters: Iterable[dict] | None\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable\n        A decorated callable yielding the benchmark family.\n    \"\"\"\n\n    # TODO: The above return typing is incorrect\n    #  (needs a func is None vs. func is not None overload)\n    def inner(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        for params in parameters:\n            name = fn.__name__\n            if params:\n                name += \"_\" + \"_\".join(f\"{k}={v}\" for k, v in params.items())\n            bm = Benchmark(fn, name=name, params=params, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    if func:\n        return inner(func)  # type: ignore\n    else:\n        return inner\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkResult","title":"BenchmarkResult","text":"<p>             Bases: <code>TypedDict</code></p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkResult(TypedDict):\n    context: dict[str, Any]\n    benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkResult.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkResult.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner","title":"AbstractBenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class AbstractBenchmarkRunner:\n    \"\"\"An abstract benchmark runner class.\"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self):\n        self.benchmarks: list[Benchmark] = list()\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(\n        self, path_or_module: str | os.PathLike[str] = \"__main__\", tags: tuple[str, ...] = ()\n    ) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        if ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            ppath = Path(path_or_module)\n            if ppath.is_dir():\n                pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n                for py in pythonpaths:\n                    logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                    self.collect(py)\n                return\n            elif ppath.is_file():\n                module = import_file_as_module(path_or_module)\n            else:\n                raise ValueError(\n                    f\"expected a module name, Python file, or directory, \"\n                    f\"got {str(path_or_module)!r}\"\n                )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if isdunder(k):\n                continue\n            elif isinstance(v, self.benchmark_type):\n                self.benchmarks.append(v)\n            elif iscontainer(v):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        self.benchmarks.append(bm)\n\n        # and finally, filter by tags.\n        self.benchmarks = [b for b in self.benchmarks if set(tags) &lt;= set(b.tags)]\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str] = \"__main__\",\n        params: dict[str, Any] | None = None,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n    ) -&gt; BenchmarkResult:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | None\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n\n        Returns\n        -------\n        BenchmarkResult\n            A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n            holding the context information, and \"benchmarks\", holding an array with the\n            benchmark results.\n        \"\"\"\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        # if we still have no benchmarks after collection, warn.\n        if not self.benchmarks:\n            logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n\n        dcontext: dict[str, Any] = dict()\n\n        for provider in context:\n            ctxval = provider()\n            if isinstance(ctxval, tuple):\n                key, val = ctxval\n                dcontext[key] = val\n            else:\n                # multi-value context information.\n                for v in ctxval:\n                    key, val = v\n                    dcontext[key] = val\n\n        results: list[dict[str, Any]] = []\n        for benchmark in self.benchmarks:\n            res: dict[str, Any] = {}\n            # TODO: Validate against interface and pass only the kwargs relevant to the benchmark\n            params |= benchmark.params\n            try:\n                benchmark.setUp(**params)\n                res.update(benchmark.fn(**params))\n            except Exception as e:\n                # TODO: This needs work\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(**params)\n                results.append(res)\n\n        return BenchmarkResult(\n            context=dcontext,\n            benchmarks=results,\n        )\n\n    def report(self) -&gt; None:\n        \"\"\"Report collected results from a previous run.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str] = '__main__', tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> DEFAULT: <code>'__main__'</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(\n    self, path_or_module: str | os.PathLike[str] = \"__main__\", tags: tuple[str, ...] = ()\n) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    if ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if isdunder(k):\n            continue\n        elif isinstance(v, self.benchmark_type):\n            self.benchmarks.append(v)\n        elif iscontainer(v):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    self.benchmarks.append(bm)\n\n    # and finally, filter by tags.\n    self.benchmarks = [b for b in self.benchmarks if set(tags) &lt;= set(b.tags)]\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str] = \"__main__\",\n    params: dict[str, Any] | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkResult\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> DEFAULT: <code>'__main__'</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>BenchmarkResult</code> <p>A JSON output representing the benchmark results. Has two top-level keys, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str] = \"__main__\",\n    params: dict[str, Any] | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkResult:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | None\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n\n    Returns\n    -------\n    BenchmarkResult\n        A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n        holding the context information, and \"benchmarks\", holding an array with the\n        benchmark results.\n    \"\"\"\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    # if we still have no benchmarks after collection, warn.\n    if not self.benchmarks:\n        logger.warning(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n\n    dcontext: dict[str, Any] = dict()\n\n    for provider in context:\n        ctxval = provider()\n        if isinstance(ctxval, tuple):\n            key, val = ctxval\n            dcontext[key] = val\n        else:\n            # multi-value context information.\n            for v in ctxval:\n                key, val = v\n                dcontext[key] = val\n\n    results: list[dict[str, Any]] = []\n    for benchmark in self.benchmarks:\n        res: dict[str, Any] = {}\n        # TODO: Validate against interface and pass only the kwargs relevant to the benchmark\n        params |= benchmark.params\n        try:\n            benchmark.setUp(**params)\n            res.update(benchmark.fn(**params))\n        except Exception as e:\n            # TODO: This needs work\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(**params)\n            results.append(res)\n\n    return BenchmarkResult(\n        context=dcontext,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.AbstractBenchmarkRunner.report","title":"report","text":"<pre><code>report() -&gt; None\n</code></pre> <p>Report collected results from a previous run.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def report(self) -&gt; None:\n    \"\"\"Report collected results from a previous run.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.iscontainer","title":"iscontainer","text":"<pre><code>iscontainer(s: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def iscontainer(s: Any) -&gt; bool:\n    return isinstance(s, (tuple, list))\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.isdunder","title":"isdunder","text":"<pre><code>isdunder(s: str) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def isdunder(s: str) -&gt; bool:\n    return s.startswith(\"__\") and s.endswith(\"__\")\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file)\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.with_suffix(\"\").as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: For absolute paths, the resulting module name will be horrifying\n    #  -&gt; find a sensible cutoff point (project root)\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>We aim to provide additional tutorials in the future - contributions are welcome!</p>"}]}