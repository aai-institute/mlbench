{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Examples</p><p>Examples on how to use nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. See the user guides (TODO: Add guides) at the bottom of this page for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark()\ndef accuracy(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, using the <code>BenchmarkReporter</code> we report the resulting accuracy metric by printing it to the terminal in a table.</p> <pre><code>import nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\n# To collect in the current file, pass \"__main__\" as module name.\nresult = r.run(\"__main__\", params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test})\nreporter.display(result)\n</code></pre> <p>The resulting output might look like this:</p> <pre><code>python benchmarks.py\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"guides/benchmarks/","title":"Defining benchmarks with decorators","text":"<p>To benchmark your machine learning code in nnbench, define your key metrics in Python functions and apply one of the provided decorators. The available decorators are  - <code>@nnbench.benchmark</code>, which runs a benchmark with supplied parameters, - <code>@nnbench.parametrize</code>, which runs several benchmarks with the supplied parameter configurations, - <code>@nnbench.product</code>, which runs benchmarks with all parameter combinations that arise from the supplied values. </p> <p>First we introduce a small machine learning example which we will subsequently use to motivate the use of the three benchmark decorators.</p> <p>We recommend to split the model training, benchmark definition, and benchmark running into different files. In this guide, these are called <code>training.py</code>, <code>benchmarks.py</code>, and <code>main.py</code>.</p>"},{"location":"guides/benchmarks/#example","title":"Example","text":"<p>Let us consider an example where we want to evaluate a <code>scikit-learn</code> random forest classifier on the Iris dataset. For this purpose, we will define several helper functions inside a file, <code>training.py</code>. We use <code>prepare_data()</code>, to load the dataset,  <code>train_rf()</code> to train a random forest model with the specified parameters, and <code>accuracy()</code> to calculate the accuracy of the supplied model on the given dataset.</p> <pre><code># training.py\nimport numpy as np\nfrom sklearn import base, metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_data() -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    data = load_iris()\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    return X_train, X_test, y_train, y_test\n\n\ndef train_rf(X_train: np.ndarray, y_train: np.ndarray, n_estimators: int, max_depth: int, random_state: int = 42) -&gt; RandomForestClassifier:\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre>"},{"location":"guides/benchmarks/#benchmark-for-single-benchmarks","title":"<code>@benchmark</code> for single benchmarks","text":"<p>Now, we define our benchmarks in a new file called <code>benchmarks.py</code>. We first encapsulate the benchmark logic into a function, <code>benchmark_accuracy()</code> which prepares the data, trains a classifier, and lastly, obtains the accuracy. To mark such a function as a benchmark, we apply the <code>@benchmark</code> decorator.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.benchmark()\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Warning</p> <p>This training benchmark is designed as a local, simple, and self-contained example to showcase nnbench.  In a real world scenario, to follow best practices, you may want to separate the data preparation and model training steps from the benchmarking logic and pass the corresponding artifacts as a parameter to the benchmark. See the user guide for more information.</p> <p>Lastly, we set up a benchmark runner in the <code>main.py</code>. There, we supply the parameters (<code>n_estimators</code>, <code>max_depth</code>, <code>random_state</code>) necessary in the function definition as a dictionary to the <code>params</code> keyword argument. </p> <pre><code># main.py\nimport nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42})\nreporter.display(result)\n</code></pre> <p>When we execute the <code>main.py</code> we get the following output:</p> <pre><code>python main.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchparametrize-for-multiple-configuration-benchmarks","title":"<code>@nnbench.parametrize</code> for multiple configuration benchmarks","text":"<p>Sometimes, we are not only interested in the performance of a model for given parameters but want to compare the performance for different configurations.  To achieve this, we can turn our single accuracy benchmark in the <code>benchmarks.py</code> file into a parametrized benchmark. To do this, replace the decorator with <code>@nnbench.parametrize</code> and supply the parameter combinations of choice as dictionaries in the first argument.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.parametrize(\n    ({\"n_estimators\": 10, \"max_depth\": 2},\n    {\"n_estimators\": 50, \"max_depth\": 5},\n    {\"n_estimators\": 100, \"max_depth\": 10})\n)\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Notice that the parametrization is still incomplete, as we did not supply a <code>random_state</code> argument. The unfilled arguments are given in <code>BenchmarkRunner.run()</code> via a dictionary passed as the <code>params</code> keyword argument.</p> <pre><code># main.py\nimport nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"random_state\": 42})\nreporter.display(result)\n</code></pre> <p>Executing the parametrized benchmark, we get an output similar to this:</p> <pre><code>python main.py  \n\n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.955556\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.866667\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.911111\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchproduct-for-benchmarks-over-parameter-configuration-grids","title":"<code>@nnbench.product</code> for benchmarks over parameter configuration grids","text":"<p>In case we want to run a benchmark scan for all possible combinations of a set of parameters, we can use the <code>@nnbench.product</code> decorator to supply the different values for each parameter.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.product(n_estimators=[10, 50, 100], max_depth=[2, 5, 10])\ndef benchmark_accuracy_product(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>We still provide the <code>random_state</code> parameter to the runner directly, like we did with the <code>@nnbench.parametrize</code> decorator. By executing the benchmark, we get results for all combinations of <code>n_estimators</code> and <code>max_depth</code>. It looks similar to this:</p> <pre><code>python main.py  \n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=10_max_depth=5    0.955556\nbenchmark_accuracy_n_estimators=10_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=50_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.911111\nbenchmark_accuracy_n_estimators=50_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=100_max_depth=2   0.933333\nbenchmark_accuracy_n_estimators=100_max_depth=5   0.955556\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.955556\n</code></pre>"},{"location":"guides/customization/","title":"Defining setup/teardown tasks, context, and <code>nnbench.Parameters</code>","text":"<p>This page introduces some customization options for benchmark runs. These options can be helpful for tasks surrounding benchmark state management, such as automatic setup and cleanup, contextualizing results with context values, and defining typed parameters with the <code>nnbench.Parameters</code> class.</p>"},{"location":"guides/customization/#defining-setup-and-teardown-tasks","title":"Defining setup and teardown tasks","text":"<p>For some benchmarks, it is important to set certain configuration values and prepare the execution environment before running. To do this, you can pass a setup task to all of the nnbench decorators via the <code>setUp</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\n@nnbench.benchmark(setUp=set_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Similarly, to revert the environment state back to its previous form (or clean up any created resources), you can supply a finalization task with the <code>tearDown</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\ndef pop_envvar(**params):\n    os.environ.pop(\"MY_ENV\")\n\n\n@nnbench.benchmark(setUp=set_envvar, tearDown=pop_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Both the setup and teardown task must take the exact same set of parameters as the benchmark function. To simplify function declaration, it is easiest to use a variadic keyword-only interface, i.e. <code>setup(**kwargs)</code>, as shown.</p> <p>Tip</p> <p>This facility works exactly the same for the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators. There, the specified setup and teardown tasks are run once before or after each of the resulting benchmarks respectively.</p>"},{"location":"guides/customization/#enriching-benchmark-metadata-with-context-values","title":"Enriching benchmark metadata with context values","text":"<p>It is often useful to log specific environment metadata in addition to the benchmark's target metrics. Such metadata can give a clearer picture of how certain models perform on a given hardware, how model architectures compare in performance, and much more. In <code>nnbench</code>, you can give additional metadata to your benchmarks as context values.</p> <p>A context value is defined here as a key-value pair where <code>key</code> is a string, and <code>value</code> is any valid JSON value holding the desired information. As an example, the context value <code>{\"cpuarch\": \"arm64\"}</code> gives information about the CPU architecture of the host machine running the benchmark.</p> <p>A context provider is a function taking no arguments and returning a Python dictionary of context values. The following is a basic example of a context provider:</p> <pre><code>import platform\n\ndef platinfo() -&gt; dict[str, str]:\n    \"\"\"Returns CPU arch, system name (Windows/Linux/Darwin), and Python version.\"\"\"\n    return {\n        \"system\": platform.system(),\n        \"cpuarch\": platform.machine(),\n        \"python_version\": platform.python_version(),\n    }\n</code></pre> <p>To supply context to your benchmarks, you can give a sequence of context providers to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\n# uses the `platinfo` context provider from above to log platform metadata.\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params={}, context=[platinfo])\n</code></pre>"},{"location":"guides/customization/#being-type-safe-by-using-nnbenchparameters","title":"Being type safe by using <code>nnbench.Parameters</code>","text":"<p>Instead of specifying your benchmark's parameters by using a raw Python dictionary, you can define a custom subclass of <code>nnbench.Parameters</code>:</p> <pre><code>import nnbench\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MyParams(nnbench.Parameters):\n    a: int\n    b: int\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\nparams = MyParams(a=1, b=2)\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params=params)\n</code></pre> <p>While this does not have a concrete advantage in terms of type safety over a raw dictionary (all inputs will be checked against the types expected from the benchmark interfaces), it guards against accidental modification of parameters breaking reproducibility.</p>"},{"location":"guides/organization/","title":"How to efficiently organize benchmark code","text":"<p>To efficiently organize benchmarks and keeping your setup modular, you can follow a few guidelines.</p>"},{"location":"guides/organization/#tip-1-separate-benchmarks-from-project-code","title":"Tip 1: Separate benchmarks from project code","text":"<p>This tip is well known from other software development practices such as unit testing. To improve project organization, consider splitting off your benchmarks into their own modules or even directories, if you have multiple benchmark workloads.</p> <p>An example project layout can look like this, with benchmarks as a separate directory at the top-level:</p> <pre><code>my-project/\n\u251c\u2500\u2500 benchmarks/ # &lt;- contains all benchmarking Python files.\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre> <p>This keeps the benchmarks neatly grouped together while siloing them away from the actual project code. Since you will most likely not run your benchmarks in a production setting, this is also advantageous for packaging, as the <code>benchmarks/</code> directory does not ship by default in this configuration.</p>"},{"location":"guides/organization/#tip-2-group-benchmarks-by-common-attributes","title":"Tip 2: Group benchmarks by common attributes","text":"<p>To maintain good organization within your benchmark directory, you can group similar benchmarks into their own Python files. As an example, if you have a set of benchmarks to establish data quality, and benchmarks for scoring trained models on curated data, you could structure them as follows:</p> <pre><code>benchmarks/\n\u251c\u2500\u2500 data_quality.py\n\u251c\u2500\u2500 model_perf.py\n\u2514\u2500\u2500 ...\n</code></pre> <p>This is helpful when running multiple benchmark workloads separately, as you can just point your benchmark runner to each of these separate files:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\ndata_metrics = runner.run(\"benchmarks/data_quality.py\", params=...)\n# same for model metrics, where instead you pass benchmarks/model_perf.py.\nmodel_metrics = runner.run(\"benchmarks/model_perf.py\", params=...)\n</code></pre>"},{"location":"guides/organization/#tip-3-attach-tags-to-benchmarks-for-selective-filtering","title":"Tip 3: Attach tags to benchmarks for selective filtering","text":"<p>For structuring benchmarks within files, you can also use tags, which are tuples of strings attached to a benchmark:</p> <pre><code># benchmarks/data_quality.py\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo1(data) -&gt; float:\n    ...\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo2(data) -&gt; int:\n    ...\n\n\n@nnbench.benchmark(tags=(\"bar\",))\ndef bar(data) -&gt; int:\n    ...\n</code></pre> <p>Now, to only run data quality benchmarks marked \"foo\", pass the corresponding tag to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\nfoo_data_metrics = runner.run(\"benchmarks/data_quality.py\", params=..., tags=(\"foo\",))\n</code></pre> <p>Tip</p> <p>This concept works exactly the same when creating benchmarks with the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators.</p>"},{"location":"guides/runners/","title":"Collecting and running benchmarks","text":"<p>nnbench provides the <code>BenchmarkRunner</code> as a compact interface to collect and run benchmarks selectively.</p>"},{"location":"guides/runners/#the-abstract-benchmarkrunner-class","title":"The abstract <code>BenchmarkRunner</code>  class","text":"<p>Let's first instantiate and then walk through the base class.</p> <pre><code>from nnbench import BenchmarkRunner\n\nrunner = BenchmarkRunner()\n</code></pre> <p>Use the <code>BenchmarkRunner.collect()</code> method to collect benchmarks from files or directories. Assume we have the following benchmark setup: <pre><code># dir_a/bm1.py\nimport nnbench\n\n@nnbench.benchmark\ndef dummy_benchmark(a: int) -&gt; int:\n    return a\n</code></pre></p> <pre><code># dir_b/bm2.py\nimport nnbench\n\n@nnbench.benchmark(tags=(\"tag\",))\ndef another_benchmark(b: int) -&gt; int:\n    return b\n\n@nnbench.benchmark\ndef yet_another_benchmark(c: int) -&gt; int:\n    return c\n</code></pre> <pre><code># dir_b/bm3.py\nimport nnbench\n@nnbench.benchmark(tags=(\"tag\",))\ndef the_last_benchmark(d: int) -&gt; int:\n    return d\n</code></pre> <p>Now we can collect benchmarks from files:</p> <p><pre><code>runner.collect('dir_a/bm1.py')\n</code></pre> Or directories:</p> <pre><code>runner.collect('dir_b')\n</code></pre> <p>This collection can happen iteratively. So, after executing the two collections our runner has all four benchmarks ready for execution.</p> <p>To remove the collected benchmarks again, use the <code>BenchmarkRunner.clear()</code> method. You can also supply tags to the runner to selectively collect only benchmarks with the appropriate tag. For example, after clearing the runner again, you can collect all benchmarks with the <code>\"tag\"</code> tag as such:</p> <pre><code>runner.collect('dir_b', tags=(\"tag\",))\n</code></pre> <p>To run the benchmarks, call the <code>BenchmarkRunner.run()</code> method and supply the necessary parameters required by the collected benchmarks.</p> <pre><code>runner.run(\"dir_b\", params={\"b\": 1, \"c\": 2, \"d\": 3})\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>context</li> <li>core</li> <li>reporter<ul> <li>base</li> <li>duckdb_sql</li> <li>file</li> <li>util</li> </ul> </li> <li>runner</li> <li>types</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], dict[str, Any]]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo","title":"PythonInfo","text":"<p>A context helper returning version info for requested installed packages.</p> <p>If a requested package is not installed, an empty string is returned instead.</p> PARAMETER  DESCRIPTION <code>*packages</code> <p>Names of the requested packages under which they exist in the current environment. For packages installed through <code>pip</code>, this equals the PyPI package name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class PythonInfo:\n    \"\"\"\n    A context helper returning version info for requested installed packages.\n\n    If a requested package is not installed, an empty string is returned instead.\n\n    Parameters\n    ----------\n    *packages: str\n        Names of the requested packages under which they exist in the current environment.\n        For packages installed through ``pip``, this equals the PyPI package name.\n    \"\"\"\n\n    key = \"python\"\n\n    def __init__(self, *packages: str):\n        self.packages = packages\n\n    def __call__(self) -&gt; dict[str, Any]:\n        from importlib.metadata import PackageNotFoundError, version\n\n        result: dict[str, Any] = dict()\n\n        result[\"version\"] = platform.python_version()\n        result[\"implementation\"] = platform.python_implementation()\n        buildno, buildtime = platform.python_build()\n        result[\"buildno\"] = buildno\n        result[\"buildtime\"] = buildtime\n\n        dependencies: dict[str, str] = {}\n        for pkg in self.packages:\n            try:\n                dependencies[pkg] = version(pkg)\n            except PackageNotFoundError:\n                dependencies[pkg] = \"\"\n\n        result[\"dependencies\"] = dependencies\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'python'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages = packages\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo","title":"GitEnvironmentInfo","text":"<p>A context helper providing the current git commit, latest tag, and upstream repository name.</p> PARAMETER  DESCRIPTION <code>remote</code> <p>Remote name for which to provide info, by default <code>\"origin\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class GitEnvironmentInfo:\n    \"\"\"\n    A context helper providing the current git commit, latest tag, and upstream repository name.\n\n    Parameters\n    ----------\n    remote: str\n        Remote name for which to provide info, by default ``\"origin\"``.\n    \"\"\"\n\n    key = \"git\"\n\n    def __init__(self, remote: str = \"origin\"):\n        self.remote = remote\n\n    def __call__(self) -&gt; dict[str, dict[str, Any]]:\n        import subprocess\n\n        def git_subprocess(args: list[str]) -&gt; subprocess.CompletedProcess:\n            if platform.system() == \"Windows\":\n                git = \"git.exe\"\n            else:\n                git = \"git\"\n\n            return subprocess.run(  # nosec: B603\n                [git, *args], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\"\n            )\n\n        result: dict[str, Any] = {\n            \"commit\": \"\",\n            \"provider\": \"\",\n            \"repository\": \"\",\n            \"tag\": \"\",\n            \"dirty\": None,\n        }\n\n        # first, check if inside a repo.\n        p = git_subprocess([\"rev-parse\", \"--is-inside-work-tree\"])\n        # if not, return empty info.\n        if p.returncode:\n            return {\"git\": result}\n\n        # secondly: get the current commit.\n        p = git_subprocess([\"rev-parse\", \"HEAD\"])\n        if not p.returncode:\n            result[\"commit\"] = p.stdout.strip()\n\n        # thirdly, get the latest tag, without a short commit SHA attached.\n        p = git_subprocess([\"describe\", \"--tags\", \"--abbrev=0\"])\n        if not p.returncode:\n            result[\"tag\"] = p.stdout.strip()\n\n        # and finally, get the remote repo name pointed to by the given remote.\n        p = git_subprocess([\"remote\", \"get-url\", self.remote])\n        if not p.returncode:\n            remotename: str = p.stdout.strip()\n            # it's an SSH remote.\n            if \"@\" in remotename:\n                prefix, sep = \"git@\", \":\"\n            else:\n                # it is HTTPS.\n                prefix, sep = \"https://\", \"/\"\n\n            remotename = remotename.removeprefix(prefix)\n            provider, reponame = remotename.split(sep, 1)\n\n            result[\"provider\"] = provider\n            result[\"repository\"] = reponame.removesuffix(\".git\")\n\n        p = git_subprocess([\"status\", \"--porcelain\"])\n        if not p.returncode:\n            result[\"dirty\"] = bool(p.stdout.strip())\n\n        return {\"git\": result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'git'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.remote","title":"remote  <code>instance-attribute</code>","text":"<pre><code>remote = remote\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo","title":"CPUInfo","text":"Source code in <code>src/nnbench/context.py</code> <pre><code>class CPUInfo:\n    key = \"cpu\"\n\n    def __init__(\n        self,\n        memunit: Literal[\"kB\", \"MB\", \"GB\"] = \"MB\",\n        frequnit: Literal[\"kHz\", \"MHz\", \"GHz\"] = \"MHz\",\n    ):\n        self.memunit = memunit\n        self.frequnit = frequnit\n        self.conversion_table: dict[str, float] = {\"k\": 1e3, \"M\": 1e6, \"G\": 1e9}\n\n    def __call__(self) -&gt; dict[str, Any]:\n        try:\n            import psutil\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"context provider {self.__class__.__name__}() needs `psutil` installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade psutil`.\"\n            )\n\n        result: dict[str, Any] = dict()\n\n        # first, the platform info.\n        result[\"architecture\"] = platform.machine()\n        result[\"bitness\"] = platform.architecture()[0]\n        result[\"processor\"] = platform.processor()\n        result[\"system\"] = platform.system()\n        result[\"system-version\"] = platform.release()\n\n        freq_struct = psutil.cpu_freq()\n        freq_conversion = self.conversion_table[self.frequnit[0]]\n        # result is in MHz, so we convert to Hz and apply the conversion factor.\n        result[\"frequency\"] = freq_struct.current * 1e6 / freq_conversion\n        result[\"frequency_unit\"] = self.frequnit\n        result[\"min_frequency\"] = freq_struct.min\n        result[\"max_frequency\"] = freq_struct.max\n        result[\"num_cpus\"] = psutil.cpu_count(logical=False)\n        result[\"num_logical_cpus\"] = psutil.cpu_count()\n\n        mem_struct = psutil.virtual_memory()\n        mem_conversion = self.conversion_table[self.memunit[0]]\n        # result is in bytes, so no need for base conversion.\n        result[\"total_memory\"] = mem_struct.total / mem_conversion\n        result[\"memory_unit\"] = self.memunit\n        # TODO: Lacks CPU cache info, which requires a solution other than psutil.\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'cpu'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.memunit","title":"memunit  <code>instance-attribute</code>","text":"<pre><code>memunit = memunit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.frequnit","title":"frequnit  <code>instance-attribute</code>","text":"<pre><code>frequnit = frequnit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.conversion_table","title":"conversion_table  <code>instance-attribute</code>","text":"<pre><code>conversion_table: dict[str, float] = {'k': 1000.0, 'M': 1000000.0, 'G': 1000000000.0}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context","title":"Context","text":"Source code in <code>src/nnbench/context.py</code> <pre><code>class Context:\n    def __init__(self, data: dict[str, Any] | None = None) -&gt; None:\n        self._data: dict[str, Any] = data or {}\n\n    def __contains__(self, key: str) -&gt; bool:\n        return key in self.keys()\n\n    @property\n    def data(self):\n        return self._data\n\n    @staticmethod\n    def _ctx_items(d: dict[str, Any], prefix: str, sep: str) -&gt; Iterator[tuple[str, Any]]:\n        \"\"\"\n        Iterate over nested dictionary items. Keys are formatted to indicate their nested path.\n\n        Parameters\n        ----------\n        d : dict[str, Any]\n            Dictionary to iterate over.\n        prefix : str\n            Current prefix to prepend to keys, used for recursion to build the full key path.\n        sep : str\n            The separator to use between levels of nesting in the key path.\n\n        Yields\n        ------\n        tuple[str, Any]\n            Iterator over key-value tuples.\n        \"\"\"\n        for k, v in d.items():\n            new_key = prefix + sep + k if prefix else k\n            if isinstance(v, dict):\n                yield from Context._ctx_items(d=v, prefix=new_key, sep=sep)\n            else:\n                yield new_key, v\n\n    def keys(self, sep: str = \".\") -&gt; Iterator[str]:\n        \"\"\"\n        Keys of the context dictionary, with an optional separator for nested keys.\n\n        Parameters\n        ----------\n        sep : str, optional\n            Separator to use for nested keys.\n\n        Yields\n        ------\n        str\n            Iterator over the context dictionary keys.\n        \"\"\"\n        for k, v in self._ctx_items(d=self._data, prefix=\"\", sep=sep):\n            yield k\n\n    def values(self) -&gt; Iterator[Any]:\n        \"\"\"\n        Values of the context dictionary, including values from nested dictionaries.\n\n        Yields\n        ------\n        Any\n            Iterator over all values in the context dictionary.\n        \"\"\"\n        for k, v in self._ctx_items(d=self._data, prefix=\"\", sep=\"\"):\n            yield v\n\n    def items(self, sep: str = \".\") -&gt; Iterator[tuple[str, Any]]:\n        \"\"\"\n        Items (key-value pairs) of the context dictionary, with an separator for nested keys.\n\n        Parameters\n        ----------\n        sep : str, optional\n            Separator to use for nested dictionary keys.\n\n        Yields\n        ------\n        tuple[str, Any]\n            Iterator over the items of the context dictionary.\n        \"\"\"\n        yield from self._ctx_items(d=self._data, prefix=\"\", sep=sep)\n\n    def update(self, other: ContextProvider | dict[str, Any] | \"Context\") -&gt; None:\n        \"\"\"\n        Updates the context.\n        If `sep` is None, perform shallow update, deep update otherwise.\n\n        Parameters\n        ----------\n        other : ContextProvider | dict[str, Any] | \"Context\"\n            The data to update the context with. This can be a dictionary, a Context instance, or a provider.\n        \"\"\"\n        if callable(other):\n            other = other()\n        elif isinstance(other, Context):\n            other = other._data\n        self._data.update(other)\n\n    @staticmethod\n    def _flatten_dict(d: dict[str, Any], prefix: str = \"\", sep: str = \".\") -&gt; dict[str, Any]:\n        \"\"\"\n        Turn a nested dictionary into a flattened dictionary.\n\n        Parameters\n        ----------\n        d: dict[str, Any]\n            (Possibly) nested dictionary to flatten.\n        prefix: str\n            Key prefix to apply at the top-level (nesting level 0).\n        sep: str\n            Separator on which to join keys, \".\" by default.\n\n        Returns\n        -------\n        dict[str, Any]\n            The flattened dictionary.\n        \"\"\"\n\n        items: list[tuple[str, Any]] = []\n        for key, value in d.items():\n            new_key = prefix + sep + key if prefix else key\n            if isinstance(value, dict):\n                items.extend(Context._flatten_dict(d=value, prefix=new_key, sep=sep).items())\n            else:\n                items.append((new_key, value))\n        return dict(items)\n\n    def flatten(self, sep: str = \".\") -&gt; dict[str, Any]:\n        \"\"\"\n        Flatten the context's dictionary, converting nested dictionaries into a single dictionary with keys separated by `sep`.\n\n        Parameters\n        ----------\n        sep : str, optional\n            The separator used to join nested keys.\n\n        Returns\n        -------\n        dict[str, Any]\n            The flattened context values as a Python dictionary.\n        \"\"\"\n\n        return self._flatten_dict(self._data, prefix=\"\", sep=sep)\n\n    @staticmethod\n    def unflatten(d: dict[str, Any], sep: str = \".\") -&gt; dict[str, Any]:\n        \"\"\"\n        Recursively unflatten a dictionary by expanding keys seperated by `sep` into nested dictionaries.\n\n        Parameters\n        ----------\n        d : dict[str, Any]\n            The dictionary to unflatten.\n        sep : str, optional\n            The separator used in the flattened keys.\n\n        Returns\n        -------\n        dict[str, Any]\n            The unflattened dictionary.\n        \"\"\"\n        sorted_keys = sorted(d.keys())\n        unflattened = {}\n        for prefix, keys in itertools.groupby(sorted_keys, key=lambda key: key.split(sep, 1)[0]):\n            key_group = list(keys)\n            if len(key_group) == 1 and sep not in key_group[0]:\n                unflattened[prefix] = d[prefix]\n            else:\n                nested_dict = {key.split(sep, 1)[1]: d[key] for key in key_group}\n                unflattened[prefix] = Context.unflatten(d=nested_dict, sep=sep)\n        return unflattened\n\n    @classmethod\n    def make(cls, d: dict[str, Any]) -&gt; \"Context\":\n        \"\"\"\n        Create a new Context instance from a given dictionary.\n\n        Parameters\n        ----------\n        d : dict[str, Any]\n            The initialization dictionary.\n\n        Returns\n        -------\n        Context\n            The new Context instance.\n        \"\"\"\n        return cls(data=cls.unflatten(d))\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.data","title":"data  <code>property</code>","text":"<pre><code>data\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.keys","title":"keys","text":"<pre><code>keys(sep: str = '.') -&gt; Iterator[str]\n</code></pre> <p>Keys of the context dictionary, with an optional separator for nested keys.</p> PARAMETER  DESCRIPTION <code>sep</code> <p>Separator to use for nested keys.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> YIELDS DESCRIPTION <code>str</code> <p>Iterator over the context dictionary keys.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>def keys(self, sep: str = \".\") -&gt; Iterator[str]:\n    \"\"\"\n    Keys of the context dictionary, with an optional separator for nested keys.\n\n    Parameters\n    ----------\n    sep : str, optional\n        Separator to use for nested keys.\n\n    Yields\n    ------\n    str\n        Iterator over the context dictionary keys.\n    \"\"\"\n    for k, v in self._ctx_items(d=self._data, prefix=\"\", sep=sep):\n        yield k\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.values","title":"values","text":"<pre><code>values() -&gt; Iterator[Any]\n</code></pre> <p>Values of the context dictionary, including values from nested dictionaries.</p> YIELDS DESCRIPTION <code>Any</code> <p>Iterator over all values in the context dictionary.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>def values(self) -&gt; Iterator[Any]:\n    \"\"\"\n    Values of the context dictionary, including values from nested dictionaries.\n\n    Yields\n    ------\n    Any\n        Iterator over all values in the context dictionary.\n    \"\"\"\n    for k, v in self._ctx_items(d=self._data, prefix=\"\", sep=\"\"):\n        yield v\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.items","title":"items","text":"<pre><code>items(sep: str = '.') -&gt; Iterator[tuple[str, Any]]\n</code></pre> <p>Items (key-value pairs) of the context dictionary, with an separator for nested keys.</p> PARAMETER  DESCRIPTION <code>sep</code> <p>Separator to use for nested dictionary keys.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> YIELDS DESCRIPTION <code>tuple[str, Any]</code> <p>Iterator over the items of the context dictionary.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>def items(self, sep: str = \".\") -&gt; Iterator[tuple[str, Any]]:\n    \"\"\"\n    Items (key-value pairs) of the context dictionary, with an separator for nested keys.\n\n    Parameters\n    ----------\n    sep : str, optional\n        Separator to use for nested dictionary keys.\n\n    Yields\n    ------\n    tuple[str, Any]\n        Iterator over the items of the context dictionary.\n    \"\"\"\n    yield from self._ctx_items(d=self._data, prefix=\"\", sep=sep)\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.update","title":"update","text":"<pre><code>update(other: ContextProvider | dict[str, Any] | 'Context') -&gt; None\n</code></pre> <p>Updates the context. If <code>sep</code> is None, perform shallow update, deep update otherwise.</p> PARAMETER  DESCRIPTION <code>other</code> <p>The data to update the context with. This can be a dictionary, a Context instance, or a provider.</p> <p> TYPE: <code>ContextProvider | dict[str, Any] | 'Context'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>def update(self, other: ContextProvider | dict[str, Any] | \"Context\") -&gt; None:\n    \"\"\"\n    Updates the context.\n    If `sep` is None, perform shallow update, deep update otherwise.\n\n    Parameters\n    ----------\n    other : ContextProvider | dict[str, Any] | \"Context\"\n        The data to update the context with. This can be a dictionary, a Context instance, or a provider.\n    \"\"\"\n    if callable(other):\n        other = other()\n    elif isinstance(other, Context):\n        other = other._data\n    self._data.update(other)\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.flatten","title":"flatten","text":"<pre><code>flatten(sep: str = '.') -&gt; dict[str, Any]\n</code></pre> <p>Flatten the context's dictionary, converting nested dictionaries into a single dictionary with keys separated by <code>sep</code>.</p> PARAMETER  DESCRIPTION <code>sep</code> <p>The separator used to join nested keys.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The flattened context values as a Python dictionary.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>def flatten(self, sep: str = \".\") -&gt; dict[str, Any]:\n    \"\"\"\n    Flatten the context's dictionary, converting nested dictionaries into a single dictionary with keys separated by `sep`.\n\n    Parameters\n    ----------\n    sep : str, optional\n        The separator used to join nested keys.\n\n    Returns\n    -------\n    dict[str, Any]\n        The flattened context values as a Python dictionary.\n    \"\"\"\n\n    return self._flatten_dict(self._data, prefix=\"\", sep=sep)\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.unflatten","title":"unflatten  <code>staticmethod</code>","text":"<pre><code>unflatten(d: dict[str, Any], sep: str = '.') -&gt; dict[str, Any]\n</code></pre> <p>Recursively unflatten a dictionary by expanding keys seperated by <code>sep</code> into nested dictionaries.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The dictionary to unflatten.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>sep</code> <p>The separator used in the flattened keys.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The unflattened dictionary.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>@staticmethod\ndef unflatten(d: dict[str, Any], sep: str = \".\") -&gt; dict[str, Any]:\n    \"\"\"\n    Recursively unflatten a dictionary by expanding keys seperated by `sep` into nested dictionaries.\n\n    Parameters\n    ----------\n    d : dict[str, Any]\n        The dictionary to unflatten.\n    sep : str, optional\n        The separator used in the flattened keys.\n\n    Returns\n    -------\n    dict[str, Any]\n        The unflattened dictionary.\n    \"\"\"\n    sorted_keys = sorted(d.keys())\n    unflattened = {}\n    for prefix, keys in itertools.groupby(sorted_keys, key=lambda key: key.split(sep, 1)[0]):\n        key_group = list(keys)\n        if len(key_group) == 1 and sep not in key_group[0]:\n            unflattened[prefix] = d[prefix]\n        else:\n            nested_dict = {key.split(sep, 1)[1]: d[key] for key in key_group}\n            unflattened[prefix] = Context.unflatten(d=nested_dict, sep=sep)\n    return unflattened\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(d: dict[str, Any]) -&gt; 'Context'\n</code></pre> <p>Create a new Context instance from a given dictionary.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The initialization dictionary.</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Context</code> <p>The new Context instance.</p> Source code in <code>src/nnbench/context.py</code> <pre><code>@classmethod\ndef make(cls, d: dict[str, Any]) -&gt; \"Context\":\n    \"\"\"\n    Create a new Context instance from a given dictionary.\n\n    Parameters\n    ----------\n    d : dict[str, Any]\n        The initialization dictionary.\n\n    Returns\n    -------\n    Context\n        The new Context instance.\n    \"\"\"\n    return cls(data=cls.unflatten(d))\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/core.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>A display name to give to the benchmark. Useful in summaries and reports.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    name: str | None\n        A display name to give to the benchmark. Useful in summaries and reports.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        for params in parameters:\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        varnames = iterables.keys()\n        for values in itertools.product(*iterables.values()):\n            params = dict(zip(varnames, values))\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner","title":"BenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"An abstract benchmark runner class.\"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self):\n        self.benchmarks: list[Benchmark] = list()\n\n    def _check(self, params: dict[str, Any]) -&gt; None:\n        param_types = {k: type(v) for k, v in params.items()}\n        allvars: dict[str, tuple[type, Any]] = {}\n        empty = inspect.Parameter.empty\n\n        def _issubtype(t1: type, t2: type) -&gt; bool:\n            \"\"\"Small helper to make typechecks work on generics.\"\"\"\n\n            def _canonicalize(t: type) -&gt; type:\n                t_origin = get_origin(t)\n                if t_origin is not None:\n                    return t_origin\n                return t\n\n            if t1 == t2:\n                return True\n\n            t1 = _canonicalize(t1)\n            t2 = _canonicalize(t2)\n            if not inspect.isclass(t1):\n                return False\n            # TODO: Extend typing checks to args.\n            return issubclass(t1, t2)\n\n        for bm in self.benchmarks:\n            for var in bm.interface.variables:\n                name, typ, default = var\n                if name in params and default != empty:\n                    logger.debug(\n                        f\"using given value {params[name]} over default value {default} \"\n                        f\"for parameter {name!r} in benchmark {bm.fn.__name__}()\"\n                    )\n\n                if typ == empty:\n                    logger.debug(f\"parameter {name!r} untyped in benchmark {bm.fn.__name__}().\")\n\n                if name in allvars:\n                    currvar = allvars[name]\n                    orig_type, orig_val = new_type, new_val = currvar\n                    # If a benchmark has a variable without a default value,\n                    # that variable is taken into the combined interface as no-default.\n                    if default == empty:\n                        new_val = default\n                    # These types need not be exact matches, just compatible.\n                    # Two types are compatible iff either is a subtype of the other.\n                    # We only log the narrowest type for each varname in the final interface,\n                    # since that determines whether an input value is admissible.\n                    if _issubtype(orig_type, typ):\n                        pass\n                    elif _issubtype(typ, orig_type):\n                        new_type = typ\n                    else:\n                        raise TypeError(\n                            f\"got incompatible types {orig_type}, {typ} for parameter {name!r}\"\n                        )\n                    newvar = (new_type, new_val)\n                    if newvar != currvar:\n                        allvars[name] = newvar\n                else:\n                    allvars[name] = (typ, default)\n\n        for name, (typ, default) in allvars.items():\n            # check if a no-default variable has no parameter.\n            if name not in param_types and default == empty:\n                raise ValueError(f\"missing value for required parameter {name!r}\")\n\n            # skip the subsequent type check if the variable is untyped.\n            if typ == empty:\n                continue\n            # type-check parameter value against the narrowest hinted type.\n            if name in param_types and not _issubtype(param_types[name], typ):\n                raise TypeError(\n                    f\"expected type {typ} for parameter {name!r}, got {param_types[name]}\"\n                )\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py, tags)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        elif ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if isdunder(k):\n                continue\n            elif isinstance(v, self.benchmark_type):\n                if not tags or set(tags) &amp; set(v.tags):\n                    self.benchmarks.append(v)\n            elif iscontainer(v):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        if not tags or set(tags) &amp; set(bm.tags):\n                            self.benchmarks.append(bm)\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str],\n        params: dict[str, Any] | Parameters | None = None,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | Parameters | None\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n\n        Returns\n        -------\n        BenchmarkRecord\n            A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n            holding the context information, and \"benchmarks\", holding an array with the\n            benchmark results.\n\n        Raises\n        ------\n        ValueError\n            If any context value is provided more than once.\n        \"\"\"\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        # if we still have no benchmarks after collection, warn and return an empty record.\n        if not self.benchmarks:\n            warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n            return BenchmarkRecord(context=Context(), benchmarks=[])\n\n        params = params or {}\n        if isinstance(params, Parameters):\n            dparams = asdict(params)\n        else:\n            dparams = params\n\n        self._check(dparams)\n\n        ctx = Context()\n\n        for provider in context:\n            ctxval = provider()\n            # we do not allow multiple values for a context key.\n            duplicates = set(ctx.keys()) &amp; set(ctxval.keys())\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx.update(ctxval)\n\n        results: list[dict[str, Any]] = []\n        for benchmark in self.benchmarks:\n            bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n            # TODO: Wrap this into an execution context\n            res: dict[str, Any] = {\n                \"name\": benchmark.name,\n                \"function\": qualname(benchmark.fn),\n                \"description\": benchmark.fn.__doc__ or \"\",\n                \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n                \"error_occurred\": False,\n                \"error_message\": \"\",\n            }\n            try:\n                benchmark.setUp(**bmparams)\n                with timer(res):\n                    res[\"value\"] = benchmark.fn(**bmparams)\n            except Exception as e:\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(**bmparams)\n                results.append(res)\n\n        return BenchmarkRecord(\n            context=ctx,\n            benchmarks=results,\n        )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    ppath = Path(path_or_module)\n    if ppath.is_dir():\n        pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n        for py in pythonpaths:\n            logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n            self.collect(py, tags)\n        return\n    elif ppath.is_file():\n        module = import_file_as_module(path_or_module)\n    elif ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        raise ValueError(\n            f\"expected a module name, Python file, or directory, \"\n            f\"got {str(path_or_module)!r}\"\n        )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if isdunder(k):\n            continue\n        elif isinstance(v, self.benchmark_type):\n            if not tags or set(tags) &amp; set(v.tags):\n                self.benchmarks.append(v)\n        elif iscontainer(v):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        self.benchmarks.append(bm)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Parameters | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>A JSON output representing the benchmark results. Has two top-level keys, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If any context value is provided more than once.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | Parameters | None\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n\n    Returns\n    -------\n    BenchmarkRecord\n        A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n        holding the context information, and \"benchmarks\", holding an array with the\n        benchmark results.\n\n    Raises\n    ------\n    ValueError\n        If any context value is provided more than once.\n    \"\"\"\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    # if we still have no benchmarks after collection, warn and return an empty record.\n    if not self.benchmarks:\n        warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n        return BenchmarkRecord(context=Context(), benchmarks=[])\n\n    params = params or {}\n    if isinstance(params, Parameters):\n        dparams = asdict(params)\n    else:\n        dparams = params\n\n    self._check(dparams)\n\n    ctx = Context()\n\n    for provider in context:\n        ctxval = provider()\n        # we do not allow multiple values for a context key.\n        duplicates = set(ctx.keys()) &amp; set(ctxval.keys())\n        if duplicates:\n            dupe, *_ = duplicates\n            raise ValueError(f\"got multiple values for context key {dupe!r}\")\n        ctx.update(ctxval)\n\n    results: list[dict[str, Any]] = []\n    for benchmark in self.benchmarks:\n        bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n        # TODO: Wrap this into an execution context\n        res: dict[str, Any] = {\n            \"name\": benchmark.name,\n            \"function\": qualname(benchmark.fn),\n            \"description\": benchmark.fn.__doc__ or \"\",\n            \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"error_occurred\": False,\n            \"error_message\": \"\",\n        }\n        try:\n            benchmark.setUp(**bmparams)\n            with timer(res):\n                res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(**bmparams)\n            results.append(res)\n\n    return BenchmarkRecord(\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.iscontainer","title":"iscontainer","text":"<pre><code>iscontainer(s: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def iscontainer(s: Any) -&gt; bool:\n    return isinstance(s, (tuple, list))\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.isdunder","title":"isdunder","text":"<pre><code>isdunder(s: str) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def isdunder(s: str) -&gt; bool:\n    return s.startswith(\"__\") and s.endswith(\"__\")\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.qualname","title":"qualname","text":"<pre><code>qualname(fn: Callable) -&gt; str\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def qualname(fn: Callable) -&gt; str:\n    fnname = fn.__name__\n    fnqualname = fn.__qualname__\n    if fnname == fnqualname:\n        return fnname\n    return f\"{fnqualname}.{fnname}\"\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.timer","title":"timer","text":"<pre><code>timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>@contextlib.contextmanager\ndef timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]:\n    start = time.perf_counter_ns()\n    try:\n        yield\n    finally:\n        end = time.perf_counter_ns()\n        bm[\"time_ns\"] = end - start\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":"<p>Useful type interfaces to override/subclass in benchmarking workflows.</p>"},{"location":"reference/nnbench/types/#nnbench.types.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord","title":"BenchmarkRecord  <code>dataclass</code>","text":"Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass BenchmarkRecord:\n    context: Context\n    benchmarks: list[dict[str, Any]]\n\n    def compact(\n        self,\n        mode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n        sep: str = \".\",\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Prepare the benchmark results, optionally inlining the context either as a\n        nested dictionary or in flattened form.\n\n        Parameters\n        ----------\n        mode: Literal[\"flatten\", \"inline\", \"omit\"]\n            How to handle the context. ``\"omit\"`` leaves out the context entirely, ``\"inline\"``\n            inserts it into the benchmark dictionary as a single entry named ``\"context\"``, and\n            ``\"flatten\"`` inserts the flattened context values into the dictionary.\n        sep: str\n            The separator to use when flattening the context, i.e. when ``mode = \"flatten\"``.\n\n        Returns\n        -------\n        list[dict[str, Any]]\n            The updated list of benchmark records.\n        \"\"\"\n        if mode == \"omit\":\n            return self.benchmarks\n\n        for b in self.benchmarks:\n            if mode == \"inline\":\n                b[\"context\"] = self.context.data\n            elif mode == \"flatten\":\n                flat = self.context.flatten(sep=sep)\n                b.update(flat)\n                b[\"_context_keys\"] = tuple(self.context.keys())\n        return self.benchmarks\n\n    @classmethod\n    def expand(cls, bms: list[dict[str, Any]]) -&gt; BenchmarkRecord:\n        \"\"\"\n        Expand a list of deserialized JSON-like objects into a benchmark record.\n        This is equivalent to extracting the context given by the method it was\n        serialized with, and then returning the rest of the data as is.\n        Parameters\n        ----------\n        bms: list[dict[str, Any]]\n            The list of benchmark dicts to expand into a record.\n\n        Returns\n        -------\n        BenchmarkRecord\n            The resulting record with the context extracted.\n\n        \"\"\"\n        dctx: dict[str, Any] = {}\n        for b in bms:\n            if \"context\" in b:\n                dctx = b.pop(\"context\")\n            elif \"_context_keys\" in b:\n                ctxkeys = b.pop(\"_context_keys\")\n                for k in ctxkeys:\n                    # This should never throw, save for data corruption.\n                    dctx[k] = b.pop(k)\n        return cls(context=Context.make(dctx), benchmarks=bms)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: Context\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.compact","title":"compact","text":"<pre><code>compact(\n    mode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\", sep: str = \".\"\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Prepare the benchmark results, optionally inlining the context either as a nested dictionary or in flattened form.</p> PARAMETER  DESCRIPTION <code>mode</code> <p>How to handle the context. <code>\"omit\"</code> leaves out the context entirely, <code>\"inline\"</code> inserts it into the benchmark dictionary as a single entry named <code>\"context\"</code>, and <code>\"flatten\"</code> inserts the flattened context values into the dictionary.</p> <p> TYPE: <code>Literal['flatten', 'inline', 'omit']</code> DEFAULT: <code>'inline'</code> </p> <code>sep</code> <p>The separator to use when flattening the context, i.e. when <code>mode = \"flatten\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>The updated list of benchmark records.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>def compact(\n    self,\n    mode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n    sep: str = \".\",\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Prepare the benchmark results, optionally inlining the context either as a\n    nested dictionary or in flattened form.\n\n    Parameters\n    ----------\n    mode: Literal[\"flatten\", \"inline\", \"omit\"]\n        How to handle the context. ``\"omit\"`` leaves out the context entirely, ``\"inline\"``\n        inserts it into the benchmark dictionary as a single entry named ``\"context\"``, and\n        ``\"flatten\"`` inserts the flattened context values into the dictionary.\n    sep: str\n        The separator to use when flattening the context, i.e. when ``mode = \"flatten\"``.\n\n    Returns\n    -------\n    list[dict[str, Any]]\n        The updated list of benchmark records.\n    \"\"\"\n    if mode == \"omit\":\n        return self.benchmarks\n\n    for b in self.benchmarks:\n        if mode == \"inline\":\n            b[\"context\"] = self.context.data\n        elif mode == \"flatten\":\n            flat = self.context.flatten(sep=sep)\n            b.update(flat)\n            b[\"_context_keys\"] = tuple(self.context.keys())\n    return self.benchmarks\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.expand","title":"expand  <code>classmethod</code>","text":"<pre><code>expand(bms: list[dict[str, Any]]) -&gt; BenchmarkRecord\n</code></pre> <p>Expand a list of deserialized JSON-like objects into a benchmark record. This is equivalent to extracting the context given by the method it was serialized with, and then returning the rest of the data as is.</p> PARAMETER  DESCRIPTION <code>bms</code> <p>The list of benchmark dicts to expand into a record.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The resulting record with the context extracted.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef expand(cls, bms: list[dict[str, Any]]) -&gt; BenchmarkRecord:\n    \"\"\"\n    Expand a list of deserialized JSON-like objects into a benchmark record.\n    This is equivalent to extracting the context given by the method it was\n    serialized with, and then returning the rest of the data as is.\n    Parameters\n    ----------\n    bms: list[dict[str, Any]]\n        The list of benchmark dicts to expand into a record.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The resulting record with the context extracted.\n\n    \"\"\"\n    dctx: dict[str, Any] = {}\n    for b in bms:\n        if \"context\" in b:\n            dctx = b.pop(\"context\")\n        elif \"_context_keys\" in b:\n            ctxkeys = b.pop(\"_context_keys\")\n            for k in ctxkeys:\n                # This should never throw, save for data corruption.\n                dctx[k] = b.pop(k)\n    return cls(context=Context.make(dctx), benchmarks=bms)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact","title":"Artifact","text":"<p>             Bases: <code>Generic[T]</code></p> <p>A base artifact class for loading (materializing) artifacts from disk or from remote storage.</p> <p>This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way. It is most useful when running models on already saved data or models, e.g. when comparing a newly trained model against a baseline in storage.</p> <p>Subclasses need to implement the <code>Artifact.materialize()</code> API, telling nnbench how to load the desired artifact from a path.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the artifact files.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>class Artifact(Generic[T]):\n    \"\"\"\n    A base artifact class for loading (materializing) artifacts from disk or from remote storage.\n\n    This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way.\n    It is most useful when running models on already saved data or models, e.g. when\n    comparing a newly trained model against a baseline in storage.\n\n    Subclasses need to implement the `Artifact.materialize()` API, telling nnbench how to\n    load the desired artifact from a path.\n\n    Parameters\n    ----------\n    path: str | os.PathLike[str]\n        Path to the artifact files.\n    \"\"\"\n\n    def __init__(self, path: str | os.PathLike[str]) -&gt; None:\n        # Save the path for later just-in-time materialization.\n        self.path = path\n        self._value: T | None = None\n\n    @classmethod\n    def materialize(cls) -&gt; \"Artifact\":\n        \"\"\"Load the artifact from storage.\"\"\"\n        raise NotImplementedError\n\n    def value(self) -&gt; T:\n        if self._value is None:\n            raise ValueError(\n                f\"artifact has not been instantiated yet, \"\n                f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n            )\n        return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = path\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.materialize","title":"materialize  <code>classmethod</code>","text":"<pre><code>materialize() -&gt; 'Artifact'\n</code></pre> <p>Load the artifact from storage.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef materialize(cls) -&gt; \"Artifact\":\n    \"\"\"Load the artifact from storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.value","title":"value","text":"<pre><code>value() -&gt; T\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def value(self) -&gt; T:\n    if self._value is None:\n        raise ValueError(\n            f\"artifact has not been instantiated yet, \"\n            f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n        )\n    return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Parameters","title":"Parameters  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Parameters:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>field(default=None)</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>field(repr=False, default=())</code> </p> <code>interface</code> <p>Interface of the benchmark function</p> <p> TYPE: <code>Interface</code> DEFAULT: <code>field(init=False, repr=False)</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    interface: Interface\n        Interface of the benchmark function\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str | None = field(default=None)\n    setUp: Callable[..., None] = field(repr=False, default=NoOp)\n    tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n    interface: Interface = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if not self.name:\n            super().__setattr__(\"name\", self.fn.__name__)\n        super().__setattr__(\"interface\", Interface.from_callable(self.fn))\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = field(default=None)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface. An instance of this class is created using the <code>from_callable</code> class method.</p> Parameters: <p>names : tuple[str, ...]     Names of the function parameters. types : tuple[type, ...]     Types of the function parameters. defaults : tuple     A tuple of the function parameters' default values. variables : tuple[Variable, ...]     A tuple of tuples, where each inner tuple contains the parameter name and type. returntype: type     The function's return type annotation, or NoneType if left untyped.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface. An instance of this class\n    is created using the `from_callable` class method.\n\n    Parameters:\n    ----------\n    names : tuple[str, ...]\n        Names of the function parameters.\n    types : tuple[type, ...]\n        Types of the function parameters.\n    defaults : tuple\n        A tuple of the function parameters' default values.\n    variables : tuple[Variable, ...]\n        A tuple of tuples, where each inner tuple contains the parameter name and type.\n    returntype: type\n        The function's return type annotation, or NoneType if left untyped.\n    \"\"\"\n\n    names: tuple[str, ...]\n    types: tuple[type, ...]\n    defaults: tuple\n    variables: tuple[Variable, ...]\n    returntype: type\n\n    @classmethod\n    def from_callable(cls, fn: Callable) -&gt; Interface:\n        \"\"\"\n        Creates an interface instance from the given callable.\n        \"\"\"\n        # Set follow_wrapped=False to get the partially filled interfaces.\n        # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n        sig = inspect.signature(fn, follow_wrapped=False)\n        ret = sig.return_annotation\n        return cls(\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(p.default for p in sig.parameters.values()),\n            tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable) -&gt; Interface\n</code></pre> <p>Creates an interface instance from the given callable.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable) -&gt; Interface:\n    \"\"\"\n    Creates an interface instance from the given callable.\n    \"\"\"\n    # Set follow_wrapped=False to get the partially filled interfaces.\n    # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n    sig = inspect.signature(fn, follow_wrapped=False)\n    ret = sig.return_annotation\n    return cls(\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(p.default for p in sig.parameters.values()),\n        tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file)\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.with_suffix(\"\").as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: For absolute paths, the resulting module name will be horrifying\n    #  -&gt; find a sensible cutoff point (project root)\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/reporter/base/","title":"base","text":""},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter","title":"BenchmarkReporter","text":"<p>The base interface for a benchmark reporter class.</p> <p>A benchmark reporter consumes benchmark results from a previous run, and subsequently reports them in the way specified by the respective implementation's <code>report_result()</code> method.</p> <p>For example, to write benchmark results to a database, you could save the credentials for authentication on the class, and then stream the results directly to the database in <code>report_result()</code>, with preprocessing if necessary.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>class BenchmarkReporter:\n    \"\"\"\n    The base interface for a benchmark reporter class.\n\n    A benchmark reporter consumes benchmark results from a previous run, and subsequently\n    reports them in the way specified by the respective implementation's ``report_result()``\n    method.\n\n    For example, to write benchmark results to a database, you could save the credentials\n    for authentication on the class, and then stream the results directly to\n    the database in ``report_result()``, with preprocessing if necessary.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._initialized = False\n\n    def initialize(self):\n        \"\"\"\n        Initialize the reporter's state.\n\n        This is the intended place to create resources like a result directory,\n        a database connection, or a HTTP client.\n        \"\"\"\n        self._initialized = True\n\n    def finalize(self):\n        \"\"\"\n        Finalize the reporter's state.\n\n        This is the intended place to destroy/release resources that were previously\n        acquired in ``initialize()``.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def display(\n        record: BenchmarkRecord,\n        benchmark_filter: str | None = None,\n        include: tuple[str, ...] | None = None,\n        exclude: tuple[str, ...] = (),\n        include_context: tuple[str, ...] = (),\n        exclude_empty: bool = True,\n        tablefmt: str = \"simple\",\n        custom_formatters: dict[str, Callable[[Any], Any]] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Display a benchmark record in the console.\n\n        Benchmarks and context values will be filtered before display\n        if any filtering is applied.\n\n        Columns that do not contain any useful information are omitted by default.\n\n        Parameters\n        ----------\n        record: BenchmarkRecord\n            The benchmark record to display.\n        benchmark_filter: str | None\n            A regex used to match benchmark names whose results to display.\n        include: tuple[str, ...] | None\n            Columns to include in the displayed table.\n        exclude: tuple[str, ...]\n            Columns to exclude from the displayed table.\n        include_context: tuple[str, ...]\n            Context values to include. Supports nested attribute via dot syntax,\n            i.e. a name \"foo.bar\" causes the member ``\"bar\"`` of the context value\n            ``\"foo\"`` to be displayed.\n        exclude_empty: bool\n            Exclude columns that only contain false-ish values.\n        tablefmt: str\n            A table format identifier to use when displaying records in the console.\n        custom_formatters: dict[str, Callable[[Any], Any]] | None\n            A mapping of column names to custom formatters, i.e. functions formatting input\n            values for display in the console.\n        \"\"\"\n        benchmarks = record.benchmarks\n        # This assumes a stable schema across benchmarks.\n        if include is None:\n            includes = set(benchmarks[0].keys())\n        else:\n            includes = set(include)\n\n        excludes = set(exclude)\n        nulls = set() if not exclude_empty else nullcols(benchmarks)\n        cols = includes - nulls - excludes\n\n        if benchmark_filter is not None:\n            regex = re.compile(benchmark_filter, flags=re.IGNORECASE)\n        else:\n            regex = None\n\n        filtered = []\n        for bm in benchmarks:\n            if regex is not None and regex.search(bm[\"name\"]) is None:\n                continue\n            filteredctx = {\n                k: v\n                for k, v in record.context.items()\n                if any(k.startswith(i) for i in include_context)\n            }\n            filteredbm = {k: v for k, v in bm.items() if k in cols}\n            filteredbm.update(filteredctx)\n            # only apply custom formatters after context merge\n            #  to allow custom formatting of context values.\n            filteredbm = {\n                k: custom_formatters.get(k, lambda x: x)(v) for k, v in filteredbm.items()\n            }\n            filtered.append(filteredbm)\n\n        print(tabulate(filtered, headers=\"keys\", tablefmt=tablefmt))\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize the reporter's state.</p> <p>This is the intended place to create resources like a result directory, a database connection, or a HTTP client.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Initialize the reporter's state.\n\n    This is the intended place to create resources like a result directory,\n    a database connection, or a HTTP client.\n    \"\"\"\n    self._initialized = True\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize the reporter's state.</p> <p>This is the intended place to destroy/release resources that were previously acquired in <code>initialize()</code>.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Finalize the reporter's state.\n\n    This is the intended place to destroy/release resources that were previously\n    acquired in ``initialize()``.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.display","title":"display  <code>staticmethod</code>","text":"<pre><code>display(\n    record: BenchmarkRecord,\n    benchmark_filter: str | None = None,\n    include: tuple[str, ...] | None = None,\n    exclude: tuple[str, ...] = (),\n    include_context: tuple[str, ...] = (),\n    exclude_empty: bool = True,\n    tablefmt: str = \"simple\",\n    custom_formatters: dict[str, Callable[[Any], Any]] | None = None,\n) -&gt; None\n</code></pre> <p>Display a benchmark record in the console.</p> <p>Benchmarks and context values will be filtered before display if any filtering is applied.</p> <p>Columns that do not contain any useful information are omitted by default.</p> PARAMETER  DESCRIPTION <code>record</code> <p>The benchmark record to display.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> <code>benchmark_filter</code> <p>A regex used to match benchmark names whose results to display.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>include</code> <p>Columns to include in the displayed table.</p> <p> TYPE: <code>tuple[str, ...] | None</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>Columns to exclude from the displayed table.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>include_context</code> <p>Context values to include. Supports nested attribute via dot syntax, i.e. a name \"foo.bar\" causes the member <code>\"bar\"</code> of the context value <code>\"foo\"</code> to be displayed.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>exclude_empty</code> <p>Exclude columns that only contain false-ish values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>tablefmt</code> <p>A table format identifier to use when displaying records in the console.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple'</code> </p> <code>custom_formatters</code> <p>A mapping of column names to custom formatters, i.e. functions formatting input values for display in the console.</p> <p> TYPE: <code>dict[str, Callable[[Any], Any]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>@staticmethod\ndef display(\n    record: BenchmarkRecord,\n    benchmark_filter: str | None = None,\n    include: tuple[str, ...] | None = None,\n    exclude: tuple[str, ...] = (),\n    include_context: tuple[str, ...] = (),\n    exclude_empty: bool = True,\n    tablefmt: str = \"simple\",\n    custom_formatters: dict[str, Callable[[Any], Any]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Display a benchmark record in the console.\n\n    Benchmarks and context values will be filtered before display\n    if any filtering is applied.\n\n    Columns that do not contain any useful information are omitted by default.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The benchmark record to display.\n    benchmark_filter: str | None\n        A regex used to match benchmark names whose results to display.\n    include: tuple[str, ...] | None\n        Columns to include in the displayed table.\n    exclude: tuple[str, ...]\n        Columns to exclude from the displayed table.\n    include_context: tuple[str, ...]\n        Context values to include. Supports nested attribute via dot syntax,\n        i.e. a name \"foo.bar\" causes the member ``\"bar\"`` of the context value\n        ``\"foo\"`` to be displayed.\n    exclude_empty: bool\n        Exclude columns that only contain false-ish values.\n    tablefmt: str\n        A table format identifier to use when displaying records in the console.\n    custom_formatters: dict[str, Callable[[Any], Any]] | None\n        A mapping of column names to custom formatters, i.e. functions formatting input\n        values for display in the console.\n    \"\"\"\n    benchmarks = record.benchmarks\n    # This assumes a stable schema across benchmarks.\n    if include is None:\n        includes = set(benchmarks[0].keys())\n    else:\n        includes = set(include)\n\n    excludes = set(exclude)\n    nulls = set() if not exclude_empty else nullcols(benchmarks)\n    cols = includes - nulls - excludes\n\n    if benchmark_filter is not None:\n        regex = re.compile(benchmark_filter, flags=re.IGNORECASE)\n    else:\n        regex = None\n\n    filtered = []\n    for bm in benchmarks:\n        if regex is not None and regex.search(bm[\"name\"]) is None:\n            continue\n        filteredctx = {\n            k: v\n            for k, v in record.context.items()\n            if any(k.startswith(i) for i in include_context)\n        }\n        filteredbm = {k: v for k, v in bm.items() if k in cols}\n        filteredbm.update(filteredctx)\n        # only apply custom formatters after context merge\n        #  to allow custom formatting of context values.\n        filteredbm = {\n            k: custom_formatters.get(k, lambda x: x)(v) for k, v in filteredbm.items()\n        }\n        filtered.append(filteredbm)\n\n    print(tabulate(filtered, headers=\"keys\", tablefmt=tablefmt))\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/","title":"duckdb_sql","text":""},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DUCKDB_INSTALLED","title":"DUCKDB_INSTALLED  <code>module-attribute</code>","text":"<pre><code>DUCKDB_INSTALLED = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter","title":"DuckDBReporter","text":"<p>             Bases: <code>FileReporter</code></p> <p>A reporter for streaming benchmark results to duckdb.</p> <p>Serializes records into flat files in a temporary directory, and reads them in again afterwards.</p> PARAMETER  DESCRIPTION <code>dbname</code> <p>Name of the database to connect to. The default value <code>\"memory\"</code> uses an in-memory duckdb database.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':memory:'</code> </p> <code>read_only</code> <p>Connect to a database in read-only mode.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>directory</code> <p>Destination directory to write records to. Must point to an existing directory. If omitted, a temporary directory will be used.</p> <p> TYPE: <code>str | PathLike[str] | None</code> DEFAULT: <code>None</code> </p> <code>delete</code> <p>Delete the directory containing the written records after use. If the destination directory is a temporary directory, delete is implicitly true always.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ModuleNotFoundError</code> <p>If <code>duckdb</code> is not installed.</p> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>class DuckDBReporter(FileReporter):\n    \"\"\"\n    A reporter for streaming benchmark results to duckdb.\n\n    Serializes records into flat files in a temporary directory, and\n    reads them in again afterwards.\n\n    Parameters\n    ----------\n    dbname: str\n        Name of the database to connect to. The default value ``\"memory\"``\n        uses an in-memory duckdb database.\n    read_only: bool\n        Connect to a database in read-only mode.\n    directory: str | os.PathLike[str] | None\n        Destination directory to write records to. Must point to an existing directory.\n        If omitted, a temporary directory will be used.\n    delete: bool\n        Delete the directory containing the written records after use. If the destination\n        directory is a temporary directory, delete is implicitly true always.\n\n    Raises\n    ------\n    ModuleNotFoundError\n        If ``duckdb`` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        dbname: str = \":memory:\",\n        read_only: bool = False,\n        directory: str | os.PathLike[str] | None = None,\n        delete: bool = False,\n    ):\n        if not DUCKDB_INSTALLED:\n            raise ModuleNotFoundError(\n                f\"class {self.__class__.__name__!r} needs `duckdb` to be installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade duckdb`.\"\n            )\n\n        super().__init__()\n        self.dbname = dbname\n        self.read_only = read_only\n\n        # A place to store intermediate JSON records.\n        if not directory:\n            self._directory = Path(tempfile.mkdtemp())\n            self.delete = True\n        else:\n            self._directory = Path(directory)\n            self.delete = delete\n\n        weakref.finalize(self, self.finalize)\n\n        self.conn: duckdb.DuckDBPyConnection | None = None\n\n    @property\n    def directory(self) -&gt; os.PathLike[str]:\n        return self._directory\n\n    def initialize(self):\n        self.conn = duckdb.connect(self.dbname, read_only=self.read_only)\n        self._initialized = True\n\n    def finalize(self):\n        if self.conn:\n            self.conn.close()\n\n        if self.delete:\n            shutil.rmtree(self._directory, ignore_errors=True)\n\n    def read_sql(\n        self,\n        file: str | os.PathLike[str],\n        driver: str | None = None,\n        include: tuple[str, ...] | None = None,\n        alias: dict[str, str] | None = None,\n        limit: int | None = None,\n    ) -&gt; BenchmarkRecord:\n        if not self._initialized:\n            self.initialize()\n\n        driver = driver or Path(file).suffix.removeprefix(\".\")\n        if driver not in [\"json\", \"csv\", \"parquet\"]:\n            raise NotImplementedError(\"duckdb only supports reading JSON, CSV or parquet files\")\n\n        alias = alias or {}\n        limit = limit or 0\n        if limit &lt; 0:\n            raise ValueError(\"'limit' must be non-negative\")\n\n        if include is None:\n            cols = \"*\"\n        else:\n            cols = \", \".join(i if i not in alias else f\"{i} AS {alias[i]}\" for i in include)\n\n        # TODO: Query support for WHERE\n        query = f\"SELECT {cols} FROM read_json_auto('{str(file)}')\"  # nosec B608\n\n        rel = self.conn.sql(query)\n        columns = rel.columns\n        results = rel.fetchall()\n\n        benchmarks = [dict(zip(columns, r)) for r in results]\n        context = Context()\n        for bm in benchmarks:\n            context.update(bm.pop(\"context\", {}))\n\n        return BenchmarkRecord(context=context, benchmarks=benchmarks)\n\n    def raw_sql(self, query: str) -&gt; duckdb.DuckDBPyRelation:\n        rel = self.conn.sql(query=query)\n        return rel\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.dbname","title":"dbname  <code>instance-attribute</code>","text":"<pre><code>dbname = dbname\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.read_only","title":"read_only  <code>instance-attribute</code>","text":"<pre><code>read_only = read_only\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.conn","title":"conn  <code>instance-attribute</code>","text":"<pre><code>conn: DuckDBPyConnection | None = None\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.directory","title":"directory  <code>property</code>","text":"<pre><code>directory: PathLike[str]\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def initialize(self):\n    self.conn = duckdb.connect(self.dbname, read_only=self.read_only)\n    self._initialized = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def finalize(self):\n    if self.conn:\n        self.conn.close()\n\n    if self.delete:\n        shutil.rmtree(self._directory, ignore_errors=True)\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.read_sql","title":"read_sql","text":"<pre><code>read_sql(\n    file: str | PathLike[str],\n    driver: str | None = None,\n    include: tuple[str, ...] | None = None,\n    alias: dict[str, str] | None = None,\n    limit: int | None = None,\n) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def read_sql(\n    self,\n    file: str | os.PathLike[str],\n    driver: str | None = None,\n    include: tuple[str, ...] | None = None,\n    alias: dict[str, str] | None = None,\n    limit: int | None = None,\n) -&gt; BenchmarkRecord:\n    if not self._initialized:\n        self.initialize()\n\n    driver = driver or Path(file).suffix.removeprefix(\".\")\n    if driver not in [\"json\", \"csv\", \"parquet\"]:\n        raise NotImplementedError(\"duckdb only supports reading JSON, CSV or parquet files\")\n\n    alias = alias or {}\n    limit = limit or 0\n    if limit &lt; 0:\n        raise ValueError(\"'limit' must be non-negative\")\n\n    if include is None:\n        cols = \"*\"\n    else:\n        cols = \", \".join(i if i not in alias else f\"{i} AS {alias[i]}\" for i in include)\n\n    # TODO: Query support for WHERE\n    query = f\"SELECT {cols} FROM read_json_auto('{str(file)}')\"  # nosec B608\n\n    rel = self.conn.sql(query)\n    columns = rel.columns\n    results = rel.fetchall()\n\n    benchmarks = [dict(zip(columns, r)) for r in results]\n    context = Context()\n    for bm in benchmarks:\n        context.update(bm.pop(\"context\", {}))\n\n    return BenchmarkRecord(context=context, benchmarks=benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.raw_sql","title":"raw_sql","text":"<pre><code>raw_sql(query: str) -&gt; DuckDBPyRelation\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def raw_sql(self, query: str) -&gt; duckdb.DuckDBPyRelation:\n    rel = self.conn.sql(query=query)\n    return rel\n</code></pre>"},{"location":"reference/nnbench/reporter/file/","title":"file","text":""},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.SerDe","title":"SerDe  <code>module-attribute</code>","text":"<pre><code>SerDe = tuple[\n    Callable[[Sequence[BenchmarkRecord], IO, FileDriverOptions], None],\n    Callable[[IO, FileDriverOptions], list[BenchmarkRecord]],\n]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileDriverOptions","title":"FileDriverOptions  <code>dataclass</code>","text":"Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>@dataclass(frozen=True)\nclass FileDriverOptions:\n    options: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Options to pass to the underlying serialization API call, e.g. ``json.dump``.\"\"\"\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\"\n    \"\"\"How to handle the context struct.\"\"\"\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileDriverOptions.options","title":"options  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>options: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Options to pass to the underlying serialization API call, e.g. <code>json.dump</code>.</p>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileDriverOptions.ctxmode","title":"ctxmode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ctxmode: Literal['flatten', 'inline', 'omit'] = 'inline'\n</code></pre> <p>How to handle the context struct.</p>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileIO","title":"FileIO","text":"Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class FileIO:\n    def read(\n        self,\n        file: str | os.PathLike[str],\n        mode: str = \"r\",\n        driver: str | None = None,\n        compression: str | None = None,\n        options: dict[str, Any] | None = None,\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Greedy version of ``FileIO.read_batched()``, returning the first read record.\n        When reading a multi-record file, this uses as much memory as the batched version.\n        \"\"\"\n        records = self.read_batched(\n            file=file, mode=mode, driver=driver, compression=compression, options=options\n        )\n        return records[0]\n\n    def read_batched(\n        self,\n        file: str | os.PathLike[str],\n        mode: str = \"r\",\n        driver: str | None = None,\n        compression: str | None = None,\n        options: dict[str, Any] | None = None,\n    ) -&gt; list[BenchmarkRecord]:\n        \"\"\"\n        Reads a set of benchmark records from the given file path.\n\n        The file driver is chosen based on the extension found on the ``file`` path.\n\n        Parameters\n        ----------\n        file: str | os.PathLike[str]\n            The file name to read from.\n        mode: str\n            File mode to use. Can be any of the modes used in builtin ``open()``.\n        driver: str | None\n            File driver implementation to use. If None, the file driver inferred from the\n            given file path's extension will be used.\n        compression: str | None\n            Compression engine to use. If None, the compression inferred from the given\n            file path's extension will be used.\n        options: dict[str, Any] | None\n            Options to pass to the respective file driver implementation.\n\n        Returns\n        -------\n        list[BenchmarkRecord]\n            The benchmark records contained in the file.\n        \"\"\"\n        fileext = Path(file).suffix.removeprefix(\".\")\n        # if the extension looks like FORMAT.COMPRESSION, we split.\n        if fileext.count(\".\") == 1:\n            # TODO: Are there file extensions with more than one meaningful part?\n            ext_driver, ext_compression = fileext.rsplit(\".\", 1)\n        else:\n            ext_driver, ext_compression = fileext, None\n\n        driver = driver or ext_driver\n        compression = compression or ext_compression\n\n        _, de = get_driver_implementation(driver)\n\n        # canonicalize extension to make sure the file gets it correctly\n        # regardless of where driver and compression came from.\n        fullext = \".\" + driver\n        if compression is not None:\n            fullext += \".\" + compression\n            file = Path(file).with_suffix(fullext)\n            fd = get_compression_algorithm(compression)(file, mode)\n        else:\n            file = Path(file).with_suffix(fullext)\n            fd = open(file, mode)\n\n        # dummy value, since the context mode is unused in read ops.\n        fdoptions = FileDriverOptions(ctxmode=\"omit\", options=options or {})\n\n        with fd as fp:\n            return de(fp, fdoptions)\n\n    def write(\n        self,\n        record: BenchmarkRecord,\n        file: str | os.PathLike[str],\n        mode: str = \"w\",\n        driver: str | None = None,\n        compression: str | None = None,\n        ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n        options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"Greedy version of ``FileIO.write_batched()``\"\"\"\n        self.write_batched(\n            [record],\n            file=file,\n            mode=mode,\n            driver=driver,\n            compression=compression,\n            ctxmode=ctxmode,\n            options=options,\n        )\n\n    def write_batched(\n        self,\n        records: Sequence[BenchmarkRecord],\n        file: str | os.PathLike[str],\n        mode: str = \"w\",\n        driver: str | None = None,\n        compression: str | None = None,\n        ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n        options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Writes a benchmark record to the given file path.\n\n        The file driver is chosen based on the extension found on the ``file`` path.\n\n        Parameters\n        ----------\n        records: Sequence[BenchmarkRecord]\n            The record to write to the database.\n        file: str | os.PathLike[str]\n            The file name to write to.\n        mode: str\n            File mode to use. Can be any of the modes used in builtin ``open()``.\n        driver: str | None\n            File driver implementation to use. If None, the file driver inferred from the\n            given file path's extension will be used.\n        compression: str | None\n            Compression engine to use. If None, the compression inferred from the given\n            file path's extension will be used.\n        ctxmode: Literal[\"flatten\", \"inline\", \"omit\"]\n            How to handle the benchmark context when writing the record data.\n        options: dict[str, Any] | None\n            Options to pass to the respective file driver implementation.\n        \"\"\"\n        fileext = Path(file).suffix.removeprefix(\".\")\n        # if the extension looks like FORMAT.COMPRESSION, we split.\n        if fileext.count(\".\") == 1:\n            ext_driver, ext_compression = fileext.rsplit(\".\", 1)\n        else:\n            ext_driver, ext_compression = fileext, None\n\n        driver = driver or ext_driver\n        compression = compression or ext_compression\n\n        ser, _ = get_driver_implementation(driver)\n\n        # canonicalize extension to make sure the file gets it correctly\n        # regardless of where driver and compression came from.\n        fullext = \".\" + driver\n        if compression is not None:\n            fullext += \".\" + compression\n            file = Path(file).with_suffix(fullext)\n            fd = get_compression_algorithm(compression)(file, mode)\n        else:\n            file = Path(file).with_suffix(fullext)\n            fd = open(file, mode)\n\n        fdoptions = FileDriverOptions(ctxmode=ctxmode, options=options or {})\n        with fd as fp:\n            ser(records, fp, fdoptions)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileIO.read","title":"read","text":"<pre><code>read(\n    file: str | PathLike[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    compression: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; BenchmarkRecord\n</code></pre> <p>Greedy version of <code>FileIO.read_batched()</code>, returning the first read record. When reading a multi-record file, this uses as much memory as the batched version.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(\n    self,\n    file: str | os.PathLike[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    compression: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Greedy version of ``FileIO.read_batched()``, returning the first read record.\n    When reading a multi-record file, this uses as much memory as the batched version.\n    \"\"\"\n    records = self.read_batched(\n        file=file, mode=mode, driver=driver, compression=compression, options=options\n    )\n    return records[0]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileIO.read_batched","title":"read_batched","text":"<pre><code>read_batched(\n    file: str | PathLike[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    compression: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; list[BenchmarkRecord]\n</code></pre> <p>Reads a set of benchmark records from the given file path.</p> <p>The file driver is chosen based on the extension found on the <code>file</code> path.</p> PARAMETER  DESCRIPTION <code>file</code> <p>The file name to read from.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>mode</code> <p>File mode to use. Can be any of the modes used in builtin <code>open()</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'r'</code> </p> <code>driver</code> <p>File driver implementation to use. If None, the file driver inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>compression</code> <p>Compression engine to use. If None, the compression inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Options to pass to the respective file driver implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[BenchmarkRecord]</code> <p>The benchmark records contained in the file.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read_batched(\n    self,\n    file: str | os.PathLike[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    compression: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; list[BenchmarkRecord]:\n    \"\"\"\n    Reads a set of benchmark records from the given file path.\n\n    The file driver is chosen based on the extension found on the ``file`` path.\n\n    Parameters\n    ----------\n    file: str | os.PathLike[str]\n        The file name to read from.\n    mode: str\n        File mode to use. Can be any of the modes used in builtin ``open()``.\n    driver: str | None\n        File driver implementation to use. If None, the file driver inferred from the\n        given file path's extension will be used.\n    compression: str | None\n        Compression engine to use. If None, the compression inferred from the given\n        file path's extension will be used.\n    options: dict[str, Any] | None\n        Options to pass to the respective file driver implementation.\n\n    Returns\n    -------\n    list[BenchmarkRecord]\n        The benchmark records contained in the file.\n    \"\"\"\n    fileext = Path(file).suffix.removeprefix(\".\")\n    # if the extension looks like FORMAT.COMPRESSION, we split.\n    if fileext.count(\".\") == 1:\n        # TODO: Are there file extensions with more than one meaningful part?\n        ext_driver, ext_compression = fileext.rsplit(\".\", 1)\n    else:\n        ext_driver, ext_compression = fileext, None\n\n    driver = driver or ext_driver\n    compression = compression or ext_compression\n\n    _, de = get_driver_implementation(driver)\n\n    # canonicalize extension to make sure the file gets it correctly\n    # regardless of where driver and compression came from.\n    fullext = \".\" + driver\n    if compression is not None:\n        fullext += \".\" + compression\n        file = Path(file).with_suffix(fullext)\n        fd = get_compression_algorithm(compression)(file, mode)\n    else:\n        file = Path(file).with_suffix(fullext)\n        fd = open(file, mode)\n\n    # dummy value, since the context mode is unused in read ops.\n    fdoptions = FileDriverOptions(ctxmode=\"omit\", options=options or {})\n\n    with fd as fp:\n        return de(fp, fdoptions)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileIO.write","title":"write","text":"<pre><code>write(\n    record: BenchmarkRecord,\n    file: str | PathLike[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    compression: str | None = None,\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n    options: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Greedy version of <code>FileIO.write_batched()</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self,\n    record: BenchmarkRecord,\n    file: str | os.PathLike[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    compression: str | None = None,\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n    options: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Greedy version of ``FileIO.write_batched()``\"\"\"\n    self.write_batched(\n        [record],\n        file=file,\n        mode=mode,\n        driver=driver,\n        compression=compression,\n        ctxmode=ctxmode,\n        options=options,\n    )\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileIO.write_batched","title":"write_batched","text":"<pre><code>write_batched(\n    records: Sequence[BenchmarkRecord],\n    file: str | PathLike[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    compression: str | None = None,\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n    options: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Writes a benchmark record to the given file path.</p> <p>The file driver is chosen based on the extension found on the <code>file</code> path.</p> PARAMETER  DESCRIPTION <code>records</code> <p>The record to write to the database.</p> <p> TYPE: <code>Sequence[BenchmarkRecord]</code> </p> <code>file</code> <p>The file name to write to.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>mode</code> <p>File mode to use. Can be any of the modes used in builtin <code>open()</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'w'</code> </p> <code>driver</code> <p>File driver implementation to use. If None, the file driver inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>compression</code> <p>Compression engine to use. If None, the compression inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>ctxmode</code> <p>How to handle the benchmark context when writing the record data.</p> <p> TYPE: <code>Literal['flatten', 'inline', 'omit']</code> DEFAULT: <code>'inline'</code> </p> <code>options</code> <p>Options to pass to the respective file driver implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write_batched(\n    self,\n    records: Sequence[BenchmarkRecord],\n    file: str | os.PathLike[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    compression: str | None = None,\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"] = \"inline\",\n    options: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Writes a benchmark record to the given file path.\n\n    The file driver is chosen based on the extension found on the ``file`` path.\n\n    Parameters\n    ----------\n    records: Sequence[BenchmarkRecord]\n        The record to write to the database.\n    file: str | os.PathLike[str]\n        The file name to write to.\n    mode: str\n        File mode to use. Can be any of the modes used in builtin ``open()``.\n    driver: str | None\n        File driver implementation to use. If None, the file driver inferred from the\n        given file path's extension will be used.\n    compression: str | None\n        Compression engine to use. If None, the compression inferred from the given\n        file path's extension will be used.\n    ctxmode: Literal[\"flatten\", \"inline\", \"omit\"]\n        How to handle the benchmark context when writing the record data.\n    options: dict[str, Any] | None\n        Options to pass to the respective file driver implementation.\n    \"\"\"\n    fileext = Path(file).suffix.removeprefix(\".\")\n    # if the extension looks like FORMAT.COMPRESSION, we split.\n    if fileext.count(\".\") == 1:\n        ext_driver, ext_compression = fileext.rsplit(\".\", 1)\n    else:\n        ext_driver, ext_compression = fileext, None\n\n    driver = driver or ext_driver\n    compression = compression or ext_compression\n\n    ser, _ = get_driver_implementation(driver)\n\n    # canonicalize extension to make sure the file gets it correctly\n    # regardless of where driver and compression came from.\n    fullext = \".\" + driver\n    if compression is not None:\n        fullext += \".\" + compression\n        file = Path(file).with_suffix(fullext)\n        fd = get_compression_algorithm(compression)(file, mode)\n    else:\n        file = Path(file).with_suffix(fullext)\n        fd = open(file, mode)\n\n    fdoptions = FileDriverOptions(ctxmode=ctxmode, options=options or {})\n    with fd as fp:\n        ser(records, fp, fdoptions)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter","title":"FileReporter","text":"<p>             Bases: <code>FileIO</code>, <code>BenchmarkReporter</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class FileReporter(FileIO, BenchmarkReporter):\n    pass\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.yaml_save","title":"yaml_save","text":"<pre><code>yaml_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def yaml_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None:\n    try:\n        import yaml\n    except ImportError:\n        raise ModuleNotFoundError(\"`pyyaml` is not installed\")\n\n    bms = []\n    for r in records:\n        bms += r.compact(mode=fdoptions.ctxmode)\n    yaml.safe_dump(bms, fp, **fdoptions.options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.yaml_load","title":"yaml_load","text":"<pre><code>yaml_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def yaml_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]:\n    try:\n        import yaml\n    except ImportError:\n        raise ModuleNotFoundError(\"`pyyaml` is not installed\")\n\n    # TODO: Use expandmany()\n    bms = yaml.safe_load(fp)\n    return [BenchmarkRecord.expand(bms)]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.json_save","title":"json_save","text":"<pre><code>json_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def json_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None:\n    import json\n\n    bm = []\n    for r in records:\n        bm += r.compact(mode=fdoptions.ctxmode)\n    json.dump(bm, fp, **fdoptions.options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.json_load","title":"json_load","text":"<pre><code>json_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def json_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]:\n    import json\n\n    benchmarks: list[dict[str, Any]] = json.load(fp, **fdoptions.options)\n    return [BenchmarkRecord.expand(benchmarks)]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ndjson_save","title":"ndjson_save","text":"<pre><code>ndjson_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def ndjson_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None:\n    import json\n\n    bm = []\n    for r in records:\n        bm += r.compact(mode=fdoptions.ctxmode)\n    fp.write(\"\\n\".join([json.dumps(b) for b in bm]))\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ndjson_load","title":"ndjson_load","text":"<pre><code>ndjson_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def ndjson_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]:\n    import json\n\n    benchmarks: list[dict[str, Any]]\n    benchmarks = [json.loads(line, **fdoptions.options) for line in fp]\n    return [BenchmarkRecord.expand(benchmarks)]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.csv_save","title":"csv_save","text":"<pre><code>csv_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def csv_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None:\n    import csv\n\n    bm = []\n    for r in records:\n        bm += r.compact(mode=fdoptions.ctxmode)\n    writer = csv.DictWriter(fp, fieldnames=bm[0].keys(), **fdoptions.options)\n\n    for b in bm:\n        writer.writerow(b)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.csv_load","title":"csv_load","text":"<pre><code>csv_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def csv_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]:\n    import csv\n\n    reader = csv.DictReader(fp, **fdoptions.options)\n\n    benchmarks: list[dict[str, Any]] = []\n    # apparently csv.DictReader has no appropriate type hint for __next__,\n    # so we supply one ourselves.\n    bm: dict[str, Any]\n    for bm in reader:\n        benchmarks.append(bm)\n\n    return [BenchmarkRecord.expand(benchmarks)]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.parquet_save","title":"parquet_save","text":"<pre><code>parquet_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def parquet_save(records: Sequence[BenchmarkRecord], fp: IO, fdoptions: FileDriverOptions) -&gt; None:\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n\n    bm = []\n    for r in records:\n        bm += r.compact(mode=fdoptions.ctxmode)\n\n    table = pa.Table.from_pylist(bm)\n    pq.write_table(table, fp, **fdoptions.options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.parquet_load","title":"parquet_load","text":"<pre><code>parquet_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def parquet_load(fp: IO, fdoptions: FileDriverOptions) -&gt; list[BenchmarkRecord]:\n    import pyarrow.parquet as pq\n\n    table = pq.read_table(fp, **fdoptions.options)\n    benchmarks: list[dict[str, Any]] = table.to_pylist()\n    return [BenchmarkRecord.expand(benchmarks)]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_driver_implementation","title":"get_driver_implementation","text":"<pre><code>get_driver_implementation(name: str) -&gt; SerDe\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_driver_implementation(name: str) -&gt; SerDe:\n    try:\n        return _file_drivers[name]\n    except KeyError:\n        raise KeyError(f\"unsupported file format {name!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.register_driver_implementation","title":"register_driver_implementation","text":"<pre><code>register_driver_implementation(name: str, impl: SerDe, clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def register_driver_implementation(name: str, impl: SerDe, clobber: bool = False) -&gt; None:\n    if name in _file_drivers and not clobber:\n        raise RuntimeError(\n            f\"driver {name!r} is already registered. To force registration, \"\n            f\"rerun with clobber=True\"\n        )\n\n    with _file_driver_lock:\n        _file_drivers[name] = impl\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.deregister_driver_implementation","title":"deregister_driver_implementation","text":"<pre><code>deregister_driver_implementation(name: str) -&gt; SerDe | None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def deregister_driver_implementation(name: str) -&gt; SerDe | None:\n    with _file_driver_lock:\n        return _file_drivers.pop(name, None)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.gzip_compression","title":"gzip_compression","text":"<pre><code>gzip_compression(filename: str | PathLike[str], mode: Literal['r', 'w'] = 'r') -&gt; IO\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def gzip_compression(filename: str | os.PathLike[str], mode: Literal[\"r\", \"w\"] = \"r\") -&gt; IO:\n    import gzip\n\n    # gzip.GzipFile does not inherit from IO[bytes],\n    # but it has all required methods, so we allow it.\n    return cast(IO[bytes], gzip.GzipFile(filename=filename, mode=mode))\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.bz2_compression","title":"bz2_compression","text":"<pre><code>bz2_compression(filename: str | PathLike[str], mode: Literal['r', 'w'] = 'r') -&gt; IO\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def bz2_compression(filename: str | os.PathLike[str], mode: Literal[\"r\", \"w\"] = \"r\") -&gt; IO:\n    import bz2\n\n    return bz2.BZ2File(filename=filename, mode=mode)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_compression_algorithm","title":"get_compression_algorithm","text":"<pre><code>get_compression_algorithm(name: str) -&gt; Callable\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_compression_algorithm(name: str) -&gt; Callable:\n    try:\n        return _compressions[name]\n    except KeyError:\n        raise KeyError(f\"unsupported compression algorithm {name!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.register_compression","title":"register_compression","text":"<pre><code>register_compression(name: str, impl: Callable, clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def register_compression(name: str, impl: Callable, clobber: bool = False) -&gt; None:\n    if name in _compressions and not clobber:\n        raise RuntimeError(\n            f\"compression {name!r} is already registered. To force registration, \"\n            f\"rerun with clobber=True\"\n        )\n\n    with _compression_lock:\n        _compressions[name] = impl\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.deregister_compression","title":"deregister_compression","text":"<pre><code>deregister_compression(name: str) -&gt; Callable | None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def deregister_compression(name: str) -&gt; Callable | None:\n    with _compression_lock:\n        return _compressions.pop(name, None)\n</code></pre>"},{"location":"reference/nnbench/reporter/util/","title":"util","text":""},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.nullcols","title":"nullcols","text":"<pre><code>nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]\n</code></pre> <p>Extracts columns that only contain false-ish data from a list of benchmarks.</p> <p>Since this data is most often not interesting, the result of this can be used to filter out these columns from the benchmark dictionaries.</p> PARAMETER  DESCRIPTION <code>_benchmarks</code> <p>The benchmarks to filter.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>set[str]</code> <p>Set of the columns (key names) that only contain false-ish values across all benchmarks.</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]:\n    \"\"\"\n    Extracts columns that only contain false-ish data from a list of benchmarks.\n\n    Since this data is most often not interesting, the result of this\n    can be used to filter out these columns from the benchmark dictionaries.\n\n    Parameters\n    ----------\n    _benchmarks: list[dict[str, Any]]\n        The benchmarks to filter.\n\n    Returns\n    -------\n    set[str]\n        Set of the columns (key names) that only contain false-ish values\n        across all benchmarks.\n    \"\"\"\n    nulls: dict[str, bool] = collections.defaultdict(bool)\n    for bm in _benchmarks:\n        for k, v in bm.items():\n            nulls[k] = nulls[k] or bool(v)\n    return set(k for k, v in nulls.items() if not v)\n</code></pre>"},{"location":"tutorials/","title":"Examples","text":"<p>This page showcases some examples of applications for nnbench. Click any of the links below for inspiration on how to use nnbench in your projects.</p> <ul> <li>How to integrate nnbench into an existing ML pipeline</li> <li>How to integrate nnbench with workflow orchestrators</li> </ul>"},{"location":"tutorials/bq/","title":"Streaming benchmarks to a cloud database","text":"<p>Once you obtain the results of your benchmarks, you will most likely want to store them somewhere. Whether that is in storage as flat files, on a server, or in a database, <code>nnbench</code> allows you to write records anywhere, provided the destination supports JSON.</p> <p>This is a small guide containing a snippet on how to stream benchmark results to a Google Cloud BigQuery table.</p>"},{"location":"tutorials/bq/#the-benchmarks","title":"The benchmarks","text":"<p>Configure your benchmarks as normal, for example by separating them into a Python file. The following is a very simple example benchmark setup.</p> <pre><code>import nnbench\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\n@nnbench.benchmark\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre>"},{"location":"tutorials/bq/#setting-up-a-bigquery-client","title":"Setting up a BigQuery client","text":"<p>In order to authenticate with BigQuery, follow the official Google Cloud documentation. In this case, we rely on Application Default Credentials (ADC), which can be configured with the <code>gcloud</code> CLI.</p> <p>To interact with BigQuery from Python, the <code>google-cloud-bigquery</code> package has to be installed. You can do this e.g. using pip via <code>pip install --upgrade google-cloud-bigquery</code>.</p>"},{"location":"tutorials/bq/#creating-a-table","title":"Creating a table","text":"<p>Within your configured project, proceed by creating a destination table to write the benchmarks to. Consider the BigQuery Python documentation on tables for how to create a table programmatically.</p> <p>Note</p> <p>If the configured dataset does not exist, you will have to create it as well, either programmatically via the <code>bigquery.Client.create_dataset</code> API or in the Google Cloud console.</p>"},{"location":"tutorials/bq/#using-bigquerys-schema-auto-detection","title":"Using BigQuery's schema auto-detection","text":"<p>In order to skip tedious schema inference by hand, we can use BigQuery's schema auto-detection from JSON records. All we have to do is configure a BigQuery load job to auto-detect the schema from the Python dictionaries in memory:</p> <pre><code>    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n</code></pre> <p>After that, write and stream the compacted benchmark record directly to your destination table. In this example, we decide to flatten the benchmark context to be able to extract scalar context values directly from the result table using raw SQL queries. Note that you have to use a custom separator (an underscore <code>\"_\"</code> in this case) for the context data, since BigQuery does not allow dots in column names.</p> <pre><code>        res.compact(mode=\"flatten\", sep=\"_\"), table_id, job_config=job_config\n    )\n    load_job.result()\n</code></pre> <p>Tip</p> <p>If you would like to save the context dictionary as a struct instead, use <code>mode = \"inline\"</code> in the call to <code>BenchmarkRecord.compact()</code>.</p> <p>And that's all! To check that the records appear as expected, you can now query the data e.g. like so:</p> <pre><code># check that the insert worked.\nquery = f'SELECT name, value, time_ns, git_commit AS commit FROM {table_id}'\nr = client.query(query)\nfor row in r.result():\n    print(r)\n</code></pre>"},{"location":"tutorials/bq/#recap-and-the-full-source-code","title":"Recap and the full source code","text":"<p>In this tutorial, we</p> <p>1) defined and ran a benchmark workload using <code>nnbench</code>. 2) configured a Google Cloud BigQuery client and a load job to insert benchmark records into a table, and 3) inserted the records into the destination table.</p> <p>The full source code for this tutorial is included below, and also in the nnbench repository.</p> <pre><code>from google.cloud import bigquery\n\nimport nnbench\nfrom nnbench.context import GitEnvironmentInfo\n\n\ndef main():\n    client = bigquery.Client()\n\n    # TODO: Fill these out with your appropriate resource names.\n    table_id = \"&lt;PROJECT&gt;.&lt;DATASET&gt;.&lt;TABLE&gt;\"\n\n    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n\n    runner = nnbench.BenchmarkRunner()\n    res = runner.run(\"benchmarks.py\", params={\"a\": 1, \"b\": 1}, context=(GitEnvironmentInfo(),))\n\n    load_job = client.load_table_from_json(\n        res.compact(mode=\"flatten\", sep=\"_\"), table_id, job_config=job_config\n    )\n    load_job.result()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/mnist/","title":"Integrating nnbench into an existing ML pipeline","text":"<p>Thanks to nnbench's modularity, we can easily integrate it into existing ML experiment code.</p> <p>As an example, we use an MNIST pipeline written for the popular ML framework JAX. While the actual data sourcing and training code is interesting on its own, we focus solely on the nnbench application part. You can find the full example code in the nnbench repository.</p>"},{"location":"tutorials/mnist/#defining-and-organizing-benchmarks","title":"Defining and organizing benchmarks","text":"<p>To properly structure our project, we avoid mixing training pipeline code and benchmark code by placing all benchmarks in a standalone file, similarly to how you might structure unit tests for your code.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom mnist import ArrayMapping, ConvNet\n\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(params: ArrayMapping, data: ArrayMapping) -&gt; float:\n    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n    cn = ConvNet()\n    y_pred = cn.apply({\"params\": params}, x_test)\n    return jnp.mean(jnp.argmax(y_pred, -1) == y_test).item()\n\n\n@nnbench.benchmark(name=\"Model size (MB)\")\ndef modelsize(params: ArrayMapping) -&gt; float:\n    nbytes = sum(x.size * x.dtype.itemsize for x in jax.tree_util.tree_leaves(params))\n    return nbytes / 1e6\n</code></pre> <p>This definition is short and sweet, and contains a few important details:</p> <ul> <li>Both functions are given the <code>@nnbench.benchmark</code> decorator - this enables our runner to find and collect them before starting the benchmark run.</li> <li>The <code>modelsize</code> benchmark is given a custom name (<code>\"Model size (MB)\"</code>), indicating that the resulting number is the combined size of the model weights in megabytes. This is done for display purposes, to improve interpretability when reporting results.</li> <li>The <code>params</code> argument is the same in both benchmarks, both in name and type. This is important, since it ensures that both benchmarks will be run with the same model weights.</li> </ul> <p>That's all - now we can shift over to our main pipeline code and see what is necessary to execute the benchmarks and visualize the results.</p>"},{"location":"tutorials/mnist/#setting-up-a-benchmark-runner-and-parameters","title":"Setting up a benchmark runner and parameters","text":"<p>After finishing the benchmark setup, we only need a few more lines to augment our pipeline.</p> <p>We assume that the benchmark file is located in the same folder as the training pipeline - thus, we can specify our parent directory as the place in which to search for benchmarks:</p> <p>Next, we can define a custom subclass of <code>nnbench.Parameters</code> to hold our benchmark parameters. Benchmark parameters are a set of variables used as inputs to the benchmark functions collected during the benchmark run.</p> <p>Since our benchmarks above are parametrized by the model weights (named <code>params</code> in the function signatures) and the MNIST data split (called <code>data</code>), we define our parameters to take exactly these two values.</p> <pre><code>class MNISTTestParameters(nnbench.Parameters):\n    params: Mapping[str, jax.Array]\n    data: ArrayMapping\n</code></pre> <p>And that's it! After we implement all training code, we just run nnbench directly after training in our top-level pipeline function:</p> <pre><code>    \"\"\"Load MNIST data and train a simple ConvNet model.\"\"\"\n    mnist = load_mnist()\n    mnist = preprocess(mnist)\n    state, data = train(mnist)\n\n    # the nnbench portion.\n    runner = nnbench.BenchmarkRunner()\n    reporter = nnbench.reporter.FileReporter()\n    params = MNISTTestParameters(params=state.params, data=data)\n    result = runner.run(HERE, params=params)\n    reporter.write(result, \"result.json\")\n</code></pre> <p>We use the <code>BenchmarkReporter</code> to print the results directly to the terminal in a table. Notice how by we can reuse the training artifacts in nnbench as parameters to obtain results right after training!</p> <p>The output might look like this:</p> <pre><code>name               value\n---------------  -------\naccuracy         0.9712\nModel size (MB)  3.29783\n</code></pre> <p>This can be improved in a number of ways - for example by enriching it with metadata about the model architecture, the used GPU, etc. For more information on how to supply context to benchmarks, check the user guide section.</p>"},{"location":"tutorials/prefect/","title":"Integrating nnbench with Prefect","text":"<p>If you have more complex workflows it is sensible to use a workflow orchestration tool to manage them.  Benchmarking with nnbench can be integrated with orchestrators. We will present an example integration with Prefect. We will explain the orchestration concepts in a high level and link to the corresponding parts of the  Prefect docs. The full example code can be found in the nnbench repository.</p> <p>In this example we want to orchestrate the training and benchmarking of a linear regression model.</p>"},{"location":"tutorials/prefect/#project-structure","title":"Project Structure","text":""},{"location":"tutorials/prefect/#defining-the-training-tasks-and-workflows","title":"Defining the training tasks and workflows","text":"<p>We recommend to separate the training and benchmarking logic. </p> <pre><code>from __future__ import annotations\n\nimport numpy as np\nfrom prefect import flow, task\nfrom sklearn import base\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\n@task\ndef make_regression_data(\n    random_state: int, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    X, y = make_regression(\n        n_samples=n_samples, n_features=n_features, noise=noise, random_state=random_state\n    )\n    return X, y\n\n\n@task\ndef make_train_test_split(\n    X: np.ndarray, y: np.ndarray, random_state: int, test_size: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    return X_train, y_train, X_test, y_test\n\n\n@task\ndef train_linear_regression(X: np.ndarray, y: np.ndarray) -&gt; base.BaseEstimator:\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\n\n@flow\ndef prepare_regression_data(\n    random_state: int = 42, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, ...]:\n    X, y = make_regression_data(\n        random_state=random_state, n_samples=n_samples, n_features=n_features, noise=noise\n    )\n    X_train, y_train, X_test, y_test = make_train_test_split(X=X, y=y, random_state=random_state)\n    return X_train, y_train, X_test, y_test\n\n\n@flow\nasync def prepare_regressor_and_test_data(\n    data_params: dict[str, int | float] | None = None,\n) -&gt; tuple[base.BaseEstimator, np.ndarray, np.ndarray]:\n    if data_params is None:\n        data_params = {}\n    X_train, y_train, X_test, y_test = prepare_regression_data(**data_params)\n    model = train_linear_regression(X=X_train, y=y_train)\n    return model, X_test, y_test\n</code></pre> <p>The <code>training.py</code> file contains functions to generate synthetic data for our regression model, facilitate a train-test-split, and finally train the regression model. We have applied Prefect's <code>@task</code> decorator.. which marks the contained logic as a discrete unit of work for Prefect.  Two other functions prepare the regression data and train the estimator.  They are labeled with the <code>@flow</code> decorator. that labels the function as a workflow that can depend on other flows or tasks. The <code>prepare_regressor_and_test_data</code> function returns the model and test data so that we can use it in our benchmarks.</p>"},{"location":"tutorials/prefect/#defining-benchmarks","title":"Defining Benchmarks","text":"<p>The benchmarks are in the <code>benchmark.py</code> file. We have two functions to calculate the mean absolute error and the mean squared error. These benchmarks are tagged to indicate they are metrics. Another two benchmarks calculate calculate information about the model, namely the inference time and size of the model. The last two functions serve to investigate the test dataset.</p> <pre><code>import pickle\nimport sys\nimport time\n\nimport numpy as np\nfrom sklearn import base, metrics\n\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mae(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_absolute_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mse(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_squared_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(name=\"Model size (bytes)\", tags=(\"model-meta\",))\ndef modelsize(model: base.BaseEstimator) -&gt; int:\n    model_bytes = pickle.dumps(model)\n    return sys.getsizeof(model_bytes)\n\n\n@nnbench.benchmark(name=\"Inference time (s)\", tags=(\"model-meta\",))\ndef inference_time(model: base.BaseEstimator, X: np.ndarray, n_iter: int = 100) -&gt; float:\n    start = time.perf_counter()\n    for i in range(n_iter):\n        _ = model.predict(X)\n    end = time.perf_counter()\n    return (end - start) / n_iter\n</code></pre> <p>We did not apply any Prefect decorators here, as we will assign <code>@task</code>s - Prefects smallest unit of work - to run a benchmark family.</p>"},{"location":"tutorials/prefect/#defining-benchmark-runners","title":"Defining Benchmark runners.","text":"<p>In the <code>runners.py</code> file, we define the logic to run our benchmarks. The runner collects the benchmarks from the specified file.  We can filter by tags and use this to define two separate tasks, one to run the metrics and the other to run the metadata benchmarks. We have applied the <code>@task</code> decorator to these functions.</p> <pre><code>@task\ndef run_metric_benchmarks(\n    model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray\n) -&gt; nnbench.types.BenchmarkRecord:\n    runner = nnbench.BenchmarkRunner()\n    results = runner.run(\n        os.path.join(dir_path, \"benchmark.py\"),\n        tags=(\"metric\",),\n        params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test},\n    )\n    return results\n\n\n@task\ndef run_metadata_benchmarks(\n    model: base.BaseEstimator, X: np.ndarray\n) -&gt; nnbench.types.BenchmarkRecord:\n    runner = nnbench.BenchmarkRunner()\n    result = runner.run(\n        os.path.join(dir_path, \"benchmark.py\"),\n        tags=(\"model-meta\",),\n        params={\"model\": model, \"X\": X},\n    )\n    return result\n</code></pre> <p>We have also defined a basic reporter that we will use to save the benchmark results with Prefect's artifact storage machinery.</p> <p><pre><code>class PrefectReporter(reporter.BenchmarkReporter):\n    def __init__(self):\n        self.logger = get_run_logger()\n\n    async def write(\n        self, record: types.BenchmarkRecord, key: str, description: str = \"Benchmark and Context\"\n    ) -&gt; None:\n        await create_table_artifact(\n            key=key,\n            table=record.compact(mode=\"flatten\"),\n            description=description,\n        )\n</code></pre> In a real-world scenario, we would report to a database and use a dedicated frontend to look at the benchmark results. But logging will suffice as we are only discussing integration with orchestrators here.</p> <p>A final compound flow executes the model training, obtains the test set and supplies it to the benchmarks we defined earlier.</p> <pre><code>@flow(persist_result=True)\nasync def train_and_benchmark(\n    data_params: dict[str, int | float] | None = None,\n) -&gt; tuple[types.BenchmarkRecord, ...]:\n    if data_params is None:\n        data_params = {}\n\n    reporter = PrefectReporter()\n\n    regressor_and_test_data: tuple[\n        base.BaseEstimator, np.ndarray, np.ndarray\n    ] = await training.prepare_regressor_and_test_data(data_params=data_params)  # type: ignore\n\n    model = regressor_and_test_data[0]\n    X_test = regressor_and_test_data[1]\n    y_test = regressor_and_test_data[2]\n\n    metadata_results: types.BenchmarkRecord = run_metadata_benchmarks(model=model, X=X_test)\n\n    metadata_results.context.update(data_params)\n    metadata_results.context.update(context.PythonInfo())\n\n    await reporter.write(\n        record=metadata_results, key=\"model-attributes\", description=\"Model Attributes\"\n    )\n\n    metric_results: types.BenchmarkRecord = run_metric_benchmarks(\n        model=model, X_test=X_test, y_test=y_test\n    )\n\n    metric_results.context.update(data_params)\n    metric_results.context.update(context.PythonInfo())\n    await reporter.write(metric_results, key=\"model-performance\", description=\"Model Performance\")\n    return metadata_results, metric_results\n</code></pre> <p>The final lines in the <code>runner.py</code> serve the <code>train_and_benchmark</code> function to make it available to Prefect for execution.</p> <pre><code>if __name__ == \"__main__\":\n    asyncio.run(train_and_benchmark.serve(name=\"benchmark-runner\"))\n</code></pre>"},{"location":"tutorials/prefect/#running-prefect","title":"Running Prefect","text":"<p>To run Prefect we have to do several things. First, we have to make sure it is installed. You can use <code>pip install -U prefect</code>. Then we have to run a Prefect server using <code>prefect server start</code>. We make our benchmark flows available by executing it, <code>python runner.py</code>. This enables us to now order an execution with the following command: <code>prefect deployment run 'train-and-benchmark/benchmark-runner'</code>. The command should also be displayed in the output of the <code>runner.py</code> execution.</p> <p>Now we can visit the local Prefect dashboard. By default it is on <code>localhost:4200</code>.  Here we see the executed tasks and workflows.</p> <p></p> <p>If we navigate to the \"Flow Runs\" tab we see more details of the flow runs.</p> <p></p> <p>In the \"Deployments\" tab you see all deployed flows. Currently, there is only our <code>train_and_benchmark</code> flow under the <code>benchmark-runner</code> name. We can trigger a custom execution of workflows in the menu behind the three dots.</p> <p></p> <p>You find the results of the benchmarks when visiting the \"Artifacts\" tab or by navigating to the \"Artifacts\" section of a specific flow execution.</p> <p>As you can see, the nnbench is easily integrated with workflow orchestrators by simply registering the execution of a benchmark runner as a task in the orchestrator.</p> <p>For more functionality of Prefect, you can check out their documentation. </p>"},{"location":"tutorials/streamlit/","title":"Streamlit","text":""},{"location":"tutorials/streamlit/#integrating-nnbench-with-streamlit-and-prefect","title":"Integrating nnbench with Streamlit and Prefect","text":"<p>In a project you may want to execute benchmarks or investigate their results with a dedicated frontend. There exist several frameworks that can help you setting up a frontend quickly. For example Streamlit, Gradio, Dash, or you could roll your own implementation using a backend framework such as Flask. In this guide we will use Streamlit and integrate it with the orchestration setup we've developed with Prefect. That guide is a prerequisite for this one.  The full example code can be found in the nnbench repository.</p>"},{"location":"tutorials/streamlit/#the-streamlit-ui","title":"The Streamlit UI","text":"<p>The Streamlit UI is launched by executing  <pre><code>streamlit run streamlit_example.py\n</code></pre> and initially looks like this: </p> <p>The user interface is assembled in the final part of <code>streamlit_example.py</code>.</p> <pre><code>\n</code></pre> <p>The user inputs are generated via the custom <code>setup_ui()</code> function which then processes the input values once the \"Run Benchmark\" button is clicked.</p> <pre><code>\n</code></pre> <p>We use a session state to keep track of all the benchmarks we ran in the current session which then are displayed within expander elements at the bottom.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#integrating-prefect-with-streamlit","title":"Integrating Prefect with Streamlit","text":"<p>To integrate Streamlit with Prefect, we have to do some initial housekeeping. Namely, we specify the URL for the <code>PreFectClient</code> as well as the storage location of run artifacts where we retrieve the benchmark results from.</p> <pre><code>\n</code></pre> <p>In this example there is no direct integration of Streamlit with nnbench, but all interactions are passing through Prefect to make use of its orchestration benefits such as caching of tasks. Another thing to note is that we are working with local instances for easier reproducibility of this example. Adapting it to work with a remote orchestration server and object storage should be straightforward.</p> <p>The main interaction of the Streamlit frontend with Prefect takes place in the <code>run_bms</code> and <code>get_bm_artifacts</code> functions.</p> <p>The former searches for a Prefect deployment <code>\"train-and-benchmark/benchmark-runner\"</code> and executes it with the benchmark parameters specified by the user. It returns the <code>storage_key</code>, which we use to retrieve the persisted benchmark results.</p> <pre><code>\n</code></pre> <p>The <code>get_bm_artifacts</code> function gets a storage key and retrieves the corresponding results. As the results are stored in raw bytes, we have some logic to reconstruct the <code>nnbench.types.BenchmarkRecord</code> object. We transform the data into Pandas <code>DataFrame</code>s, which are later processed by Streamlit to display the results in tables.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#running-the-example","title":"Running the example","text":"<p>To run the example, we have to do several things.  First, we need to start Prefect using <code>prefect server start</code> in the command line.  Next, we need to make the <code>\"train-and-benchmark/benchmark-runner\"</code> deployment available. We do so by running the corresponding Python file, <code>python runner.py</code>. You find that file in the <code>examples/prefect/src</code> directory.  If you are recreating this example on your machine, make sure you have the full contents of the <code>prefect</code> directory available in addition to the <code>streamlit_example.py</code>. For more information, you can look into the Prefect Guide.</p> <p>Now that Prefect is set up, you can launch a local instance of Streamlit with <code>streamlit run streamlit_example.py</code>.</p> <p>For more information on how to work with Streamlit, visit their docs.</p>"}]}