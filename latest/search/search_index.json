{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Examples</p><p>Examples on how to use nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. See the user guides (TODO: Add guides) at the bottom of this page for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark()\ndef accuracy(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, using the <code>ConsoleReporter</code> we report the resulting accuracy metric by printing it to the terminal in a table.</p> <pre><code>import nnbench\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = nnbench.BenchmarkRunner()\nreporter = ConsoleReporter()\n\n# To collect in the current file, pass \"__main__\" as module name.\nresult = r.run(\"__main__\", params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test})\nreporter.write(result)\n</code></pre> <p>The resulting output might look like this:</p> <pre><code>python benchmarks.py  \n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"guides/benchmarks/","title":"Defining benchmarks with decorators","text":"<p>To benchmark your machine learning code in nnbench, define your key metrics in Python functions and apply one of the provided decorators. The available decorators are  - <code>@nnbench.benchmark</code>, which runs a benchmark with supplied parameters, - <code>@nnbench.parametrize</code>, which runs several benchmarks with the supplied parameter configurations, - <code>@nnbench.product</code>, which runs benchmarks with all parameter combinations that arise from the supplied values. </p> <p>First we introduce a small machine learning example which we will subsequently use to motivate the use of the three benchmark decorators.</p> <p>We recommend to split the model training, benchmark definition, and benchmark running into different files. In this guide, these are called <code>training.py</code>, <code>benchmarks.py</code>, and <code>main.py</code>.</p>"},{"location":"guides/benchmarks/#example","title":"Example","text":"<p>Let us consider an example where we want to evaluate a <code>scikit-learn</code> random forest classifier on the Iris dataset. For this purpose, we will define several helper functions inside a file, <code>training.py</code>. We use <code>prepare_data()</code>, to load the dataset,  <code>train_rf()</code> to train a random forest model with the specified parameters, and <code>accuracy()</code> to calculate the accuracy of the supplied model on the given dataset.</p> <pre><code># training.py\nimport numpy as np\nfrom sklearn import base, metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_data() -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    data = load_iris()\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    return X_train, X_test, y_train, y_test\n\n\ndef train_rf(X_train: np.ndarray, y_train: np.ndarray, n_estimators: int, max_depth: int, random_state: int = 42) -&gt; RandomForestClassifier:\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre>"},{"location":"guides/benchmarks/#benchmark-for-single-benchmarks","title":"<code>@benchmark</code> for single benchmarks","text":"<p>Now, we define our benchmarks in a new file called <code>benchmarks.py</code>. We first encapsulate the benchmark logic into a function, <code>benchmark_accuracy()</code> which prepares the data, trains a classifier, and lastly, obtains the accuracy. To mark such a function as a benchmark, we apply the <code>@benchmark</code> decorator.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.benchmark()\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Warning</p> <p>This training benchmark is designed as a local, simple, and self-contained example to showcase nnbench.  In a real world scenario, to follow best practices, you may want to separate the data preparation and model training steps from the benchmarking logic and pass the corresponding artifacts as a parameter to the benchmark. See the user guide for more information.</p> <p>Lastly, we set up a benchmark runner in the <code>main.py</code>. There, we supply the parameters (<code>n_estimators</code>, <code>max_depth</code>, <code>random_state</code>) necessary in the function definition as a dictionary to the <code>params</code> keyword argument. </p> <pre><code># main.py\nimport nnbench\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = nnbench.BenchmarkRunner()\nreporter = ConsoleReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42})\nreporter.report(result)\n</code></pre> <p>When we execute the <code>main.py</code> we get the following output:</p> <pre><code>python main.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchparametrize-for-multiple-configuration-benchmarks","title":"<code>@nnbench.parametrize</code> for multiple configuration benchmarks","text":"<p>Sometimes, we are not only interested in the performance of a model for given parameters but want to compare the performance for different configurations.  To achieve this, we can turn our single accuracy benchmark in the <code>benchmarks.py</code> file into a parametrized benchmark. To do this, replace the decorator with <code>@nnbench.parametrize</code> and supply the parameter combinations of choice as dictionaries in the first argument.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.parametrize(\n    ({\"n_estimators\": 10, \"max_depth\": 2},\n    {\"n_estimators\": 50, \"max_depth\": 5},\n    {\"n_estimators\": 100, \"max_depth\": 10})\n)\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Notice that the parametrization is still incomplete, as we did not supply a <code>random_state</code> argument. The unfilled arguments are given in <code>BenchmarkRunner.run()</code> via a dictionary passed as the <code>params</code> keyword argument.</p> <pre><code># main.py\nimport nnbench\nfrom nnbench.reporter import ConsoleReporter\n\n\nr = nnbench.BenchmarkRunner()\nreporter = ConsoleReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"random_state\": 42})\nreporter.report(result)\n</code></pre> <p>Executing the parametrized benchmark, we get an output similar to this:</p> <pre><code>python main.py  \n\n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.955556\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.866667\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.911111\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchproduct-for-benchmarks-over-parameter-configuration-grids","title":"<code>@nnbench.product</code> for benchmarks over parameter configuration grids","text":"<p>In case we want to run a benchmark scan for all possible combinations of a set of parameters, we can use the <code>@nnbench.product</code> decorator to supply the different values for each parameter.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.product(n_estimators=[10, 50, 100], max_depth=[2, 5, 10])\ndef benchmark_accuracy_product(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>We still provide the <code>random_state</code> parameter to the runner directly, like we did with the <code>@nnbench.parametrize</code> decorator. By executing the benchmark, we get results for all combinations of <code>n_estimators</code> and <code>max_depth</code>. It looks similar to this:</p> <pre><code>python main.py  \n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=10_max_depth=5    0.955556\nbenchmark_accuracy_n_estimators=10_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=50_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.911111\nbenchmark_accuracy_n_estimators=50_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=100_max_depth=2   0.933333\nbenchmark_accuracy_n_estimators=100_max_depth=5   0.955556\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.955556\n</code></pre>"},{"location":"guides/customization/","title":"Defining setup/teardown tasks, context, and <code>nnbench.Parameters</code>","text":"<p>This page introduces some customization options for benchmark runs. These options can be helpful for tasks surrounding benchmark state management, such as automatic setup and cleanup, contextualizing results with context values, and defining typed parameters with the <code>nnbench.Parameters</code> class.</p>"},{"location":"guides/customization/#defining-setup-and-teardown-tasks","title":"Defining setup and teardown tasks","text":"<p>For some benchmarks, it is important to set certain configuration values and prepare the execution environment before running. To do this, you can pass a setup task to all of the nnbench decorators via the <code>setUp</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\n@nnbench.benchmark(setUp=set_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Similarly, to revert the environment state back to its previous form (or clean up any created resources), you can supply a finalization task with the <code>tearDown</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\ndef pop_envvar(**params):\n    os.environ.pop(\"MY_ENV\")\n\n\n@nnbench.benchmark(setUp=set_envvar, tearDown=pop_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Both the setup and teardown task must take the exact same set of parameters as the benchmark function. To simplify function declaration, it is easiest to use a variadic keyword-only interface, i.e. <code>setup(**kwargs)</code>, as shown.</p> <p>Tip</p> <p>This facility works exactly the same for the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators. There, the specified setup and teardown tasks are run once before or after each of the resulting benchmarks respectively.</p>"},{"location":"guides/customization/#enriching-benchmark-metadata-with-context-values","title":"Enriching benchmark metadata with context values","text":"<p>It is often useful to log specific environment metadata in addition to the benchmark's target metrics. Such metadata can give a clearer picture of how certain models perform on a given hardware, how model architectures compare in performance, and much more. In <code>nnbench</code>, you can give additional metadata to your benchmarks as context values.</p> <p>A context value is defined here as a key-value pair where <code>key</code> is a string, and <code>value</code> is any valid JSON value holding the desired information. As an example, the context value <code>{\"cpuarch\": \"arm64\"}</code> gives information about the CPU architecture of the host machine running the benchmark.</p> <p>A context provider is a function taking no arguments and returning a Python dictionary of context values. The following is a basic example of a context provider:</p> <pre><code>import platform\n\ndef platinfo() -&gt; dict[str, str]:\n    \"\"\"Returns CPU arch, system name (Windows/Linux/Darwin), and Python version.\"\"\"\n    return {\n        \"system\": platform.system(),\n        \"cpuarch\": platform.machine(),\n        \"python_version\": platform.python_version(),\n    }\n</code></pre> <p>To supply context to your benchmarks, you can give a sequence of context providers to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\n# uses the `platinfo` context provider from above to log platform metadata.\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params={}, context=[platinfo])\n</code></pre>"},{"location":"guides/customization/#being-type-safe-by-using-nnbenchparameters","title":"Being type safe by using <code>nnbench.Parameters</code>","text":"<p>Instead of specifying your benchmark's parameters by using a raw Python dictionary, you can define a custom subclass of <code>nnbench.Parameters</code>:</p> <pre><code>import nnbench\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MyParams(nnbench.Parameters):\n    a: int\n    b: int\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\nparams = MyParams(a=1, b=2)\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params=params)\n</code></pre> <p>While this does not have a concrete advantage in terms of type safety over a raw dictionary (all inputs will be checked against the types expected from the benchmark interfaces), it guards against accidental modification of parameters breaking reproducibility.</p>"},{"location":"guides/organization/","title":"How to efficiently organize benchmark code","text":"<p>To efficiently organize benchmarks and keeping your setup modular, you can follow a few guidelines.</p>"},{"location":"guides/organization/#tip-1-separate-benchmarks-from-project-code","title":"Tip 1: Separate benchmarks from project code","text":"<p>This tip is well known from other software development practices such as unit testing. To improve project organization, consider splitting off your benchmarks into their own modules or even directories, if you have multiple benchmark workloads.</p> <p>An example project layout can look like this, with benchmarks as a separate directory at the top-level:</p> <pre><code>my-project/\n\u251c\u2500\u2500 benchmarks/ # &lt;- contains all benchmarking Python files.\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre> <p>This keeps the benchmarks neatly grouped together while siloing them away from the actual project code. Since you will most likely not run your benchmarks in a production setting, this is also advantageous for packaging, as the <code>benchmarks/</code> directory does not ship by default in this configuration.</p>"},{"location":"guides/organization/#tip-2-group-benchmarks-by-common-attributes","title":"Tip 2: Group benchmarks by common attributes","text":"<p>To maintain good organization within your benchmark directory, you can group similar benchmarks into their own Python files. As an example, if you have a set of benchmarks to establish data quality, and benchmarks for scoring trained models on curated data, you could structure them as follows:</p> <pre><code>benchmarks/\n\u251c\u2500\u2500 data_quality.py\n\u251c\u2500\u2500 model_perf.py\n\u2514\u2500\u2500 ...\n</code></pre> <p>This is helpful when running multiple benchmark workloads separately, as you can just point your benchmark runner to each of these separate files:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\ndata_metrics = runner.run(\"benchmarks/data_quality.py\", params=...)\n# same for model metrics, where instead you pass benchmarks/model_perf.py.\nmodel_metrics = runner.run(\"benchmarks/model_perf.py\", params=...)\n</code></pre>"},{"location":"guides/organization/#tip-3-attach-tags-to-benchmarks-for-selective-filtering","title":"Tip 3: Attach tags to benchmarks for selective filtering","text":"<p>For structuring benchmarks within files, you can also use tags, which are tuples of strings attached to a benchmark:</p> <pre><code># benchmarks/data_quality.py\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo1(data) -&gt; float:\n    ...\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo2(data) -&gt; int:\n    ...\n\n\n@nnbench.benchmark(tags=(\"bar\",))\ndef bar(data) -&gt; int:\n    ...\n</code></pre> <p>Now, to only run data quality benchmarks marked \"foo\", pass the corresponding tag to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\nfoo_data_metrics = runner.run(\"benchmarks/data_quality.py\", params=..., tags=(\"foo\",))\n</code></pre> <p>Tip</p> <p>This concept works exactly the same when creating benchmarks with the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators.</p>"},{"location":"guides/runners/","title":"Collecting and running benchmarks","text":"<p>nnbench provides the <code>BenchmarkRunner</code> as a compact interface to collect and run benchmarks selectively.</p>"},{"location":"guides/runners/#the-abstract-benchmarkrunner-class","title":"The abstract <code>BenchmarkRunner</code>  class","text":"<p>Let's first instantiate and then walk through the base class.</p> <pre><code>from nnbench import BenchmarkRunner\n\nrunner = BenchmarkRunner()\n</code></pre> <p>Use the <code>BenchmarkRunner.collect()</code> method to collect benchmarks from files or directories. Assume we have the following benchmark setup: <pre><code># dir_a/bm1.py\nimport nnbench\n\n@nnbench.benchmark\ndef dummy_benchmark(a: int) -&gt; int:\n    return a\n</code></pre></p> <pre><code># dir_b/bm2.py\nimport nnbench\n\n@nnbench.benchmark(tags=(\"tag\",))\ndef another_benchmark(b: int) -&gt; int:\n    return b\n\n@nnbench.benchmark\ndef yet_another_benchmark(c: int) -&gt; int:\n    return c\n</code></pre> <pre><code># dir_b/bm3.py\nimport nnbench\n@nnbench.benchmark(tags=(\"tag\",))\ndef the_last_benchmark(d: int) -&gt; int:\n    return d\n</code></pre> <p>Now we can collect benchmarks from files:</p> <p><pre><code>runner.collect('dir_a/bm1.py')\n</code></pre> Or directories:</p> <pre><code>runner.collect('dir_b')\n</code></pre> <p>This collection can happen iteratively. So, after executing the two collections our runner has all four benchmarks ready for execution.</p> <p>To remove the collected benchmarks again, use the <code>BenchmarkRunner.clear()</code> method. You can also supply tags to the runner to selectively collect only benchmarks with the appropriate tag. For example, after clearing the runner again, you can collect all benchmarks with the <code>\"tag\"</code> tag as such:</p> <pre><code>runner.collect('dir_b', tags=(\"tag\",))\n</code></pre> <p>To run the benchmarks, call the <code>BenchmarkRunner.run()</code> method and supply the necessary parameters required by the collected benchmarks.</p> <pre><code>runner.run(\"dir_b\", params={\"b\": 1, \"c\": 2, \"d\": 3})\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>context</li> <li>core</li> <li>reporter<ul> <li>base</li> <li>console</li> <li>util</li> </ul> </li> <li>runner</li> <li>types</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/#nnbench.add_reporters","title":"add_reporters","text":"<pre><code>add_reporters()\n</code></pre> Source code in <code>src/nnbench/__init__.py</code> <pre><code>def add_reporters():\n    eps = entry_points()\n\n    if hasattr(eps, \"select\"):  # Python 3.10+ / importlib.metadata &gt;= 3.9.0\n        reporters = eps.select(group=\"nnbench.reporters\")\n    else:\n        reporters = eps.get(\"nnbench.reporters\", [])  # type: ignore\n\n    for rep in reporters:\n        key, clsname = rep.name.split(\"=\", 1)\n        register_reporter(key, clsname)\n</code></pre>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], dict[str, Any]]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo","title":"PythonInfo","text":"<p>A context helper returning version info for requested installed packages.</p> <p>If a requested package is not installed, an empty string is returned instead.</p> PARAMETER  DESCRIPTION <code>*packages</code> <p>Names of the requested packages under which they exist in the current environment. For packages installed through <code>pip</code>, this equals the PyPI package name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class PythonInfo:\n    \"\"\"\n    A context helper returning version info for requested installed packages.\n\n    If a requested package is not installed, an empty string is returned instead.\n\n    Parameters\n    ----------\n    *packages: str\n        Names of the requested packages under which they exist in the current environment.\n        For packages installed through ``pip``, this equals the PyPI package name.\n    \"\"\"\n\n    key = \"python\"\n\n    def __init__(self, *packages: str):\n        self.packages = packages\n\n    def __call__(self) -&gt; dict[str, Any]:\n        from importlib.metadata import PackageNotFoundError, version\n\n        result: dict[str, Any] = dict()\n\n        result[\"version\"] = platform.python_version()\n        result[\"implementation\"] = platform.python_implementation()\n        buildno, buildtime = platform.python_build()\n        result[\"buildno\"] = buildno\n        result[\"buildtime\"] = buildtime\n\n        dependencies: dict[str, str] = {}\n        for pkg in self.packages:\n            try:\n                dependencies[pkg] = version(pkg)\n            except PackageNotFoundError:\n                dependencies[pkg] = \"\"\n\n        result[\"dependencies\"] = dependencies\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'python'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages = packages\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo","title":"GitEnvironmentInfo","text":"<p>A context helper providing the current git commit, latest tag, and upstream repository name.</p> PARAMETER  DESCRIPTION <code>remote</code> <p>Remote name for which to provide info, by default <code>\"origin\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class GitEnvironmentInfo:\n    \"\"\"\n    A context helper providing the current git commit, latest tag, and upstream repository name.\n\n    Parameters\n    ----------\n    remote: str\n        Remote name for which to provide info, by default ``\"origin\"``.\n    \"\"\"\n\n    key = \"git\"\n\n    def __init__(self, remote: str = \"origin\"):\n        self.remote = remote\n\n    def __call__(self) -&gt; dict[str, dict[str, Any]]:\n        import subprocess\n\n        def git_subprocess(args: list[str]) -&gt; subprocess.CompletedProcess:\n            if platform.system() == \"Windows\":\n                git = \"git.exe\"\n            else:\n                git = \"git\"\n\n            return subprocess.run(  # nosec: B603\n                [git, *args], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\"\n            )\n\n        result: dict[str, Any] = {\n            \"commit\": \"\",\n            \"provider\": \"\",\n            \"repository\": \"\",\n            \"tag\": \"\",\n            \"dirty\": None,\n        }\n\n        # first, check if inside a repo.\n        p = git_subprocess([\"rev-parse\", \"--is-inside-work-tree\"])\n        # if not, return empty info.\n        if p.returncode:\n            return {\"git\": result}\n\n        # secondly: get the current commit.\n        p = git_subprocess([\"rev-parse\", \"HEAD\"])\n        if not p.returncode:\n            result[\"commit\"] = p.stdout.strip()\n\n        # thirdly, get the latest tag, without a short commit SHA attached.\n        p = git_subprocess([\"describe\", \"--tags\", \"--abbrev=0\"])\n        if not p.returncode:\n            result[\"tag\"] = p.stdout.strip()\n\n        # and finally, get the remote repo name pointed to by the given remote.\n        p = git_subprocess([\"remote\", \"get-url\", self.remote])\n        if not p.returncode:\n            remotename: str = p.stdout.strip()\n            # it's an SSH remote.\n            if \"@\" in remotename:\n                prefix, sep = \"git@\", \":\"\n            else:\n                # it is HTTPS.\n                prefix, sep = \"https://\", \"/\"\n\n            remotename = remotename.removeprefix(prefix)\n            provider, reponame = remotename.split(sep, 1)\n\n            result[\"provider\"] = provider\n            result[\"repository\"] = reponame.removesuffix(\".git\")\n\n        p = git_subprocess([\"status\", \"--porcelain\"])\n        if not p.returncode:\n            result[\"dirty\"] = bool(p.stdout.strip())\n\n        return {\"git\": result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'git'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.remote","title":"remote  <code>instance-attribute</code>","text":"<pre><code>remote = remote\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo","title":"CPUInfo","text":"Source code in <code>src/nnbench/context.py</code> <pre><code>class CPUInfo:\n    key = \"cpu\"\n\n    def __init__(\n        self,\n        memunit: Literal[\"kB\", \"MB\", \"GB\"] = \"MB\",\n        frequnit: Literal[\"kHz\", \"MHz\", \"GHz\"] = \"MHz\",\n    ):\n        self.memunit = memunit\n        self.frequnit = frequnit\n        self.conversion_table: dict[str, float] = {\"k\": 1e3, \"M\": 1e6, \"G\": 1e9}\n\n    def __call__(self) -&gt; dict[str, Any]:\n        try:\n            import psutil\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"context provider {self.__class__.__name__}() needs `psutil` installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade psutil`.\"\n            )\n\n        result: dict[str, Any] = dict()\n\n        # first, the platform info.\n        result[\"architecture\"] = platform.machine()\n        result[\"bitness\"] = platform.architecture()[0]\n        result[\"processor\"] = platform.processor()\n        result[\"system\"] = platform.system()\n        result[\"system-version\"] = platform.release()\n\n        freq_struct = psutil.cpu_freq()\n        freq_conversion = self.conversion_table[self.frequnit[0]]\n        # result is in MHz, so we convert to Hz and apply the conversion factor.\n        result[\"frequency\"] = freq_struct.current * 1e6 / freq_conversion\n        result[\"frequency_unit\"] = self.frequnit\n        result[\"min_frequency\"] = freq_struct.min\n        result[\"max_frequency\"] = freq_struct.max\n        result[\"num_cpus\"] = psutil.cpu_count(logical=False)\n        result[\"num_logical_cpus\"] = psutil.cpu_count()\n\n        mem_struct = psutil.virtual_memory()\n        mem_conversion = self.conversion_table[self.memunit[0]]\n        # result is in bytes, so no need for base conversion.\n        result[\"total_memory\"] = mem_struct.total / mem_conversion\n        result[\"memory_unit\"] = self.memunit\n        # TODO: Lacks CPU cache info, which requires a solution other than psutil.\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'cpu'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.memunit","title":"memunit  <code>instance-attribute</code>","text":"<pre><code>memunit = memunit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.frequnit","title":"frequnit  <code>instance-attribute</code>","text":"<pre><code>frequnit = frequnit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.conversion_table","title":"conversion_table  <code>instance-attribute</code>","text":"<pre><code>conversion_table: dict[str, float] = {'k': 1000.0, 'M': 1000000.0, 'G': 1000000000.0}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/core.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>A display name to give to the benchmark. Useful in summaries and reports.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str | None = None,\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    name: str | None\n        A display name to give to the benchmark. Useful in summaries and reports.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        for params in parameters:\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER  DESCRIPTION <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        varnames = iterables.keys()\n        for values in itertools.product(*iterables.values()):\n            params = dict(zip(varnames, values))\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            wrapper = update_wrapper(partial(fn, **params), fn)\n            bm = Benchmark(wrapper, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner","title":"BenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"An abstract benchmark runner class.\"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self):\n        self.benchmarks: list[Benchmark] = list()\n\n    def _check(self, params: dict[str, Any]) -&gt; None:\n        param_types = {k: type(v) for k, v in params.items()}\n        allvars: dict[str, tuple[type, Any]] = {}\n        empty = inspect.Parameter.empty\n\n        def _issubtype(t1: type, t2: type) -&gt; bool:\n            \"\"\"Small helper to make typechecks work on generics.\"\"\"\n\n            def _canonicalize(t: type) -&gt; type:\n                t_origin = get_origin(t)\n                if t_origin is not None:\n                    return t_origin\n                return t\n\n            if t1 == t2:\n                return True\n\n            t1 = _canonicalize(t1)\n            t2 = _canonicalize(t2)\n            if not inspect.isclass(t1):\n                return False\n            # TODO: Extend typing checks to args.\n            return issubclass(t1, t2)\n\n        for bm in self.benchmarks:\n            for var in bm.interface.variables:\n                name, typ, default = var\n                if name in params and default != empty:\n                    logger.debug(\n                        f\"using given value {params[name]} over default value {default} \"\n                        f\"for parameter {name!r} in benchmark {bm.fn.__name__}()\"\n                    )\n\n                if typ == empty:\n                    logger.debug(f\"parameter {name!r} untyped in benchmark {bm.fn.__name__}().\")\n\n                if name in allvars:\n                    currvar = allvars[name]\n                    orig_type, orig_val = new_type, new_val = currvar\n                    # If a benchmark has a variable without a default value,\n                    # that variable is taken into the combined interface as no-default.\n                    if default == empty:\n                        new_val = default\n                    # These types need not be exact matches, just compatible.\n                    # Two types are compatible iff either is a subtype of the other.\n                    # We only log the narrowest type for each varname in the final interface,\n                    # since that determines whether an input value is admissible.\n                    if _issubtype(orig_type, typ):\n                        pass\n                    elif _issubtype(typ, orig_type):\n                        new_type = typ\n                    else:\n                        raise TypeError(\n                            f\"got incompatible types {orig_type}, {typ} for parameter {name!r}\"\n                        )\n                    newvar = (new_type, new_val)\n                    if newvar != currvar:\n                        allvars[name] = newvar\n                else:\n                    allvars[name] = (typ, default)\n\n        for name, (typ, default) in allvars.items():\n            # check if a no-default variable has no parameter.\n            if name not in param_types and default == empty:\n                raise ValueError(f\"missing value for required parameter {name!r}\")\n\n            # skip the subsequent type check if the variable is untyped.\n            if typ == empty:\n                continue\n            # type-check parameter value against the narrowest hinted type.\n            if name in param_types and not _issubtype(param_types[name], typ):\n                raise TypeError(\n                    f\"expected type {typ} for parameter {name!r}, got {param_types[name]}\"\n                )\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py, tags)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        elif ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if isdunder(k):\n                continue\n            elif isinstance(v, self.benchmark_type):\n                if not tags or set(tags) &amp; set(v.tags):\n                    self.benchmarks.append(v)\n            elif iscontainer(v):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        if not tags or set(tags) &amp; set(bm.tags):\n                            self.benchmarks.append(bm)\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str],\n        params: dict[str, Any] | Parameters | None = None,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | Parameters | None\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n\n        Returns\n        -------\n        BenchmarkRecord\n            A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n            holding the context information, and \"benchmarks\", holding an array with the\n            benchmark results.\n\n        Raises\n        ------\n        ValueError\n            If any context value is provided more than once.\n        \"\"\"\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        # if we still have no benchmarks after collection, warn and return an empty record.\n        if not self.benchmarks:\n            warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n            return BenchmarkRecord(context={}, benchmarks=[])\n\n        params = params or {}\n        if isinstance(params, Parameters):\n            dparams = asdict(params)\n        else:\n            dparams = params\n\n        self._check(dparams)\n\n        ctx: dict[str, Any] = dict()\n        ctxkeys = set(ctx.keys())\n\n        for provider in context:\n            ctxval = provider()\n            valkeys = set(ctxval.keys())\n            # we do not allow multiple values for a context key.\n            duplicates = ctxkeys &amp; valkeys\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx |= ctxval\n            ctxkeys |= valkeys\n\n        results: list[dict[str, Any]] = []\n        for benchmark in self.benchmarks:\n            bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n            # TODO: Wrap this into an execution context\n            res: dict[str, Any] = {\n                \"name\": benchmark.name,\n                \"function\": f\"{benchmark.fn.__qualname__}.{benchmark.fn.__name__}\",\n                \"description\": benchmark.fn.__doc__,\n                \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n                \"error_occurred\": False,\n                \"error_message\": \"\",\n            }\n            try:\n                benchmark.setUp(**bmparams)\n                with timer(res):\n                    res[\"value\"] = benchmark.fn(**bmparams)\n            except Exception as e:\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(**bmparams)\n                results.append(res)\n\n        return BenchmarkRecord(\n            context=ctx,\n            benchmarks=results,\n        )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    ppath = Path(path_or_module)\n    if ppath.is_dir():\n        pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n        for py in pythonpaths:\n            logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n            self.collect(py, tags)\n        return\n    elif ppath.is_file():\n        module = import_file_as_module(path_or_module)\n    elif ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        raise ValueError(\n            f\"expected a module name, Python file, or directory, \"\n            f\"got {str(path_or_module)!r}\"\n        )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if isdunder(k):\n            continue\n        elif isinstance(v, self.benchmark_type):\n            if not tags or set(tags) &amp; set(v.tags):\n                self.benchmarks.append(v)\n        elif iscontainer(v):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        self.benchmarks.append(bm)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER  DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Parameters | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>A JSON output representing the benchmark results. Has two top-level keys, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If any context value is provided more than once.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | Parameters | None\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n\n    Returns\n    -------\n    BenchmarkRecord\n        A JSON output representing the benchmark results. Has two top-level keys, \"context\"\n        holding the context information, and \"benchmarks\", holding an array with the\n        benchmark results.\n\n    Raises\n    ------\n    ValueError\n        If any context value is provided more than once.\n    \"\"\"\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    # if we still have no benchmarks after collection, warn and return an empty record.\n    if not self.benchmarks:\n        warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n        return BenchmarkRecord(context={}, benchmarks=[])\n\n    params = params or {}\n    if isinstance(params, Parameters):\n        dparams = asdict(params)\n    else:\n        dparams = params\n\n    self._check(dparams)\n\n    ctx: dict[str, Any] = dict()\n    ctxkeys = set(ctx.keys())\n\n    for provider in context:\n        ctxval = provider()\n        valkeys = set(ctxval.keys())\n        # we do not allow multiple values for a context key.\n        duplicates = ctxkeys &amp; valkeys\n        if duplicates:\n            dupe, *_ = duplicates\n            raise ValueError(f\"got multiple values for context key {dupe!r}\")\n        ctx |= ctxval\n        ctxkeys |= valkeys\n\n    results: list[dict[str, Any]] = []\n    for benchmark in self.benchmarks:\n        bmparams = {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n        # TODO: Wrap this into an execution context\n        res: dict[str, Any] = {\n            \"name\": benchmark.name,\n            \"function\": f\"{benchmark.fn.__qualname__}.{benchmark.fn.__name__}\",\n            \"description\": benchmark.fn.__doc__,\n            \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"error_occurred\": False,\n            \"error_message\": \"\",\n        }\n        try:\n            benchmark.setUp(**bmparams)\n            with timer(res):\n                res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(**bmparams)\n            results.append(res)\n\n    return BenchmarkRecord(\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.iscontainer","title":"iscontainer","text":"<pre><code>iscontainer(s: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def iscontainer(s: Any) -&gt; bool:\n    return isinstance(s, (tuple, list))\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.isdunder","title":"isdunder","text":"<pre><code>isdunder(s: str) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def isdunder(s: str) -&gt; bool:\n    return s.startswith(\"__\") and s.endswith(\"__\")\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.timer","title":"timer","text":"<pre><code>timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>@contextlib.contextmanager\ndef timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]:\n    start = time.perf_counter_ns()\n    try:\n        yield\n    finally:\n        end = time.perf_counter_ns()\n        bm[\"time_ns\"] = end - start\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":"<p>Useful type interfaces to override/subclass in benchmarking workflows.</p>"},{"location":"reference/nnbench/types/#nnbench.types.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord","title":"BenchmarkRecord","text":"<p>             Bases: <code>TypedDict</code></p> Source code in <code>src/nnbench/types.py</code> <pre><code>class BenchmarkRecord(TypedDict):\n    context: dict[str, Any]\n    benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact","title":"Artifact","text":"<p>             Bases: <code>Generic[T]</code></p> <p>A base artifact class for loading (materializing) artifacts from disk or from remote storage.</p> <p>This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way. It is most useful when running models on already saved data or models, e.g. when comparing a newly trained model against a baseline in storage.</p> <p>Subclasses need to implement the <code>Artifact.materialize()</code> API, telling nnbench how to load the desired artifact from a path.</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the artifact files.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>class Artifact(Generic[T]):\n    \"\"\"\n    A base artifact class for loading (materializing) artifacts from disk or from remote storage.\n\n    This is a helper to convey which kind of type gets loaded for a benchmark in a type-safe way.\n    It is most useful when running models on already saved data or models, e.g. when\n    comparing a newly trained model against a baseline in storage.\n\n    Subclasses need to implement the `Artifact.materialize()` API, telling nnbench how to\n    load the desired artifact from a path.\n\n    Parameters\n    ----------\n    path: str | os.PathLike[str]\n        Path to the artifact files.\n    \"\"\"\n\n    def __init__(self, path: str | os.PathLike[str]) -&gt; None:\n        # Save the path for later just-in-time materialization.\n        self.path = path\n        self._value: T | None = None\n\n    @classmethod\n    def materialize(cls) -&gt; \"Artifact\":\n        \"\"\"Load the artifact from storage.\"\"\"\n        raise NotImplementedError\n\n    def value(self) -&gt; T:\n        if self._value is None:\n            raise ValueError(\n                f\"artifact has not been instantiated yet, \"\n                f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n            )\n        return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = path\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.materialize","title":"materialize  <code>classmethod</code>","text":"<pre><code>materialize() -&gt; 'Artifact'\n</code></pre> <p>Load the artifact from storage.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef materialize(cls) -&gt; \"Artifact\":\n    \"\"\"Load the artifact from storage.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Artifact.value","title":"value","text":"<pre><code>value() -&gt; T\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def value(self) -&gt; T:\n    if self._value is None:\n        raise ValueError(\n            f\"artifact has not been instantiated yet, \"\n            f\"perhaps you forgot to call {self.__class__.__name__}.materialize()?\"\n        )\n    return self._value\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Parameters","title":"Parameters  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Parameters:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>field(default=None)</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>field(repr=False, default=NoOp)</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>field(repr=False, default=())</code> </p> <code>interface</code> <p>Interface of the benchmark function</p> <p> TYPE: <code>Interface</code> DEFAULT: <code>field(init=False, repr=False)</code> </p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    interface: Interface\n        Interface of the benchmark function\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str | None = field(default=None)\n    setUp: Callable[..., None] = field(repr=False, default=NoOp)\n    tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n    interface: Interface = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if not self.name:\n            super().__setattr__(\"name\", self.fn.__name__)\n        super().__setattr__(\"interface\", Interface.from_callable(self.fn))\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = field(default=None)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[..., None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface. An instance of this class is created using the <code>from_callable</code> class method.</p> Parameters: <p>names : tuple[str, ...]     Names of the function parameters. types : tuple[type, ...]     Types of the function parameters. defaults : tuple     A tuple of the function parameters' default values. variables : tuple[Variable, ...]     A tuple of tuples, where each inner tuple contains the parameter name and type. returntype: type     The function's return type annotation, or NoneType if left untyped.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface. An instance of this class\n    is created using the `from_callable` class method.\n\n    Parameters:\n    ----------\n    names : tuple[str, ...]\n        Names of the function parameters.\n    types : tuple[type, ...]\n        Types of the function parameters.\n    defaults : tuple\n        A tuple of the function parameters' default values.\n    variables : tuple[Variable, ...]\n        A tuple of tuples, where each inner tuple contains the parameter name and type.\n    returntype: type\n        The function's return type annotation, or NoneType if left untyped.\n    \"\"\"\n\n    names: tuple[str, ...]\n    types: tuple[type, ...]\n    defaults: tuple\n    variables: tuple[Variable, ...]\n    returntype: type\n\n    @classmethod\n    def from_callable(cls, fn: Callable) -&gt; Interface:\n        \"\"\"\n        Creates an interface instance from the given callable.\n        \"\"\"\n        # Set follow_wrapped=False to get the partially filled interfaces.\n        # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n        sig = inspect.signature(fn, follow_wrapped=False)\n        ret = sig.return_annotation\n        return cls(\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(p.default for p in sig.parameters.values()),\n            tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable) -&gt; Interface\n</code></pre> <p>Creates an interface instance from the given callable.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable) -&gt; Interface:\n    \"\"\"\n    Creates an interface instance from the given callable.\n    \"\"\"\n    # Set follow_wrapped=False to get the partially filled interfaces.\n    # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n    sig = inspect.signature(fn, follow_wrapped=False)\n    ret = sig.return_annotation\n    return cls(\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(p.default for p in sig.parameters.values()),\n        tuple((k, v.annotation, v.default) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.NoOp","title":"NoOp","text":"<pre><code>NoOp(**kwargs: Any) -&gt; None\n</code></pre> Source code in <code>src/nnbench/types.py</code> <pre><code>def NoOp(**kwargs: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file)\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.with_suffix(\"\").as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: For absolute paths, the resulting module name will be horrifying\n    #  -&gt; find a sensible cutoff point (project root)\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.reporter_registry","title":"reporter_registry  <code>module-attribute</code>","text":"<pre><code>reporter_registry: MappingProxyType[str, type[BenchmarkReporter]] = MappingProxyType(\n    _reporter_registry\n)\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.register_reporter","title":"register_reporter","text":"<pre><code>register_reporter(key: str, cls_or_name: str | type[BenchmarkReporter]) -&gt; None\n</code></pre> <p>Register a reporter class by its fully qualified module path.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to register the reporter under. Subsequently, this key can be used in place of reporter classes in code.</p> <p> TYPE: <code>str</code> </p> <code>cls_or_name</code> <p>Name of or full module path to the reporter class. For example, when registering a class <code>MyReporter</code> located in <code>my_module</code>, <code>name</code> should be <code>my_module.MyReporter</code>.</p> <p> TYPE: <code>str | type[BenchmarkReporter]</code> </p> Source code in <code>src/nnbench/reporter/__init__.py</code> <pre><code>def register_reporter(key: str, cls_or_name: str | type[BenchmarkReporter]) -&gt; None:\n    \"\"\"\n    Register a reporter class by its fully qualified module path.\n\n    Parameters\n    ----------\n    key: str\n        The key to register the reporter under. Subsequently, this key can be used in place\n        of reporter classes in code.\n    cls_or_name: str | type[BenchmarkReporter]\n        Name of or full module path to the reporter class. For example, when registering a class\n        ``MyReporter`` located in ``my_module``, ``name`` should be ``my_module.MyReporter``.\n    \"\"\"\n\n    if isinstance(cls_or_name, str):\n        name = cls_or_name\n        modname, clsname = name.rsplit(\".\", 1)\n        mod = importlib.import_module(modname)\n        cls = getattr(mod, clsname)\n        _reporter_registry[key] = cls\n    else:\n        # name = cls_or_name.__module__ + \".\" + cls_or_name.__qualname__\n        _reporter_registry[key] = cls_or_name\n</code></pre>"},{"location":"reference/nnbench/reporter/base/","title":"base","text":""},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter","title":"BenchmarkReporter","text":"<p>The base interface for a benchmark reporter class.</p> <p>A benchmark reporter consumes benchmark results from a previous run, and subsequently reports them in the way specified by the respective implementation's <code>report_result()</code> method.</p> <p>For example, to write benchmark results to a database, you could save the credentials for authentication on the class, and then stream the results directly to the database in <code>report_result()</code>, with preprocessing if necessary.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>class BenchmarkReporter:\n    \"\"\"\n    The base interface for a benchmark reporter class.\n\n    A benchmark reporter consumes benchmark results from a previous run, and subsequently\n    reports them in the way specified by the respective implementation's ``report_result()``\n    method.\n\n    For example, to write benchmark results to a database, you could save the credentials\n    for authentication on the class, and then stream the results directly to\n    the database in ``report_result()``, with preprocessing if necessary.\n    \"\"\"\n\n    merge: bool = False\n    \"\"\"Whether to merge multiple BenchmarkRecords before reporting.\"\"\"\n\n    def initialize(self):\n        \"\"\"\n        Initialize the reporter's state.\n\n        This is the place where to create a result directory, a database connection,\n        or a HTTP client.\n        \"\"\"\n        pass\n\n    def finalize(self):\n        \"\"\"\n        Finalize the reporter's state.\n\n        This is the place to destroy / release resources that were previously\n        acquired in ``initialize()``.\n        \"\"\"\n        pass\n\n    merge_records = staticmethod(default_merge)\n\n    def read(self) -&gt; BenchmarkRecord:\n        raise NotImplementedError\n\n    def read_batched(self) -&gt; list[BenchmarkRecord]:\n        raise NotImplementedError\n\n    def write(self, record: BenchmarkRecord) -&gt; None:\n        raise NotImplementedError\n\n    def write_batched(self, records: Sequence[BenchmarkRecord]) -&gt; None:\n        # by default, merge first and then write.\n        if self.merge:\n            merged = self.merge_records(records)\n            self.write(merged)\n        else:\n            # write everything in a loop.\n            for record in records:\n                self.write(record)\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.merge","title":"merge  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>merge: bool = False\n</code></pre> <p>Whether to merge multiple BenchmarkRecords before reporting.</p>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.merge_records","title":"merge_records  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>merge_records = staticmethod(default_merge)\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize the reporter's state.</p> <p>This is the place where to create a result directory, a database connection, or a HTTP client.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Initialize the reporter's state.\n\n    This is the place where to create a result directory, a database connection,\n    or a HTTP client.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize the reporter's state.</p> <p>This is the place to destroy / release resources that were previously acquired in <code>initialize()</code>.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Finalize the reporter's state.\n\n    This is the place to destroy / release resources that were previously\n    acquired in ``initialize()``.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.read","title":"read","text":"<pre><code>read() -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def read(self) -&gt; BenchmarkRecord:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.read_batched","title":"read_batched","text":"<pre><code>read_batched() -&gt; list[BenchmarkRecord]\n</code></pre> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def read_batched(self) -&gt; list[BenchmarkRecord]:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def write(self, record: BenchmarkRecord) -&gt; None:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.write_batched","title":"write_batched","text":"<pre><code>write_batched(records: Sequence[BenchmarkRecord]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def write_batched(self, records: Sequence[BenchmarkRecord]) -&gt; None:\n    # by default, merge first and then write.\n    if self.merge:\n        merged = self.merge_records(records)\n        self.write(merged)\n    else:\n        # write everything in a loop.\n        for record in records:\n            self.write(record)\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.default_merge","title":"default_merge","text":"<pre><code>default_merge(records: Sequence[BenchmarkRecord]) -&gt; BenchmarkRecord\n</code></pre> <p>Merges a number of benchmark records into one.</p> <p>The resulting record has an empty top-level context, since the context values might be different in all respective records.</p> <p>TODO: Think about merging contexts here to preserve the record model,  padding missing values with a placeholder if not present.  -&gt; Might be easier with an OOP Context class.</p> PARAMETER  DESCRIPTION <code>records</code> <p>The records to merge.</p> <p> TYPE: <code>Sequence[BenchmarkRecord]</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The merged record, with all benchmark contexts inlined into their respective benchmarks.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def default_merge(records: Sequence[BenchmarkRecord]) -&gt; BenchmarkRecord:\n    \"\"\"\n    Merges a number of benchmark records into one.\n\n    The resulting record has an empty top-level context, since the context\n    values might be different in all respective records.\n\n    TODO: Think about merging contexts here to preserve the record model,\n     padding missing values with a placeholder if not present.\n     -&gt; Might be easier with an OOP Context class.\n\n    Parameters\n    ----------\n    records: Sequence[BenchmarkRecord]\n        The records to merge.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The merged record, with all benchmark contexts inlined into their\n        respective benchmarks.\n\n    \"\"\"\n    merged = BenchmarkRecord(context=dict(), benchmarks=[])\n    for record in records:\n        ctx, benchmarks = record[\"context\"], record[\"benchmarks\"]\n        for bm in benchmarks:\n            bm[\"context\"] = ctx\n        merged[\"benchmarks\"].extend(benchmarks)\n    return merged\n</code></pre>"},{"location":"reference/nnbench/reporter/console/","title":"console","text":""},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter","title":"ConsoleReporter","text":"<p>             Bases: <code>BenchmarkReporter</code></p> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>class ConsoleReporter(BenchmarkReporter):\n    def __init__(\n        self,\n        tablefmt: str = \"simple\",\n        custom_formatters: dict[str, Callable[[Any], Any]] | None = None,\n    ):\n        self.tablefmt = tablefmt\n        self.custom_formatters: dict[str, Callable[[Any], Any]] = custom_formatters or {}\n\n    def write(\n        self,\n        record: BenchmarkRecord,\n        benchmark_filter: str | None = None,\n        include_context: tuple[str, ...] = (),\n        exclude_empty: bool = True,\n    ) -&gt; None:\n        try:\n            from tabulate import tabulate\n        except ModuleNotFoundError:\n            raise ValueError(\n                f\"class {self.__class__.__name__}() requires `tabulate` to be installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n            )\n\n        ctx, benchmarks = record[\"context\"], record[\"benchmarks\"]\n\n        nulls = set() if not exclude_empty else nullcols(benchmarks)\n\n        if benchmark_filter is not None:\n            regex = re.compile(benchmark_filter, flags=re.IGNORECASE)\n        else:\n            regex = None\n\n        filtered = []\n        for bm in benchmarks:\n            if regex is not None and regex.search(bm[\"name\"]) is None:\n                continue\n            filteredctx = {\n                k: v\n                for k, v in flatten(ctx).items()\n                if any(k.startswith(i) for i in include_context)\n            }\n            filteredbm = {k: v for k, v in bm.items() if k not in nulls}\n            filteredbm.update(filteredctx)\n            # only apply custom formatters after context merge\n            #  to allow custom formatting of context values.\n            filteredbm = {\n                k: self.custom_formatters.get(k, lambda x: x)(v) for k, v in filteredbm.items()\n            }\n            filtered.append(filteredbm)\n\n        print(tabulate(filtered, headers=\"keys\", tablefmt=self.tablefmt))\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.tablefmt","title":"tablefmt  <code>instance-attribute</code>","text":"<pre><code>tablefmt = tablefmt\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.custom_formatters","title":"custom_formatters  <code>instance-attribute</code>","text":"<pre><code>custom_formatters: dict[str, Callable[[Any], Any]] = custom_formatters or {}\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.write","title":"write","text":"<pre><code>write(\n    record: BenchmarkRecord,\n    benchmark_filter: str | None = None,\n    include_context: tuple[str, ...] = (),\n    exclude_empty: bool = True,\n) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def write(\n    self,\n    record: BenchmarkRecord,\n    benchmark_filter: str | None = None,\n    include_context: tuple[str, ...] = (),\n    exclude_empty: bool = True,\n) -&gt; None:\n    try:\n        from tabulate import tabulate\n    except ModuleNotFoundError:\n        raise ValueError(\n            f\"class {self.__class__.__name__}() requires `tabulate` to be installed. \"\n            f\"To install, run `{sys.executable} -m pip install --upgrade tabulate`.\"\n        )\n\n    ctx, benchmarks = record[\"context\"], record[\"benchmarks\"]\n\n    nulls = set() if not exclude_empty else nullcols(benchmarks)\n\n    if benchmark_filter is not None:\n        regex = re.compile(benchmark_filter, flags=re.IGNORECASE)\n    else:\n        regex = None\n\n    filtered = []\n    for bm in benchmarks:\n        if regex is not None and regex.search(bm[\"name\"]) is None:\n            continue\n        filteredctx = {\n            k: v\n            for k, v in flatten(ctx).items()\n            if any(k.startswith(i) for i in include_context)\n        }\n        filteredbm = {k: v for k, v in bm.items() if k not in nulls}\n        filteredbm.update(filteredctx)\n        # only apply custom formatters after context merge\n        #  to allow custom formatting of context values.\n        filteredbm = {\n            k: self.custom_formatters.get(k, lambda x: x)(v) for k, v in filteredbm.items()\n        }\n        filtered.append(filteredbm)\n\n    print(tabulate(filtered, headers=\"keys\", tablefmt=self.tablefmt))\n</code></pre>"},{"location":"reference/nnbench/reporter/util/","title":"util","text":""},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.nullcols","title":"nullcols","text":"<pre><code>nullcols(_benchmarks: list[dict[str, Any]]) -&gt; tuple[str, ...]\n</code></pre> <p>Extracts columns that only contain false-ish data from a list of benchmarks.</p> <p>Since this data is most often not interesting, the result of this can be used to filter out these columns from the benchmark dictionaries.</p> PARAMETER  DESCRIPTION <code>_benchmarks</code> <p>The benchmarks to filter.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>tuple[str, ...]</code> <p>Tuple of the columns (key names) that only contain false-ish values across all benchmarks.</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def nullcols(_benchmarks: list[dict[str, Any]]) -&gt; tuple[str, ...]:\n    \"\"\"\n    Extracts columns that only contain false-ish data from a list of benchmarks.\n\n    Since this data is most often not interesting, the result of this\n    can be used to filter out these columns from the benchmark dictionaries.\n\n    Parameters\n    ----------\n    _benchmarks: list[dict[str, Any]]\n        The benchmarks to filter.\n\n    Returns\n    -------\n    tuple[str, ...]\n        Tuple of the columns (key names) that only contain false-ish values\n        across all benchmarks.\n    \"\"\"\n    nulls: dict[str, bool] = collections.defaultdict(bool)\n    for bm in _benchmarks:\n        for k, v in bm.items():\n            nulls[k] = nulls[k] or bool(v)\n    return tuple(k for k, v in nulls.items() if not v)\n</code></pre>"},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.flatten","title":"flatten","text":"<pre><code>flatten(d: dict[str, Any], prefix: str = '', sep: str = '.') -&gt; dict[str, Any]\n</code></pre> <p>Turn a nested dictionary into a flattened dictionary.</p> PARAMETER  DESCRIPTION <code>d</code> <p>(Possibly) nested dictionary to flatten.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>prefix</code> <p>Key prefix to apply at the top-level (nesting level 0).</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>sep</code> <p>Separator on which to join keys, \".\" by default.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The flattened dictionary.</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def flatten(d: dict[str, Any], prefix: str = \"\", sep: str = \".\") -&gt; dict[str, Any]:\n    \"\"\"\n    Turn a nested dictionary into a flattened dictionary.\n\n    Parameters\n    ----------\n    d: dict[str, Any]\n        (Possibly) nested dictionary to flatten.\n    prefix: str\n        Key prefix to apply at the top-level (nesting level 0).\n    sep: str\n        Separator on which to join keys, \".\" by default.\n\n    Returns\n    -------\n    dict[str, Any]\n        The flattened dictionary.\n    \"\"\"\n\n    items: list[tuple[str, Any]] = []\n    for key, value in d.items():\n        new_key = prefix + sep + key if prefix else key\n        if isinstance(value, dict):\n            items.extend(flatten(value, new_key, sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n</code></pre>"},{"location":"tutorials/","title":"Examples","text":"<p>This page showcases some examples of applications for nnbench. Click any of the links below for inspiration on how to use nnbench in your projects.</p> <ul> <li>How to integrate nnbench into an existing ML pipeline</li> </ul>"},{"location":"tutorials/mnist/","title":"Integrating nnbench into an existing ML pipeline","text":"<p>Thanks to nnbench's modularity, we can easily integrate it into existing ML experiment code.</p> <p>As an example, we use an MNIST pipeline written for the popular ML framework JAX. While the actual data sourcing and training code is interesting on its own, we focus solely on the nnbench application part. You can find the full example code in the nnbench repository.</p>"},{"location":"tutorials/mnist/#defining-and-organizing-benchmarks","title":"Defining and organizing benchmarks","text":"<p>To properly structure our project, we avoid mixing training pipeline code and benchmark code by placing all benchmarks in a standalone file, similarly to how you might structure unit tests for your code.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom mnist import ArrayMapping, ConvNet\n\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(params: ArrayMapping, data: ArrayMapping) -&gt; float:\n    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n    cn = ConvNet()\n    y_pred = cn.apply({\"params\": params}, x_test)\n    return jnp.mean(jnp.argmax(y_pred, -1) == y_test).item()\n\n\n@nnbench.benchmark(name=\"Model size (MB)\")\ndef modelsize(params: ArrayMapping) -&gt; float:\n    nbytes = sum(x.size * x.dtype.itemsize for x in jax.tree_util.tree_leaves(params))\n    return nbytes / 1e6\n</code></pre> <p>This definition is short and sweet, and contains a few important details:</p> <ul> <li>Both functions are given the <code>@nnbench.benchmark</code> decorator - this enables our runner to find and collect them before starting the benchmark run.</li> <li>The <code>modelsize</code> benchmark is given a custom name (<code>\"Model size (MB)\"</code>), indicating that the resulting number is the combined size of the model weights in megabytes. This is done for display purposes, to improve interpretability when reporting results.</li> <li>The <code>params</code> argument is the same in both benchmarks, both in name and type. This is important, since it ensures that both benchmarks will be run with the same model weights.</li> </ul> <p>That's all - now we can shift over to our main pipeline code and see what is necessary to execute the benchmarks and visualize the results.</p>"},{"location":"tutorials/mnist/#setting-up-a-benchmark-runner-and-parameters","title":"Setting up a benchmark runner and parameters","text":"<p>After finishing the benchmark setup, we only need a few more lines to augment our pipeline.</p> <p>We assume that the benchmark file is located in the same folder as the training pipeline - thus, we can specify our parent directory as the place in which to search for benchmarks:</p> <pre><code>HERE = Path(__file__).parent\n</code></pre> <p>Next, we can define a custom subclass of <code>nnbench.Parameters</code> to hold our benchmark parameters. Benchmark parameters are a set of variables used as inputs to the benchmark functions collected during the benchmark run.</p> <p>Since our benchmarks above are parametrized by the model weights (named <code>params</code> in the function signatures) and the MNIST data split (called <code>data</code>), we define our parameters to take exactly these two values.</p> <pre><code>@dataclass(frozen=True)\nclass MNISTTestParameters(nnbench.Parameters):\n    params: Mapping[str, jax.Array]\n    data: ArrayMapping\n</code></pre> <p>And that's it! After we implement all training code, we just run nnbench directly after training in our top-level pipeline function:</p> <pre><code>def mnist_jax():\n    \"\"\"Load MNIST data and train a simple ConvNet model.\"\"\"\n    mnist = load_mnist()\n    mnist = preprocess(mnist)\n    state, data = train(mnist)\n\n    # the nnbench portion.\n    runner = nnbench.BenchmarkRunner()\n    reporter = ConsoleReporter()\n    params = MNISTTestParameters(params=state.params, data=data)\n    result = runner.run(HERE, params=params)\n</code></pre> <p>We use the <code>ConsoleReporter</code> to print the results directly to the terminal in a table. Notice how by we can reuse the training artifacts in nnbench as parameters to obtain results right after training!</p> <p>The output might look like this:</p> <pre><code>name               value\n---------------  -------\naccuracy         0.9712\nModel size (MB)  3.29783\n</code></pre> <p>This can be improved in a number of ways - for example by enriching it with metadata about the model architecture, the used GPU, etc. For more information on how to supply context to benchmarks, check the user guide section.</p>"}]}