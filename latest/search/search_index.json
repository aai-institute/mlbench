{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Examples</p><p>Examples on how to use nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd nnbench\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>pytest\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, use <code>pip-compile</code> to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\npip-compile --extra=dev --no-annotate --output-file=requirements-dev.txt pyproject.toml\n</code></pre> <p>Tip</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. See the user guides (TODO: Add guides) at the bottom of this page for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark()\ndef accuracy(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, using the <code>BenchmarkReporter</code> we report the resulting accuracy metric by printing it to the terminal in a table.</p> <pre><code>import nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\n# To collect in the current file, pass \"__main__\" as module name.\nresult = r.run(\"__main__\", params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test})\nreporter.display(result)\n</code></pre> <p>The resulting output might look like this:</p> <pre><code>python benchmarks.py\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p>"},{"location":"guides/benchmarks/","title":"Defining benchmarks with decorators","text":"<p>To benchmark your machine learning code in nnbench, define your key metrics in Python functions and apply one of the provided decorators. The available decorators are  - <code>@nnbench.benchmark</code>, which runs a benchmark with supplied parameters, - <code>@nnbench.parametrize</code>, which runs several benchmarks with the supplied parameter configurations, - <code>@nnbench.product</code>, which runs benchmarks with all parameter combinations that arise from the supplied values. </p> <p>First we introduce a small machine learning example which we will subsequently use to motivate the use of the three benchmark decorators.</p> <p>We recommend to split the model training, benchmark definition, and benchmark running into different files. In this guide, these are called <code>training.py</code>, <code>benchmarks.py</code>, and <code>main.py</code>.</p>"},{"location":"guides/benchmarks/#example","title":"Example","text":"<p>Let us consider an example where we want to evaluate a <code>scikit-learn</code> random forest classifier on the Iris dataset. For this purpose, we will define several helper functions inside a file, <code>training.py</code>. We use <code>prepare_data()</code>, to load the dataset,  <code>train_rf()</code> to train a random forest model with the specified parameters, and <code>accuracy()</code> to calculate the accuracy of the supplied model on the given dataset.</p> <pre><code># training.py\nimport numpy as np\nfrom sklearn import base, metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_data() -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    data = load_iris()\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    return X_train, X_test, y_train, y_test\n\n\ndef train_rf(X_train: np.ndarray, y_train: np.ndarray, n_estimators: int, max_depth: int, random_state: int = 42) -&gt; RandomForestClassifier:\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre>"},{"location":"guides/benchmarks/#benchmark-for-single-benchmarks","title":"<code>@benchmark</code> for single benchmarks","text":"<p>Now, we define our benchmarks in a new file called <code>benchmarks.py</code>. We first encapsulate the benchmark logic into a function, <code>benchmark_accuracy()</code> which prepares the data, trains a classifier, and lastly, obtains the accuracy. To mark such a function as a benchmark, we apply the <code>@benchmark</code> decorator.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.benchmark()\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Warning</p> <p>This training benchmark is designed as a local, simple, and self-contained example to showcase nnbench.  In a real world scenario, to follow best practices, you may want to separate the data preparation and model training steps from the benchmarking logic and pass the corresponding artifacts as a parameter to the benchmark. See the user guide for more information.</p> <p>Lastly, we set up a benchmark runner in the <code>main.py</code>. There, we supply the parameters (<code>n_estimators</code>, <code>max_depth</code>, <code>random_state</code>) necessary in the function definition as a dictionary to the <code>params</code> keyword argument. </p> <pre><code># main.py\nimport nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42})\nreporter.display(result)\n</code></pre> <p>When we execute the <code>main.py</code> we get the following output:</p> <pre><code>python main.py  \n\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchparametrize-for-multiple-configuration-benchmarks","title":"<code>@nnbench.parametrize</code> for multiple configuration benchmarks","text":"<p>Sometimes, we are not only interested in the performance of a model for given parameters but want to compare the performance for different configurations.  To achieve this, we can turn our single accuracy benchmark in the <code>benchmarks.py</code> file into a parametrized benchmark. To do this, replace the decorator with <code>@nnbench.parametrize</code> and supply the parameter combinations of choice as dictionaries in the first argument.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.parametrize(\n    ({\"n_estimators\": 10, \"max_depth\": 2},\n    {\"n_estimators\": 50, \"max_depth\": 5},\n    {\"n_estimators\": 100, \"max_depth\": 10})\n)\ndef benchmark_accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>Notice that the parametrization is still incomplete, as we did not supply a <code>random_state</code> argument. The unfilled arguments are given in <code>BenchmarkRunner.run()</code> via a dictionary passed as the <code>params</code> keyword argument.</p> <pre><code># main.py\nimport nnbench\n\n\nr = nnbench.BenchmarkRunner()\nreporter = nnbench.BenchmarkReporter()\n\nresult = r.run(\"benchmarks.py\", params={\"random_state\": 42})\nreporter.display(result)\n</code></pre> <p>Executing the parametrized benchmark, we get an output similar to this:</p> <pre><code>python main.py  \n\n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.955556\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.866667\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.911111\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchproduct-for-benchmarks-over-parameter-configuration-grids","title":"<code>@nnbench.product</code> for benchmarks over parameter configuration grids","text":"<p>In case we want to run a benchmark scan for all possible combinations of a set of parameters, we can use the <code>@nnbench.product</code> decorator to supply the different values for each parameter.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy\n\n@nnbench.product(n_estimators=[10, 50, 100], max_depth=[2, 5, 10])\ndef benchmark_accuracy_product(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    acc = accuracy(model=rf, X_test=X_test, y_test=y_test)\n    return acc\n</code></pre> <p>We still provide the <code>random_state</code> parameter to the runner directly, like we did with the <code>@nnbench.parametrize</code> decorator. By executing the benchmark, we get results for all combinations of <code>n_estimators</code> and <code>max_depth</code>. It looks similar to this:</p> <pre><code>python main.py  \n\nname                                                 value\n------------------------------------------------  --------\nbenchmark_accuracy_n_estimators=10_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=10_max_depth=5    0.955556\nbenchmark_accuracy_n_estimators=10_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=50_max_depth=2    0.933333\nbenchmark_accuracy_n_estimators=50_max_depth=5    0.911111\nbenchmark_accuracy_n_estimators=50_max_depth=10   0.977778\nbenchmark_accuracy_n_estimators=100_max_depth=2   0.933333\nbenchmark_accuracy_n_estimators=100_max_depth=5   0.955556\nbenchmark_accuracy_n_estimators=100_max_depth=10  0.955556\n</code></pre>"},{"location":"guides/customization/","title":"Defining setup/teardown tasks, context, and <code>nnbench.Parameters</code>","text":"<p>This page introduces some customization options for benchmark runs. These options can be helpful for tasks surrounding benchmark state management, such as automatic setup and cleanup, contextualizing results with context values, and defining typed parameters with the <code>nnbench.Parameters</code> class.</p>"},{"location":"guides/customization/#defining-setup-and-teardown-tasks","title":"Defining setup and teardown tasks","text":"<p>For some benchmarks, it is important to set certain configuration values and prepare the execution environment before running. To do this, you can pass a setup task to all of the nnbench decorators via the <code>setUp</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\n@nnbench.benchmark(setUp=set_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Similarly, to revert the environment state back to its previous form (or clean up any created resources), you can supply a finalization task with the <code>tearDown</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\ndef pop_envvar(**params):\n    os.environ.pop(\"MY_ENV\")\n\n\n@nnbench.benchmark(setUp=set_envvar, tearDown=pop_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Both the setup and teardown task must take the exact same set of parameters as the benchmark function. To simplify function declaration, it is easiest to use a variadic keyword-only interface, i.e. <code>setup(**kwargs)</code>, as shown.</p> <p>Tip</p> <p>This facility works exactly the same for the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators. There, the specified setup and teardown tasks are run once before or after each of the resulting benchmarks respectively.</p>"},{"location":"guides/customization/#enriching-benchmark-metadata-with-context-values","title":"Enriching benchmark metadata with context values","text":"<p>It is often useful to log specific environment metadata in addition to the benchmark's target metrics. Such metadata can give a clearer picture of how certain models perform on a given hardware, how model architectures compare in performance, and much more. In <code>nnbench</code>, you can give additional metadata to your benchmarks as context values.</p> <p>A context value is defined here as a key-value pair where <code>key</code> is a string, and <code>value</code> is any valid JSON value holding the desired information. As an example, the context value <code>{\"cpuarch\": \"arm64\"}</code> gives information about the CPU architecture of the host machine running the benchmark.</p> <p>A context provider is a function taking no arguments and returning a Python dictionary of context values. The following is a basic example of a context provider:</p> <pre><code>import platform\n\ndef platinfo() -&gt; dict[str, str]:\n    \"\"\"Returns CPU arch, system name (Windows/Linux/Darwin), and Python version.\"\"\"\n    return {\n        \"system\": platform.system(),\n        \"cpuarch\": platform.machine(),\n        \"python_version\": platform.python_version(),\n    }\n</code></pre> <p>To supply context to your benchmarks, you can give a sequence of context providers to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\n# uses the `platinfo` context provider from above to log platform metadata.\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params={}, context=[platinfo])\n</code></pre>"},{"location":"guides/customization/#being-type-safe-by-using-nnbenchparameters","title":"Being type safe by using <code>nnbench.Parameters</code>","text":"<p>Instead of specifying your benchmark's parameters by using a raw Python dictionary, you can define a custom subclass of <code>nnbench.Parameters</code>:</p> <pre><code>import nnbench\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MyParams(nnbench.Parameters):\n    a: int\n    b: int\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\nparams = MyParams(a=1, b=2)\nrunner = nnbench.BenchmarkRunner()\nresult = runner.run(__name__, params=params)\n</code></pre> <p>While this does not have a concrete advantage in terms of type safety over a raw dictionary (all inputs will be checked against the types expected from the benchmark interfaces), it guards against accidental modification of parameters breaking reproducibility.</p>"},{"location":"guides/memoization/","title":"Using memoization for memory efficient benchmarks","text":"<p>In machine learning workloads, models and datasets of greater-than-memory size are frequently encountered. Especially when loading and benchmarking several models in succession, for example with a parametrization, available memory can quickly become a bottleneck.</p> <p>To address this problem, this guide introduces memos as a way to reduce memory pressure when benchmarking multiple memory-intensive models and datasets sequentially.</p>"},{"location":"guides/memoization/#using-the-nnbenchmemo-class","title":"Using the <code>nnbench.Memo</code> class","text":"<p>The key to efficient memory utilization in nnbench is memoization. nnbench itself provides the <code>nnbench.Memo</code> class, a generic base class that can be subclassed to yield a value and cache it for subsequent invocations.</p> <p>To subclass a memo, overload the <code>Memo.__call__()</code> operator like so:</p> <pre><code>import numpy as np\nfrom nnbench.types import Memo, cached_memo\n\nclass MyType:\n    \"\"\"Contains a huge array, similarly to a model.\"\"\"\n    a: np.ndarray = np.zeros((10000, 10000))\n\nclass MyMemo(Memo[MyType]):\n\n    @cached_memo\n    def __call__(self) -&gt; MyType:\n        return MyType()\n</code></pre> <p>The most important part of the above definition is the <code>@cached_memo</code> decorator, which adds the computed value to a module-level cache, from where it can be reused when requested. <code>nnbench.Memo</code> objects do not take any arguments, meaning that all external state necessary to compute the value needs to be passed in the <code>Memo.__init__()</code> function. In this way, nnbench's memos work similarly to e.g. React's useMemo hook.</p> <p>Warning</p> <p>You must explicitly hint the returned type in the <code>Memo.__call__()</code> annotation, which needs to match the generic type specialization (the type in the square brackets in the class definition), otherwise nnbench will throw errors when validating benchmark parameters.</p>"},{"location":"guides/memoization/#supplying-memoized-values-to-benchmarks","title":"Supplying memoized values to benchmarks","text":"<p>Memoization is especially useful when parametrizing benchmarks over models and datasets.</p> <p>Suppose we have a <code>Model</code> class wrapping a large (in the order of available memory) NumPy array. If we have multiple <code>Model</code> instances, but cannot load all of them into memory at the same time in a benchmark run, we can load the (serialized) models lazily using nnbench memos.</p> <pre><code>import gc\n\nimport numpy as np\n\nimport nnbench\nfrom nnbench.types import Memo, cached_memo\nfrom nnbench.types.memo import evict_memo, get_memo_by_value\n\nclass Model:\n    def __init__(self, arr: np.ndarray):\n        self.array = arr\n\n    def apply(self, arr: np.ndarray) -&gt; np.ndarray:\n        return self.array @ arr\n\nclass ModelMemo(Memo[Model]):\n    def __init__(self, path):\n        self.path = path\n\n    @cached_memo\n    def __call__(self) -&gt; Model:\n        arr = np.load(self.path)\n        return Model(arr)\n\n\ndef tearDown(state, params):\n    print(\"Evicting memo for benchmark parameter 'model':\")\n    m = get_memo_by_value(params[\"model\"])\n    if m is not None:\n        evict_memo(m)\n        gc.collect()\n\n\n@nnbench.product(\n    model=[ModelMemo(p) for p in (\"model1.npz\", \"model2.npz\", \"model3.npz\")],\n    tearDown=tearDown,\n)\ndef accuracy(model: Model, data: np.ndarray) -&gt; float:\n    return np.sum(model.apply(data))\n</code></pre> <p>After each benchmark, each model memo's corresponding value is evicted from nnbench's memoization cache in the <code>tearDown</code> task.</p> <p>Warning</p> <p>If you evict a value before its last use in a benchmark, it will be recomputed, potentially slowing down benchmark execution by a lot.</p>"},{"location":"guides/memoization/#summary","title":"Summary","text":"<ul> <li>Use <code>nnbench.Memo</code>s to lazy-load and explicitly control the lifetime of objects with large memory footprint.</li> <li>Annotate memos with their specialized type to avoid problems with nnbench's type checking and parameter validation. </li> <li>Use teardown tasks after benchmarks to evict memoized values from the memo cache.</li> </ul>"},{"location":"guides/organization/","title":"How to efficiently organize benchmark code","text":"<p>To efficiently organize benchmarks and keeping your setup modular, you can follow a few guidelines.</p>"},{"location":"guides/organization/#tip-1-separate-benchmarks-from-project-code","title":"Tip 1: Separate benchmarks from project code","text":"<p>This tip is well known from other software development practices such as unit testing. To improve project organization, consider splitting off your benchmarks into their own modules or even directories, if you have multiple benchmark workloads.</p> <p>An example project layout can look like this, with benchmarks as a separate directory at the top-level:</p> <pre><code>my-project/\n\u251c\u2500\u2500 benchmarks/ # &lt;- contains all benchmarking Python files.\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre> <p>This keeps the benchmarks neatly grouped together while siloing them away from the actual project code. Since you will most likely not run your benchmarks in a production setting, this is also advantageous for packaging, as the <code>benchmarks/</code> directory does not ship by default in this configuration.</p>"},{"location":"guides/organization/#tip-2-group-benchmarks-by-common-attributes","title":"Tip 2: Group benchmarks by common attributes","text":"<p>To maintain good organization within your benchmark directory, you can group similar benchmarks into their own Python files. As an example, if you have a set of benchmarks to establish data quality, and benchmarks for scoring trained models on curated data, you could structure them as follows:</p> <pre><code>benchmarks/\n\u251c\u2500\u2500 data_quality.py\n\u251c\u2500\u2500 model_perf.py\n\u2514\u2500\u2500 ...\n</code></pre> <p>This is helpful when running multiple benchmark workloads separately, as you can just point your benchmark runner to each of these separate files:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\ndata_metrics = runner.run(\"benchmarks/data_quality.py\", params=...)\n# same for model metrics, where instead you pass benchmarks/model_perf.py.\nmodel_metrics = runner.run(\"benchmarks/model_perf.py\", params=...)\n</code></pre>"},{"location":"guides/organization/#tip-3-attach-tags-to-benchmarks-for-selective-filtering","title":"Tip 3: Attach tags to benchmarks for selective filtering","text":"<p>For structuring benchmarks within files, you can also use tags, which are tuples of strings attached to a benchmark:</p> <pre><code># benchmarks/data_quality.py\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo1(data) -&gt; float:\n    ...\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo2(data) -&gt; int:\n    ...\n\n\n@nnbench.benchmark(tags=(\"bar\",))\ndef bar(data) -&gt; int:\n    ...\n</code></pre> <p>Now, to only run data quality benchmarks marked \"foo\", pass the corresponding tag to <code>BenchmarkRunner.run()</code>:</p> <pre><code>import nnbench\n\nrunner = nnbench.BenchmarkRunner()\nfoo_data_metrics = runner.run(\"benchmarks/data_quality.py\", params=..., tags=(\"foo\",))\n</code></pre> <p>Tip</p> <p>This concept works exactly the same when creating benchmarks with the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators.</p>"},{"location":"guides/runners/","title":"Collecting and running benchmarks","text":"<p>nnbench provides the <code>BenchmarkRunner</code> as a compact interface to collect and run benchmarks selectively.</p>"},{"location":"guides/runners/#the-abstract-benchmarkrunner-class","title":"The abstract <code>BenchmarkRunner</code>  class","text":"<p>Let's first instantiate and then walk through the base class.</p> <pre><code>from nnbench import BenchmarkRunner\n\nrunner = BenchmarkRunner()\n</code></pre> <p>Use the <code>BenchmarkRunner.collect()</code> method to collect benchmarks from files or directories. Assume we have the following benchmark setup: <pre><code># dir_a/bm1.py\nimport nnbench\n\n@nnbench.benchmark\ndef dummy_benchmark(a: int) -&gt; int:\n    return a\n</code></pre></p> <pre><code># dir_b/bm2.py\nimport nnbench\n\n@nnbench.benchmark(tags=(\"tag\",))\ndef another_benchmark(b: int) -&gt; int:\n    return b\n\n@nnbench.benchmark\ndef yet_another_benchmark(c: int) -&gt; int:\n    return c\n</code></pre> <pre><code># dir_b/bm3.py\nimport nnbench\n@nnbench.benchmark(tags=(\"tag\",))\ndef the_last_benchmark(d: int) -&gt; int:\n    return d\n</code></pre> <p>Now we can collect benchmarks from files:</p> <p><pre><code>runner.collect('dir_a/bm1.py')\n</code></pre> Or directories:</p> <pre><code>runner.collect('dir_b')\n</code></pre> <p>This collection can happen iteratively. So, after executing the two collections our runner has all four benchmarks ready for execution.</p> <p>To remove the collected benchmarks again, use the <code>BenchmarkRunner.clear()</code> method. You can also supply tags to the runner to selectively collect only benchmarks with the appropriate tag. For example, after clearing the runner again, you can collect all benchmarks with the <code>\"tag\"</code> tag as such:</p> <pre><code>runner.collect('dir_b', tags=(\"tag\",))\n</code></pre> <p>To run the benchmarks, call the <code>BenchmarkRunner.run()</code> method and supply the necessary parameters required by the collected benchmarks.</p> <pre><code>runner.run(\"dir_b\", params={\"b\": 1, \"c\": 2, \"d\": 3})\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>cli</li> <li>compare</li> <li>context</li> <li>core</li> <li>reporter<ul> <li>base</li> <li>console</li> <li>duckdb_sql</li> <li>file</li> <li>util</li> </ul> </li> <li>runner</li> <li>types<ul> <li>benchmark</li> <li>interface</li> <li>memo</li> </ul> </li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/#nnbench.default_runner","title":"default_runner","text":"<pre><code>default_runner() -&gt; BenchmarkRunner\n</code></pre> Source code in <code>src/nnbench/__init__.py</code> <pre><code>def default_runner() -&gt; BenchmarkRunner:\n    return BenchmarkRunner()\n</code></pre>"},{"location":"reference/nnbench/#nnbench.default_reporter","title":"default_reporter","text":"<pre><code>default_reporter() -&gt; BenchmarkReporter\n</code></pre> Source code in <code>src/nnbench/__init__.py</code> <pre><code>def default_reporter() -&gt; BenchmarkReporter:\n    return ConsoleReporter()\n</code></pre>"},{"location":"reference/nnbench/#modules","title":"Modules","text":""},{"location":"reference/nnbench/#cli","title":"cli","text":""},{"location":"reference/nnbench/#compare","title":"compare","text":""},{"location":"reference/nnbench/#context","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/#core","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/#reporter","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/#runner","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/#types","title":"types","text":""},{"location":"reference/nnbench/#util","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/cli/","title":"cli","text":""},{"location":"reference/nnbench/cli/#nnbench.cli.CustomFormatter","title":"CustomFormatter","text":"<p>               Bases: <code>RawDescriptionHelpFormatter</code></p> Source code in <code>src/nnbench/cli.py</code> <pre><code>class CustomFormatter(argparse.RawDescriptionHelpFormatter):\n    def _format_action_invocation(self, action):\n        if not action.option_strings:\n            (metavar,) = self._metavar_formatter(action, action.dest)(1)\n            return metavar\n        elif isinstance(action, argparse.BooleanOptionalAction):\n            if len(action.option_strings) == 2:\n                true_opt, false_opt = action.option_strings\n                return \"--[no-]\" + true_opt[2:]\n        else:\n            parts = []\n            # if the Optional doesn't take a value, format is:\n            #    -s, --long\n            if action.nargs == 0:\n                parts.extend(action.option_strings)\n\n            # if the Optional takes a value, format is:\n            #    -s, --long ARGS\n            else:\n                default = action.dest.upper()\n                args_string = self._format_args(action, default)\n                parts.extend(action.option_strings)\n                parts[-1] += f\" {args_string}\"\n            return \", \".join(parts)\n</code></pre>"},{"location":"reference/nnbench/cli/#nnbench.cli.main","title":"main","text":"<pre><code>main() -&gt; int\n</code></pre> Source code in <code>src/nnbench/cli.py</code> <pre><code>def main() -&gt; int:\n    parser = argparse.ArgumentParser(\"nnbench\", formatter_class=CustomFormatter)\n    parser.add_argument(\"--version\", action=\"version\", version=_VERSION)\n    subparsers = parser.add_subparsers(\n        title=\"Available commands\",\n        required=False,\n        dest=\"command\",\n        metavar=\"\",\n    )\n    run_parser = subparsers.add_parser(\n        \"run\", help=\"Run a benchmark workload.\", formatter_class=CustomFormatter\n    )\n    # can be a directory, single file, or glob\n    run_parser.add_argument(\n        \"benchmarks\",\n        nargs=\"?\",\n        metavar=\"&lt;benchmarks&gt;\",\n        help=\"Python file or directory of files containing benchmarks to run.\",\n        default=\"benchmarks\",\n    )\n    run_parser.add_argument(\n        \"--context\",\n        action=\"append\",\n        metavar=\"&lt;key=value&gt;\",\n        help=\"Additional context values giving information about the benchmark run.\",\n        default=list(),\n    )\n    run_parser.add_argument(\n        \"-t\",\n        \"--tag\",\n        action=\"append\",\n        metavar=\"&lt;tag&gt;\",\n        dest=\"tags\",\n        help=\"Only run benchmarks marked with one or more given tag(s).\",\n        default=tuple(),\n    )\n    run_parser.add_argument(\n        \"-o\",\n        \"--output-file\",\n        metavar=\"&lt;file&gt;\",\n        dest=\"outfile\",\n        help=\"File or stream to write results to, defaults to stdout.\",\n        default=sys.stdout,\n    )\n    run_parser.add_argument(\n        \"--typecheck\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Whether or not to strictly check types of benchmark inputs.\",\n    )\n\n    compare_parser = subparsers.add_parser(\n        \"compare\",\n        help=\"Compare results from multiple benchmark runs.\",\n        formatter_class=CustomFormatter,\n    )\n    compare_parser.add_argument(\n        \"records\",\n        nargs=\"+\",\n        help=\"Records to compare results for. Can be given as local files or remote URIs.\",\n    )\n    compare_parser.add_argument(\n        \"-P\",\n        \"--include-parameter\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"parameters\",\n        default=list(),\n        help=\"Names of input parameters to display in the comparison table.\",\n    )\n    compare_parser.add_argument(\n        \"-C\",\n        \"--include-context\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"contextvals\",\n        default=list(),\n        help=\"Context values to display in the comparison table. Use dotted syntax for nested context values.\",\n    )\n    compare_parser.add_argument(\n        \"-E\",\n        \"--extra-column\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"extra_cols\",\n        default=list(),\n        help=\"Additional record data to display in the comparison table.\",\n    )\n    # TODO: Add customization option for rich table displays\n\n    try:\n        args = parser.parse_args()\n        if args.command == \"run\":\n            context: dict[str, Any] = {}\n            for val in args.context:\n                try:\n                    k, v = val.split(\"=\")\n                except ValueError:\n                    raise ValueError(\"context values need to be of the form &lt;key&gt;=&lt;value&gt;\")\n                # TODO: Support builtin providers in the runner\n                context[k] = v\n\n            record = BenchmarkRunner(typecheck=args.typecheck).run(\n                args.benchmarks,\n                tags=tuple(args.tags),\n                context=[lambda: context],\n            )\n\n            outfile = args.outfile\n            if outfile == sys.stdout:\n                reporter = ConsoleReporter()\n                reporter.display(record)\n            else:\n                f = FileReporter()\n                f.write(record, outfile)\n        elif args.command == \"compare\":\n            from nnbench.compare import compare\n\n            f = FileReporter()\n            records = [f.read(file) for file in args.records]\n            compare(\n                records=records,\n                parameters=args.parameters,\n                contextvals=args.contextvals,\n            )\n\n        return 0\n    except Exception as e:\n        sys.stderr.write(f\"error: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/nnbench/compare/","title":"compare","text":""},{"location":"reference/nnbench/compare/#nnbench.compare.get_value_by_name","title":"get_value_by_name","text":"<pre><code>get_value_by_name(record: BenchmarkRecord, name: str, missing: str) -&gt; str\n</code></pre> Source code in <code>src/nnbench/compare.py</code> <pre><code>def get_value_by_name(record: BenchmarkRecord, name: str, missing: str) -&gt; str:\n    metric_names = [b[\"name\"] for b in record.benchmarks]\n    if name not in metric_names:\n        return missing\n\n    res = record.benchmarks[metric_names.index(name)]\n    if res.get(\"error_occurred\", False):\n        errmsg = res.get(\"error_message\", \"&lt;unknown&gt;\")\n        return \"[red]ERROR: [/red]\" + errmsg\n    return str(res.get(\"value\", missing))\n</code></pre>"},{"location":"reference/nnbench/compare/#nnbench.compare.compare","title":"compare","text":"<pre><code>compare(\n    records: Sequence[BenchmarkRecord],\n    parameters: Sequence[str] | None = None,\n    contextvals: Sequence[str] | None = None,\n    missing: str = _MISSING,\n) -&gt; None\n</code></pre> Source code in <code>src/nnbench/compare.py</code> <pre><code>def compare(\n    records: Sequence[BenchmarkRecord],\n    parameters: Sequence[str] | None = None,\n    contextvals: Sequence[str] | None = None,\n    missing: str = _MISSING,\n) -&gt; None:\n    t = Table()\n\n    rows: list[list[str]] = []\n    columns: list[str] = []\n\n    # Add metric names first, without duplicates.\n    for record in records:\n        names = [b[\"name\"] for b in record.benchmarks]\n        for name in names:\n            if name not in set(columns):\n                columns.append(name)\n\n    names = copy.deepcopy(columns)\n\n    # Then parameters, if any\n    if parameters is not None:\n        columns += [f\"Params-&gt;{p}\" for p in parameters]\n\n    if contextvals is not None:\n        columns += contextvals\n\n    # Main loop, extracts values from the individual records,\n    # or a placeholder if there are any.\n    for record in records:\n        # TODO: Add record-level run name, extract\n        # flatten facilitates dotted access to nested context values, e.g. git.branch\n        ctx = flatten(record.context)\n        row = [get_value_by_name(record, name, _MISSING) for name in names]\n        # hacky, extra cols is likely now broken\n        b = record.benchmarks[0]\n        # TODO: Add record-level parameters struct as the union of all benchmark inputs\n        if parameters is not None:\n            params = b.get(\"parameters\", {})\n            row += [str(params.get(p)) for p in parameters]\n        if contextvals is not None:\n            row += [str(ctx.get(cval, missing)) for cval in contextvals]\n        rows.append(row)\n\n    for column in columns:\n        t.add_column(column)\n    for row in rows:\n        t.add_row(*row)\n\n    c = Console()\n    c.print(t)\n</code></pre>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], dict[str, Any]]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo","title":"PythonInfo","text":"<p>A context helper returning version info for requested installed packages.</p> <p>If a requested package is not installed, an empty string is returned instead.</p> PARAMETER DESCRIPTION <code>*packages</code> <p>Names of the requested packages under which they exist in the current environment. For packages installed through <code>pip</code>, this equals the PyPI package name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>()</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class PythonInfo:\n    \"\"\"\n    A context helper returning version info for requested installed packages.\n\n    If a requested package is not installed, an empty string is returned instead.\n\n    Parameters\n    ----------\n    *packages: str\n        Names of the requested packages under which they exist in the current environment.\n        For packages installed through ``pip``, this equals the PyPI package name.\n    \"\"\"\n\n    key = \"python\"\n\n    def __init__(self, *packages: str):\n        self.packages = packages\n\n    def __call__(self) -&gt; dict[str, Any]:\n        from importlib.metadata import PackageNotFoundError, version\n\n        result: dict[str, Any] = dict()\n\n        result[\"version\"] = platform.python_version()\n        result[\"implementation\"] = platform.python_implementation()\n        buildno, buildtime = platform.python_build()\n        result[\"buildno\"] = buildno\n        result[\"buildtime\"] = buildtime\n\n        dependencies: dict[str, str] = {}\n        for pkg in self.packages:\n            try:\n                dependencies[pkg] = version(pkg)\n            except PackageNotFoundError:\n                dependencies[pkg] = \"\"\n\n        result[\"dependencies\"] = dependencies\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'python'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages = packages\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo","title":"GitEnvironmentInfo","text":"<p>A context helper providing the current git commit, latest tag, and upstream repository name.</p> PARAMETER DESCRIPTION <code>remote</code> <p>Remote name for which to provide info, by default <code>\"origin\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class GitEnvironmentInfo:\n    \"\"\"\n    A context helper providing the current git commit, latest tag, and upstream repository name.\n\n    Parameters\n    ----------\n    remote: str\n        Remote name for which to provide info, by default ``\"origin\"``.\n    \"\"\"\n\n    key = \"git\"\n\n    def __init__(self, remote: str = \"origin\"):\n        self.remote = remote\n\n    def __call__(self) -&gt; dict[str, dict[str, Any]]:\n        import subprocess\n\n        def git_subprocess(args: list[str]) -&gt; subprocess.CompletedProcess:\n            if platform.system() == \"Windows\":\n                git = \"git.exe\"\n            else:\n                git = \"git\"\n\n            return subprocess.run(  # nosec: B603\n                [git, *args],\n                capture_output=True,\n                encoding=\"utf-8\",\n            )\n\n        result: dict[str, Any] = {\n            \"commit\": \"\",\n            \"provider\": \"\",\n            \"repository\": \"\",\n            \"tag\": \"\",\n            \"dirty\": None,\n        }\n\n        # first, check if inside a repo.\n        p = git_subprocess([\"rev-parse\", \"--is-inside-work-tree\"])\n        # if not, return empty info.\n        if p.returncode:\n            return {\"git\": result}\n\n        # secondly: get the current commit.\n        p = git_subprocess([\"rev-parse\", \"HEAD\"])\n        if not p.returncode:\n            result[\"commit\"] = p.stdout.strip()\n\n        # thirdly, get the latest tag, without a short commit SHA attached.\n        p = git_subprocess([\"describe\", \"--tags\", \"--abbrev=0\"])\n        if not p.returncode:\n            result[\"tag\"] = p.stdout.strip()\n\n        # and finally, get the remote repo name pointed to by the given remote.\n        p = git_subprocess([\"remote\", \"get-url\", self.remote])\n        if not p.returncode:\n            remotename: str = p.stdout.strip()\n            if \"@\" in remotename:\n                # it's an SSH remote.\n                prefix, sep = \"git@\", \":\"\n            else:\n                # it is HTTPS.\n                prefix, sep = \"https://\", \"/\"\n\n            remotename = remotename.removeprefix(prefix)\n            provider, reponame = remotename.split(sep, 1)\n\n            result[\"provider\"] = provider\n            result[\"repository\"] = reponame.removesuffix(\".git\")\n\n        p = git_subprocess([\"status\", \"--porcelain\"])\n        if not p.returncode:\n            result[\"dirty\"] = bool(p.stdout.strip())\n\n        return {\"git\": result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'git'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.remote","title":"remote  <code>instance-attribute</code>","text":"<pre><code>remote = remote\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo","title":"CPUInfo","text":"Source code in <code>src/nnbench/context.py</code> <pre><code>class CPUInfo:\n    key = \"cpu\"\n\n    def __init__(\n        self,\n        memunit: Literal[\"kB\", \"MB\", \"GB\"] = \"MB\",\n        frequnit: Literal[\"kHz\", \"MHz\", \"GHz\"] = \"MHz\",\n    ):\n        self.memunit = memunit\n        self.frequnit = frequnit\n        self.conversion_table: dict[str, float] = {\"k\": 1e3, \"M\": 1e6, \"G\": 1e9}\n\n    def __call__(self) -&gt; dict[str, Any]:\n        try:\n            import psutil\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"context provider {self.__class__.__name__}() needs `psutil` installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade psutil`.\"\n            )\n\n        result: dict[str, Any] = dict()\n\n        # first, the platform info.\n        result[\"architecture\"] = platform.machine()\n        result[\"bitness\"] = platform.architecture()[0]\n        result[\"processor\"] = platform.processor()\n        result[\"system\"] = platform.system()\n        result[\"system-version\"] = platform.release()\n\n        try:\n            # The CPU frequency is not available on some ARM devices\n            freq_struct = psutil.cpu_freq()\n            result[\"min_frequency\"] = freq_struct.min\n            result[\"max_frequency\"] = freq_struct.max\n            freq_conversion = self.conversion_table[self.frequnit[0]]\n            # result is in MHz, so we convert to Hz and apply the conversion factor.\n            result[\"frequency\"] = freq_struct.current * 1e6 / freq_conversion\n        except RuntimeError:\n            result[\"frequency\"] = 0\n            result[\"min_frequency\"] = 0\n            result[\"max_frequency\"] = 0\n\n        result[\"frequency_unit\"] = self.frequnit\n        result[\"num_cpus\"] = psutil.cpu_count(logical=False)\n        result[\"num_logical_cpus\"] = psutil.cpu_count()\n\n        mem_struct = psutil.virtual_memory()\n        mem_conversion = self.conversion_table[self.memunit[0]]\n        # result is in bytes, so no need for base conversion.\n        result[\"total_memory\"] = mem_struct.total / mem_conversion\n        result[\"memory_unit\"] = self.memunit\n        # TODO: Lacks CPU cache info, which requires a solution other than psutil.\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'cpu'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.memunit","title":"memunit  <code>instance-attribute</code>","text":"<pre><code>memunit = memunit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.frequnit","title":"frequnit  <code>instance-attribute</code>","text":"<pre><code>frequnit = frequnit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.conversion_table","title":"conversion_table  <code>instance-attribute</code>","text":"<pre><code>conversion_table: dict[str, float] = {'k': 1000.0, 'M': 1000000.0, 'G': 1000000000.0}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], Benchmark]\n</code></pre><pre><code>benchmark(\n    func: Callable[..., Any],\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark\n</code></pre> <pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>BenchmarkRunner.run()</code>.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>A display name to give to the benchmark. Useful in summaries and reports.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    name: str\n        A display name to give to the benchmark. Useful in summaries and reports.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER DESCRIPTION <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        for params in parameters:\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            bm = Benchmark(\n                fn,\n                name=name,\n                params=params,\n                setUp=setUp,\n                tearDown=tearDown,\n                tags=tags,\n            )\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; Callable[[Callable], list[Benchmark]]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>BenchmarkRunner.run()</code>.</p> PARAMETER DESCRIPTION <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], list[Benchmark]]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; Callable[[Callable], list[Benchmark]]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `BenchmarkRunner.run()`.\n\n    Parameters\n    ----------\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    Callable[[Callable], list[Benchmark]]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; list[Benchmark]:\n        benchmarks = []\n        names = set()\n        varnames = iterables.keys()\n        for values in itertools.product(*iterables.values()):\n            params = dict(zip(varnames, values))\n            _check_against_interface(params, fn)\n\n            name = namegen(fn, **params)\n            if name in names:\n                warnings.warn(\n                    f\"Got duplicate name {name!r} for benchmark {fn.__name__}(). \"\n                    f\"Perhaps you specified a parameter configuration twice?\"\n                )\n            names.add(name)\n\n            bm = Benchmark(\n                fn,\n                name=name,\n                params=params,\n                setUp=setUp,\n                tearDown=tearDown,\n                tags=tags,\n            )\n            benchmarks.append(bm)\n        return benchmarks\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner","title":"BenchmarkRunner","text":"<p>An abstract benchmark runner class.</p> <p>Collects benchmarks from a module or file using the collect() method. Runs a previously collected benchmark workload with parameters in the run() method, outputting the results to a JSON-like document.</p> <p>Optionally checks input parameters against the benchmark function's interfaces, raising an error if the input types do not match the expected types.</p> PARAMETER DESCRIPTION <code>typecheck</code> <p>Whether to check parameter types against the expected benchmark input types. Type mismatches will result in an error before the workload is run.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>params_repr_hooks</code> <p>A mapping of data types to functions computing a compressed representation of benchmark parameters of said types. Used in self.params_repr().</p> <p> TYPE: <code>dict[type, Callable]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nnbench/runner.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"\n    An abstract benchmark runner class.\n\n    Collects benchmarks from a module or file using the collect() method.\n    Runs a previously collected benchmark workload with parameters in the run() method,\n    outputting the results to a JSON-like document.\n\n    Optionally checks input parameters against the benchmark function's interfaces,\n    raising an error if the input types do not match the expected types.\n\n    Parameters\n    ----------\n    typecheck: bool\n        Whether to check parameter types against the expected benchmark input types.\n        Type mismatches will result in an error before the workload is run.\n    params_repr_hooks: dict[type, Callable]\n        A mapping of data types to functions computing a compressed representation\n        of benchmark parameters of said types. Used in self.params_repr().\n    \"\"\"\n\n    benchmark_type = Benchmark\n\n    def __init__(self, typecheck: bool = True, params_repr_hooks: dict[type, Callable] = None):\n        self.benchmarks: list[Benchmark] = list()\n        self.typecheck = typecheck\n        self._params_repr_hooks = params_repr_hooks or {}\n\n    def register_repr_hook(self, typ: type, fn: Callable) -&gt; None:\n        self._params_repr_hooks[typ] = fn\n\n    def deregister_repr_hook(self, typ: type) -&gt; Callable | None:\n        return self._params_repr_hooks.pop(typ, None)\n\n    def _check(self, params: dict[str, Any]) -&gt; None:\n        allvars: dict[str, tuple[type, Any]] = {}\n        required: set[str] = set()\n        empty = inspect.Parameter.empty\n\n        def _issubtype(t1: type, t2: type) -&gt; bool:\n            \"\"\"Small helper to make typechecks work on generics.\"\"\"\n\n            if t1 == t2:\n                return True\n\n            t1 = get_origin(t1) or t1\n            t2 = get_origin(t2) or t2\n            if not inspect.isclass(t1):\n                return False\n            # TODO: Extend typing checks to args.\n            return issubclass(t1, t2)\n\n        # stitch together the union interface comprised of all benchmarks.\n        for bm in self.benchmarks:\n            for var in bm.interface.variables:\n                name, typ, default = var\n                if default is empty:\n                    required.add(name)\n                if name in params and default != empty:\n                    logger.debug(\n                        f\"using given value {params[name]} over default value {default} \"\n                        f\"for parameter {name!r} in benchmark {bm.name}()\"\n                    )\n\n                if typ == empty:\n                    logger.debug(f\"parameter {name!r} untyped in benchmark {bm.name}().\")\n\n                if name in allvars:\n                    currvar = allvars[name]\n                    orig_type, orig_val = new_type, new_val = currvar\n                    # If a benchmark has a variable without a default value,\n                    # that variable is taken into the combined interface as no-default.\n                    if default is empty:\n                        new_val = default\n                    # These types need not be exact matches, just compatible.\n                    # Two types are compatible iff either is a subtype of the other.\n                    # We only log the narrowest type for each varname in the final interface,\n                    # since that determines whether an input value is admissible.\n                    if _issubtype(orig_type, typ):\n                        pass\n                    elif _issubtype(typ, orig_type):\n                        new_type = typ\n                    else:\n                        raise TypeError(\n                            f\"got incompatible types {orig_type}, {typ} for parameter {name!r}\"\n                        )\n                    newvar = (new_type, new_val)\n                    if newvar != currvar:\n                        allvars[name] = newvar\n                else:\n                    allvars[name] = (typ, default)\n\n        # check if any required variable has no parameter.\n        missing = required - params.keys()\n        if missing:\n            msng, *_ = missing\n            raise ValueError(f\"missing value for required parameter {msng!r}\")\n\n        # TODO(n.junge): This doesn't pick up mistyped defaults\n        #  (admittedly, that's likely user error)\n        for k, v in params.items():\n            if k not in allvars:\n                warnings.warn(\n                    f\"ignoring parameter {k!r} since it is not part of any benchmark interface.\"\n                )\n                continue\n\n            typ, default = allvars[k]\n            # skip the subsequent type check if the variable is untyped.\n            if typ == empty:\n                continue\n\n            vtype = type(v)\n            if is_memo(v) and not is_memo_type(typ):\n                # in case of a thunk, check the result type of __call__() instead.\n                vtype = inspect.signature(v).return_annotation\n\n            # type-check parameter value against the narrowest hinted type.\n            if not _issubtype(vtype, typ):\n                raise TypeError(f\"expected type {typ} for parameter {k!r}, got {vtype}\")\n\n    def params_repr(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Compute a compressed representation of benchmark parameters.\n\n        This is necessary to break reference cycles from the parameters to the records,\n        which prevent garbage collection of memory-intensive values.\n\n        Parameters\n        ----------\n        params: dict[str, Any]\n            Benchmark parameters to compute a compressed representation of.\n\n        Returns\n        -------\n        dict[str, Any]\n            A compressed representation of the benchmark input parameters.\n        \"\"\"\n        containers = (tuple, list, set, frozenset)\n        natives = (float, int, str, bool, bytes, complex)\n        compressed: dict[str, Any] = {}\n\n        def _compress_impl(val):\n            vtype = type(val)\n            if vtype in self._params_repr_hooks:\n                return self._params_repr_hooks[vtype](val)\n            if isinstance(val, natives):\n                # save native types without modification...\n                return val\n            else:\n                # ... or return the string repr.\n                return repr(val)\n\n        for k, v in params.items():\n            if isinstance(v, containers):\n                container_type = type(v)\n                compressed[k] = container_type(_compress_impl(vv) for vv in v)\n            elif isinstance(v, dict):\n                compressed[k] = self.params_repr(v)\n            else:\n                compressed[k] = _compress_impl(v)\n        return compressed\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all registered benchmarks.\"\"\"\n        self.benchmarks.clear()\n\n    def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n        # TODO: functools.cache this guy\n        \"\"\"\n        Discover benchmarks in a module and memoize them for later use.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n\n        Raises\n        ------\n        ValueError\n            If the given path is not a Python file, directory, or module name.\n        \"\"\"\n        ppath = Path(path_or_module)\n        if ppath.is_dir():\n            pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n            for py in pythonpaths:\n                logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n                self.collect(py, tags)\n            return\n        elif ppath.is_file():\n            module = import_file_as_module(path_or_module)\n        elif ismodule(path_or_module):\n            module = sys.modules[str(path_or_module)]\n        else:\n            raise ValueError(\n                f\"expected a module name, Python file, or directory, \"\n                f\"got {str(path_or_module)!r}\"\n            )\n\n        # iterate through the module dict members to register\n        for k, v in module.__dict__.items():\n            if k.startswith(\"__\") and k.endswith(\"__\"):\n                # dunder names are ignored.\n                continue\n            elif isinstance(v, self.benchmark_type):\n                if not tags or set(tags) &amp; set(v.tags):\n                    self.benchmarks.append(v)\n            elif isinstance(v, list | tuple | set | frozenset):\n                for bm in v:\n                    if isinstance(bm, self.benchmark_type):\n                        if not tags or set(tags) &amp; set(bm.tags):\n                            self.benchmarks.append(bm)\n\n    def run(\n        self,\n        path_or_module: str | os.PathLike[str],\n        params: dict[str, Any] | Parameters | None = None,\n        tags: tuple[str, ...] = (),\n        context: Sequence[ContextProvider] = (),\n        name: str | None = None,\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Run a previously collected benchmark workload.\n\n        Parameters\n        ----------\n        path_or_module: str | os.PathLike[str]\n            Name or path of the module to discover benchmarks in. Can also be a directory,\n            in which case benchmarks are collected from the Python files therein.\n        params: dict[str, Any] | Parameters | None\n            Parameters to use for the benchmark run. Names have to match positional and keyword\n            argument names of the benchmark functions.\n        tags: tuple[str, ...]\n            Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n            these tags are collected.\n        context: Sequence[ContextProvider]\n            Additional context to log with the benchmark in the output JSON record. Useful for\n            obtaining environment information and configuration, like CPU/GPU hardware info,\n            ML model metadata, and more.\n        name: str | None\n            A name for the currently started run. If None, a name will be automatically generated.\n\n        Returns\n        -------\n        BenchmarkRecord\n            A JSON output representing the benchmark results. Has three top-level keys,\n            \"name\" giving the benchmark run name, \"context\" holding the context information,\n            and \"benchmarks\", holding an array with the benchmark results.\n        \"\"\"\n        name = name or \"nnbench-\" + platform.node() + \"-\" + uuid.uuid1().hex[:8]\n\n        if not self.benchmarks:\n            self.collect(path_or_module, tags)\n\n        family_sizes: dict[str, Any] = collections.defaultdict(int)\n        family_indices: dict[str, Any] = collections.defaultdict(int)\n\n        ctx: dict[str, Any] = {}\n        for provider in context:\n            val = provider()\n            duplicates = set(ctx.keys()) &amp; set(val.keys())\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx.update(val)\n\n        # if we didn't find any benchmarks, warn and return an empty record.\n        if not self.benchmarks:\n            warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n            return BenchmarkRecord(name=name, context=ctx, benchmarks=[])\n\n        for bm in self.benchmarks:\n            family_sizes[bm.fn.__name__] += 1\n\n        if isinstance(params, Parameters):\n            dparams = asdict(params)\n        else:\n            dparams = params or {}\n\n        if self.typecheck:\n            self._check(dparams)\n\n        results: list[dict[str, Any]] = []\n\n        def _maybe_dememo(v, expected_type):\n            \"\"\"Compute and memoize a value if a memo is given for a variable.\"\"\"\n            if is_memo(v) and not is_memo_type(expected_type):\n                return v()\n            return v\n\n        for benchmark in self.benchmarks:\n            bm_family = benchmark.fn.__name__\n            state = State(\n                name=benchmark.name,\n                family=bm_family,\n                family_size=family_sizes[bm_family],\n                family_index=family_indices[bm_family],\n            )\n            family_indices[bm_family] += 1\n            bmtypes = dict(zip(benchmark.interface.names, benchmark.interface.types))\n            bmparams = dict(zip(benchmark.interface.names, benchmark.interface.defaults))\n            bmparams |= {k: v for k, v in dparams.items() if k in bmparams}\n            bmparams = {k: _maybe_dememo(v, bmtypes[k]) for k, v in bmparams.items()}\n\n            # TODO: Wrap this into an execution context\n            res: dict[str, Any] = {\n                \"name\": benchmark.name,\n                \"function\": qualname(benchmark.fn),\n                \"description\": benchmark.fn.__doc__ or \"\",\n                \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n                \"error_occurred\": False,\n                \"error_message\": \"\",\n                \"parameters\": self.params_repr(bmparams),\n            }\n            try:\n                benchmark.setUp(state, bmparams)\n                with timer(res):\n                    res[\"value\"] = benchmark.fn(**bmparams)\n            except Exception as e:\n                res[\"error_occurred\"] = True\n                res[\"error_message\"] = str(e)\n            finally:\n                benchmark.tearDown(state, bmparams)\n                results.append(res)\n\n        return BenchmarkRecord(\n            name=name,\n            context=ctx,\n            benchmarks=results,\n        )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmark_type","title":"benchmark_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>benchmark_type = Benchmark\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[Benchmark] = list()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.typecheck","title":"typecheck  <code>instance-attribute</code>","text":"<pre><code>typecheck = typecheck\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.register_repr_hook","title":"register_repr_hook","text":"<pre><code>register_repr_hook(typ: type, fn: Callable) -&gt; None\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def register_repr_hook(self, typ: type, fn: Callable) -&gt; None:\n    self._params_repr_hooks[typ] = fn\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.deregister_repr_hook","title":"deregister_repr_hook","text":"<pre><code>deregister_repr_hook(typ: type) -&gt; Callable | None\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def deregister_repr_hook(self, typ: type) -&gt; Callable | None:\n    return self._params_repr_hooks.pop(typ, None)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.params_repr","title":"params_repr","text":"<pre><code>params_repr(params: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Compute a compressed representation of benchmark parameters.</p> <p>This is necessary to break reference cycles from the parameters to the records, which prevent garbage collection of memory-intensive values.</p> PARAMETER DESCRIPTION <code>params</code> <p>Benchmark parameters to compute a compressed representation of.</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A compressed representation of the benchmark input parameters.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def params_repr(self, params: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Compute a compressed representation of benchmark parameters.\n\n    This is necessary to break reference cycles from the parameters to the records,\n    which prevent garbage collection of memory-intensive values.\n\n    Parameters\n    ----------\n    params: dict[str, Any]\n        Benchmark parameters to compute a compressed representation of.\n\n    Returns\n    -------\n    dict[str, Any]\n        A compressed representation of the benchmark input parameters.\n    \"\"\"\n    containers = (tuple, list, set, frozenset)\n    natives = (float, int, str, bool, bytes, complex)\n    compressed: dict[str, Any] = {}\n\n    def _compress_impl(val):\n        vtype = type(val)\n        if vtype in self._params_repr_hooks:\n            return self._params_repr_hooks[vtype](val)\n        if isinstance(val, natives):\n            # save native types without modification...\n            return val\n        else:\n            # ... or return the string repr.\n            return repr(val)\n\n    for k, v in params.items():\n        if isinstance(v, containers):\n            container_type = type(v)\n            compressed[k] = container_type(_compress_impl(vv) for vv in v)\n        elif isinstance(v, dict):\n            compressed[k] = self.params_repr(v)\n        else:\n            compressed[k] = _compress_impl(v)\n    return compressed\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all registered benchmarks.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all registered benchmarks.\"\"\"\n    self.benchmarks.clear()\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.collect","title":"collect","text":"<pre><code>collect(path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()) -&gt; None\n</code></pre> <p>Discover benchmarks in a module and memoize them for later use.</p> PARAMETER DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(self, path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()) -&gt; None:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module and memoize them for later use.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    ppath = Path(path_or_module)\n    if ppath.is_dir():\n        pythonpaths = (p for p in ppath.iterdir() if p.suffix == \".py\")\n        for py in pythonpaths:\n            logger.debug(f\"Collecting benchmarks from submodule {py.name!r}.\")\n            self.collect(py, tags)\n        return\n    elif ppath.is_file():\n        module = import_file_as_module(path_or_module)\n    elif ismodule(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        raise ValueError(\n            f\"expected a module name, Python file, or directory, \"\n            f\"got {str(path_or_module)!r}\"\n        )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if k.startswith(\"__\") and k.endswith(\"__\"):\n            # dunder names are ignored.\n            continue\n        elif isinstance(v, self.benchmark_type):\n            if not tags or set(tags) &amp; set(v.tags):\n                self.benchmarks.append(v)\n        elif isinstance(v, list | tuple | set | frozenset):\n            for bm in v:\n                if isinstance(bm, self.benchmark_type):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        self.benchmarks.append(bm)\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.BenchmarkRunner.run","title":"run","text":"<pre><code>run(\n    path_or_module: str | PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n    name: str | None = None,\n) -&gt; BenchmarkRecord\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Parameters | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>context</code> <p>Additional context to log with the benchmark in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Sequence[ContextProvider]</code> DEFAULT: <code>()</code> </p> <code>name</code> <p>A name for the currently started run. If None, a name will be automatically generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>A JSON output representing the benchmark results. Has three top-level keys, \"name\" giving the benchmark run name, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    self,\n    path_or_module: str | os.PathLike[str],\n    params: dict[str, Any] | Parameters | None = None,\n    tags: tuple[str, ...] = (),\n    context: Sequence[ContextProvider] = (),\n    name: str | None = None,\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    params: dict[str, Any] | Parameters | None\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n    context: Sequence[ContextProvider]\n        Additional context to log with the benchmark in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n    name: str | None\n        A name for the currently started run. If None, a name will be automatically generated.\n\n    Returns\n    -------\n    BenchmarkRecord\n        A JSON output representing the benchmark results. Has three top-level keys,\n        \"name\" giving the benchmark run name, \"context\" holding the context information,\n        and \"benchmarks\", holding an array with the benchmark results.\n    \"\"\"\n    name = name or \"nnbench-\" + platform.node() + \"-\" + uuid.uuid1().hex[:8]\n\n    if not self.benchmarks:\n        self.collect(path_or_module, tags)\n\n    family_sizes: dict[str, Any] = collections.defaultdict(int)\n    family_indices: dict[str, Any] = collections.defaultdict(int)\n\n    ctx: dict[str, Any] = {}\n    for provider in context:\n        val = provider()\n        duplicates = set(ctx.keys()) &amp; set(val.keys())\n        if duplicates:\n            dupe, *_ = duplicates\n            raise ValueError(f\"got multiple values for context key {dupe!r}\")\n        ctx.update(val)\n\n    # if we didn't find any benchmarks, warn and return an empty record.\n    if not self.benchmarks:\n        warnings.warn(f\"No benchmarks found in path/module {str(path_or_module)!r}.\")\n        return BenchmarkRecord(name=name, context=ctx, benchmarks=[])\n\n    for bm in self.benchmarks:\n        family_sizes[bm.fn.__name__] += 1\n\n    if isinstance(params, Parameters):\n        dparams = asdict(params)\n    else:\n        dparams = params or {}\n\n    if self.typecheck:\n        self._check(dparams)\n\n    results: list[dict[str, Any]] = []\n\n    def _maybe_dememo(v, expected_type):\n        \"\"\"Compute and memoize a value if a memo is given for a variable.\"\"\"\n        if is_memo(v) and not is_memo_type(expected_type):\n            return v()\n        return v\n\n    for benchmark in self.benchmarks:\n        bm_family = benchmark.fn.__name__\n        state = State(\n            name=benchmark.name,\n            family=bm_family,\n            family_size=family_sizes[bm_family],\n            family_index=family_indices[bm_family],\n        )\n        family_indices[bm_family] += 1\n        bmtypes = dict(zip(benchmark.interface.names, benchmark.interface.types))\n        bmparams = dict(zip(benchmark.interface.names, benchmark.interface.defaults))\n        bmparams |= {k: v for k, v in dparams.items() if k in bmparams}\n        bmparams = {k: _maybe_dememo(v, bmtypes[k]) for k, v in bmparams.items()}\n\n        # TODO: Wrap this into an execution context\n        res: dict[str, Any] = {\n            \"name\": benchmark.name,\n            \"function\": qualname(benchmark.fn),\n            \"description\": benchmark.fn.__doc__ or \"\",\n            \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"error_occurred\": False,\n            \"error_message\": \"\",\n            \"parameters\": self.params_repr(bmparams),\n        }\n        try:\n            benchmark.setUp(state, bmparams)\n            with timer(res):\n                res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(state, bmparams)\n            results.append(res)\n\n    return BenchmarkRecord(\n        name=name,\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.qualname","title":"qualname","text":"<pre><code>qualname(fn: Callable) -&gt; str\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>def qualname(fn: Callable) -&gt; str:\n    if fn.__name__ == fn.__qualname__:\n        return fn.__name__\n    return f\"{fn.__qualname__}.{fn.__name__}\"\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.timer","title":"timer","text":"<pre><code>timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]\n</code></pre> Source code in <code>src/nnbench/runner.py</code> <pre><code>@contextlib.contextmanager\ndef timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]:\n    start = time.perf_counter_ns()\n    try:\n        yield\n    finally:\n        end = time.perf_counter_ns()\n        bm[\"time_ns\"] = end - start\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.flatten","title":"flatten","text":"<pre><code>flatten(d: dict, sep: str = '.', prefix: str = '') -&gt; dict\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def flatten(d: dict, sep: str = \".\", prefix: str = \"\") -&gt; dict:\n    d_flat = {}\n    for k, v in d.items():\n        new_key = prefix + sep + k if prefix else k\n        if isinstance(v, dict):\n            d_flat.update(flatten(v, sep=sep, prefix=new_key))\n        else:\n            d_flat[k] = v\n    return d_flat\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.unflatten","title":"unflatten","text":"<pre><code>unflatten(d: dict, sep: str = '.') -&gt; dict\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def unflatten(d: dict, sep: str = \".\") -&gt; dict:\n    sorted_keys = sorted(d.keys())\n    unflattened = {}\n    for prefix, keys in itertools.groupby(sorted_keys, key=lambda key: key.split(sep, 1)[0]):\n        key_group = list(keys)\n        if len(key_group) == 1 and sep not in key_group[0]:\n            unflattened[prefix] = d[prefix]\n        else:\n            nested_dict = {key.split(sep, 1)[1]: d[key] for key in key_group}\n            unflattened[prefix] = unflatten(nested_dict, sep=sep)\n    return unflattened\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.ismodule","title":"ismodule","text":"<pre><code>ismodule(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def ismodule(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"Convert a file name to its corresponding Python module name.\"\"\"\n    fpath = Path(file).with_suffix(\"\")\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    fpath = Path(file).resolve()  # Python module __file__ paths are absolute.\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: Recomputing this map in a loop can be expensive if many modules are loaded.\n    modmap = {m.__file__: m for m in sys.modules.values() if getattr(m, \"__file__\", None)}\n    spath = str(fpath)\n    if spath in modmap:\n        # if the module under \"file\" has already been loaded, return it,\n        # otherwise we get nasty type errors in collection.\n        return modmap[spath]\n\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>A lightweight interface for refining, displaying, and streaming benchmark results to various sinks.</p>"},{"location":"reference/nnbench/reporter/base/","title":"base","text":""},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter","title":"BenchmarkReporter","text":"<p>The base interface for a benchmark reporter class.</p> <p>A benchmark reporter consumes benchmark results from a previous run, and subsequently reports them in the way specified by the respective implementation's <code>report_result()</code> method.</p> <p>For example, to write benchmark results to a database, you could save the credentials for authentication on the class, and then stream the results directly to the database in <code>report_result()</code>, with preprocessing if necessary.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>class BenchmarkReporter:\n    \"\"\"\n    The base interface for a benchmark reporter class.\n\n    A benchmark reporter consumes benchmark results from a previous run, and subsequently\n    reports them in the way specified by the respective implementation's ``report_result()``\n    method.\n\n    For example, to write benchmark results to a database, you could save the credentials\n    for authentication on the class, and then stream the results directly to\n    the database in ``report_result()``, with preprocessing if necessary.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._initialized = False\n\n    def initialize(self):\n        \"\"\"\n        Initialize the reporter's state.\n\n        This is the intended place to create resources like a result directory,\n        a database connection, or a HTTP client.\n        \"\"\"\n        self._initialized = True\n\n    def finalize(self):\n        \"\"\"\n        Finalize the reporter's state.\n\n        This is the intended place to destroy/release resources that were previously\n        acquired in ``initialize()``.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize the reporter's state.</p> <p>This is the intended place to create resources like a result directory, a database connection, or a HTTP client.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Initialize the reporter's state.\n\n    This is the intended place to create resources like a result directory,\n    a database connection, or a HTTP client.\n    \"\"\"\n    self._initialized = True\n</code></pre>"},{"location":"reference/nnbench/reporter/base/#nnbench.reporter.base.BenchmarkReporter.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize the reporter's state.</p> <p>This is the intended place to destroy/release resources that were previously acquired in <code>initialize()</code>.</p> Source code in <code>src/nnbench/reporter/base.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Finalize the reporter's state.\n\n    This is the intended place to destroy/release resources that were previously\n    acquired in ``initialize()``.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/nnbench/reporter/console/","title":"console","text":""},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter","title":"ConsoleReporter","text":"<p>               Bases: <code>BenchmarkReporter</code></p> <p>The base interface for a console reporter class.</p> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>class ConsoleReporter(BenchmarkReporter):\n    \"\"\"\n    The base interface for a console reporter class.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # TODO: Add context manager to register live console prints\n        self.console = Console(**kwargs)\n\n    def display(self, record: BenchmarkRecord) -&gt; None:\n        \"\"\"\n        Display a benchmark record in the console.\n\n        Benchmarks and context values will be filtered before display\n        if any filtering is applied.\n\n        Columns that do not contain any useful information are omitted by default.\n\n        Parameters\n        ----------\n        record: BenchmarkRecord\n            The benchmark record to display.\n        \"\"\"\n        t = Table()\n\n        rows: list[list[str]] = []\n        columns: list[str] = [\"Benchmark\", \"Value\", \"Wall time (ns)\", \"Parameters\"]\n\n        # print context values\n        for k, v in record.context.items():\n            print(f\"{k}: {v}\")\n\n        for bm in record.benchmarks:\n            row = [bm[\"name\"], get_value_by_name(bm), str(bm[\"time_ns\"]), str(bm[\"parameters\"])]\n            rows.append(row)\n\n        for column in columns:\n            t.add_column(column)\n        for row in rows:\n            t.add_row(*row)\n\n        self.console.print(t, overflow=\"ellipsis\")\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = Console(**kwargs)\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.display","title":"display","text":"<pre><code>display(record: BenchmarkRecord) -&gt; None\n</code></pre> <p>Display a benchmark record in the console.</p> <p>Benchmarks and context values will be filtered before display if any filtering is applied.</p> <p>Columns that do not contain any useful information are omitted by default.</p> PARAMETER DESCRIPTION <code>record</code> <p>The benchmark record to display.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def display(self, record: BenchmarkRecord) -&gt; None:\n    \"\"\"\n    Display a benchmark record in the console.\n\n    Benchmarks and context values will be filtered before display\n    if any filtering is applied.\n\n    Columns that do not contain any useful information are omitted by default.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The benchmark record to display.\n    \"\"\"\n    t = Table()\n\n    rows: list[list[str]] = []\n    columns: list[str] = [\"Benchmark\", \"Value\", \"Wall time (ns)\", \"Parameters\"]\n\n    # print context values\n    for k, v in record.context.items():\n        print(f\"{k}: {v}\")\n\n    for bm in record.benchmarks:\n        row = [bm[\"name\"], get_value_by_name(bm), str(bm[\"time_ns\"]), str(bm[\"parameters\"])]\n        rows.append(row)\n\n    for column in columns:\n        t.add_column(column)\n    for row in rows:\n        t.add_row(*row)\n\n    self.console.print(t, overflow=\"ellipsis\")\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.get_value_by_name","title":"get_value_by_name","text":"<pre><code>get_value_by_name(result: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def get_value_by_name(result: dict[str, Any]) -&gt; str:\n    if result.get(\"error_occurred\", False):\n        errmsg = result.get(\"error_message\", \"&lt;unknown&gt;\")\n        return \"[red]ERROR: [/red]\" + errmsg\n    return str(result.get(\"value\", _MISSING))\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/","title":"duckdb_sql","text":""},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DUCKDB_INSTALLED","title":"DUCKDB_INSTALLED  <code>module-attribute</code>","text":"<pre><code>DUCKDB_INSTALLED = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter","title":"DuckDBReporter","text":"<p>               Bases: <code>FileReporter</code></p> <p>A reporter for streaming benchmark results to duckdb.</p> <p>Serializes records into flat files in a temporary directory, and reads them in again afterwards.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>Name of the database to connect to. The default value <code>\"memory\"</code> uses an in-memory duckdb database.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':memory:'</code> </p> <code>read_only</code> <p>Connect to a database in read-only mode.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>directory</code> <p>Destination directory to write records to. Must point to an existing directory. If omitted, a temporary directory will be used.</p> <p> TYPE: <code>str | PathLike[str] | None</code> DEFAULT: <code>None</code> </p> <code>delete</code> <p>Delete the directory containing the written records after use. If the destination directory is a temporary directory, delete is implicitly true always.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ModuleNotFoundError</code> <p>If <code>duckdb</code> is not installed.</p> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>class DuckDBReporter(FileReporter):\n    \"\"\"\n    A reporter for streaming benchmark results to duckdb.\n\n    Serializes records into flat files in a temporary directory, and\n    reads them in again afterwards.\n\n    Parameters\n    ----------\n    dbname: str\n        Name of the database to connect to. The default value ``\"memory\"``\n        uses an in-memory duckdb database.\n    read_only: bool\n        Connect to a database in read-only mode.\n    directory: str | os.PathLike[str] | None\n        Destination directory to write records to. Must point to an existing directory.\n        If omitted, a temporary directory will be used.\n    delete: bool\n        Delete the directory containing the written records after use. If the destination\n        directory is a temporary directory, delete is implicitly true always.\n\n    Raises\n    ------\n    ModuleNotFoundError\n        If ``duckdb`` is not installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        dbname: str = \":memory:\",\n        read_only: bool = False,\n        directory: str | os.PathLike[str] | None = None,\n        delete: bool = False,\n    ):\n        if not DUCKDB_INSTALLED:\n            raise ModuleNotFoundError(\n                f\"class {self.__class__.__name__!r} needs `duckdb` to be installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade duckdb`.\"\n            )\n\n        super().__init__()\n        self.dbname = dbname\n        self.read_only = read_only\n\n        # A place to store intermediate JSON records.\n        if not directory:\n            self._directory = Path(tempfile.mkdtemp())\n            self.delete = True\n        else:\n            self._directory = Path(directory)\n            self.delete = delete\n\n        weakref.finalize(self, self.finalize)\n\n        self.conn: duckdb.DuckDBPyConnection | None = None\n\n    @property\n    def directory(self) -&gt; os.PathLike[str]:\n        return self._directory\n\n    def initialize(self):\n        self.conn = duckdb.connect(self.dbname, read_only=self.read_only)\n        self._initialized = True\n\n    def finalize(self):\n        if self.conn:\n            self.conn.close()\n\n        if self.delete:\n            shutil.rmtree(self._directory, ignore_errors=True)\n\n    def read_sql(\n        self,\n        file: str | os.PathLike[str],\n        driver: str | None = None,\n        include: tuple[str, ...] | None = None,\n        alias: dict[str, str] | None = None,\n        limit: int | None = None,\n    ) -&gt; BenchmarkRecord:\n        if not self._initialized:\n            self.initialize()\n\n        driver = driver or Path(file).suffix.removeprefix(\".\")\n        if driver not in [\"json\", \"csv\", \"parquet\"]:\n            raise NotImplementedError(\"duckdb only supports reading JSON, CSV or parquet files\")\n\n        alias = alias or {}\n        limit = limit or 0\n        if limit &lt; 0:\n            raise ValueError(\"'limit' must be non-negative\")\n\n        if include is None:\n            cols = \"*\"\n        else:\n            cols = \", \".join(i if i not in alias else f\"{i} AS {alias[i]}\" for i in include)\n\n        # TODO: Query support for WHERE\n        query = f\"SELECT {cols} FROM read_json_auto('{str(file)}')\"  # nosec B608\n\n        rel = self.conn.sql(query)\n        columns = rel.columns\n        results = rel.fetchall()\n\n        benchmarks = [dict(zip(columns, r)) for r in results]\n        context = {}\n        for bm in benchmarks:\n            context.update(bm.pop(\"context\", {}))\n\n        return BenchmarkRecord(context=context, benchmarks=benchmarks)\n\n    def raw_sql(self, query: str) -&gt; \"duckdb.DuckDBPyRelation\":\n        rel = self.conn.sql(query=query)\n        return rel\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.dbname","title":"dbname  <code>instance-attribute</code>","text":"<pre><code>dbname = dbname\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.read_only","title":"read_only  <code>instance-attribute</code>","text":"<pre><code>read_only = read_only\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.delete","title":"delete  <code>instance-attribute</code>","text":"<pre><code>delete = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.conn","title":"conn  <code>instance-attribute</code>","text":"<pre><code>conn: DuckDBPyConnection | None = None\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.directory","title":"directory  <code>property</code>","text":"<pre><code>directory: PathLike[str]\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def initialize(self):\n    self.conn = duckdb.connect(self.dbname, read_only=self.read_only)\n    self._initialized = True\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def finalize(self):\n    if self.conn:\n        self.conn.close()\n\n    if self.delete:\n        shutil.rmtree(self._directory, ignore_errors=True)\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.read_sql","title":"read_sql","text":"<pre><code>read_sql(\n    file: str | PathLike[str],\n    driver: str | None = None,\n    include: tuple[str, ...] | None = None,\n    alias: dict[str, str] | None = None,\n    limit: int | None = None,\n) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def read_sql(\n    self,\n    file: str | os.PathLike[str],\n    driver: str | None = None,\n    include: tuple[str, ...] | None = None,\n    alias: dict[str, str] | None = None,\n    limit: int | None = None,\n) -&gt; BenchmarkRecord:\n    if not self._initialized:\n        self.initialize()\n\n    driver = driver or Path(file).suffix.removeprefix(\".\")\n    if driver not in [\"json\", \"csv\", \"parquet\"]:\n        raise NotImplementedError(\"duckdb only supports reading JSON, CSV or parquet files\")\n\n    alias = alias or {}\n    limit = limit or 0\n    if limit &lt; 0:\n        raise ValueError(\"'limit' must be non-negative\")\n\n    if include is None:\n        cols = \"*\"\n    else:\n        cols = \", \".join(i if i not in alias else f\"{i} AS {alias[i]}\" for i in include)\n\n    # TODO: Query support for WHERE\n    query = f\"SELECT {cols} FROM read_json_auto('{str(file)}')\"  # nosec B608\n\n    rel = self.conn.sql(query)\n    columns = rel.columns\n    results = rel.fetchall()\n\n    benchmarks = [dict(zip(columns, r)) for r in results]\n    context = {}\n    for bm in benchmarks:\n        context.update(bm.pop(\"context\", {}))\n\n    return BenchmarkRecord(context=context, benchmarks=benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/duckdb_sql/#nnbench.reporter.duckdb_sql.DuckDBReporter.raw_sql","title":"raw_sql","text":"<pre><code>raw_sql(query: str) -&gt; DuckDBPyRelation\n</code></pre> Source code in <code>src/nnbench/reporter/duckdb_sql.py</code> <pre><code>def raw_sql(self, query: str) -&gt; \"duckdb.DuckDBPyRelation\":\n    rel = self.conn.sql(query=query)\n    return rel\n</code></pre>"},{"location":"reference/nnbench/reporter/file/","title":"file","text":""},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.SerDe","title":"SerDe  <code>module-attribute</code>","text":"<pre><code>SerDe = tuple[\n    Callable[[BenchmarkRecord, IO, dict[str, Any]], None],\n    Callable[[IO, dict[str, Any]], BenchmarkRecord],\n]\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter","title":"FileReporter","text":"<p>               Bases: <code>BenchmarkReporter</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class FileReporter(BenchmarkReporter):\n    def read(\n        self,\n        file: str | os.PathLike[str] | IO[str],\n        mode: str = \"r\",\n        driver: str | None = None,\n        options: dict[str, Any] | None = None,\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Reads a benchmark record from the given file path.\n\n        The file driver is chosen based on the extension in the ``file`` pathname.\n\n        Parameters\n        ----------\n        file: str | os.PathLike[str] | IO[str]\n            The file name to read from.\n        mode: str\n            File mode to use. Can be any of the modes used in builtin ``open()``.\n        driver: str | None\n            File driver implementation to use. If None, the file driver inferred from the\n            given file path's extension will be used.\n        options: dict[str, Any] | None\n            Options to pass to the respective file driver implementation.\n\n        Returns\n        -------\n        BenchmarkRecord\n            The benchmark record contained in the file.\n\n        Raises\n        ------\n        ValueError\n            If no registered file driver matches the file extension and no other driver\n            was explicitly specified.\n        \"\"\"\n        driver = driver or get_extension(file)\n\n        if not driver:\n            raise ValueError(\n                f\"could not infer a file driver to write file {str(file)!r}, \"\n                f\"and no file driver was specified (available drivers: \"\n                f\"{', '.join(repr(d) for d in _file_drivers)})\"\n            )\n        _, de = get_driver_implementation(driver)\n\n        if isinstance(file, str | os.PathLike):\n            protocol = get_protocol(file)\n            if protocol == \"file\":\n                fd = open(file, mode)\n            else:\n                try:\n                    import fsspec\n                except ImportError:\n                    raise RuntimeError(\"non-local URIs require the fsspec package\")\n                fs = fsspec.filesystem(protocol)\n                # NB(njunge): I sure hope this is standardized by fsspec\n                fd = fs.open(file, mode)\n        elif hasattr(file, \"read\"):\n            fd = file\n        else:\n            raise TypeError(\"filename must be a str, bytes, file or PathLike object\")\n\n        with fd as fp:\n            return de(fp, options or {})\n\n    def write(\n        self,\n        record: BenchmarkRecord,\n        file: str | os.PathLike[str] | IO[str],\n        mode: str = \"w\",\n        driver: str | None = None,\n        options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Writes a benchmark record to the given file path.\n\n        The file driver is chosen based on the extension found on the ``file`` path.\n\n        Parameters\n        ----------\n        record: BenchmarkRecord\n            The record to write to the database.\n        file: str | os.PathLike[str]\n            The file name to write to.\n        mode: str\n            File mode to use. Can be any of the modes used in builtin ``open()``.\n        driver: str | None\n            File driver implementation to use. If None, the file driver inferred from the\n            given file path's extension will be used.\n        options: dict[str, Any] | None\n            Options to pass to the respective file driver implementation.\n\n        Raises\n        ------\n        ValueError\n            If no registered file driver matches the file extension and no other driver\n            was explicitly specified.\n        \"\"\"\n        driver = driver or get_extension(file)\n\n        if not driver:\n            raise ValueError(\n                f\"could not infer a file driver to write file {str(file)!r}, \"\n                f\"and no file driver was specified (available drivers: \"\n                f\"{', '.join(repr(d) for d in _file_drivers)})\"\n            )\n        ser, _ = get_driver_implementation(driver)\n\n        if isinstance(file, str | os.PathLike):\n            protocol = get_protocol(file)\n            if protocol == \"file\":\n                fd = open(file, mode)\n            else:\n                try:\n                    import fsspec\n                except ImportError:\n                    raise RuntimeError(\"non-local URIs require the fsspec package\")\n                fs = fsspec.filesystem(protocol)\n                fd = fs.open(file, mode)\n        elif hasattr(file, \"write\"):\n            fd = file\n        else:\n            raise TypeError(\"filename must be a str, bytes, file or PathLike object\")\n\n        with fd as fp:\n            ser(record, fp, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter.read","title":"read","text":"<pre><code>read(\n    file: str | PathLike[str] | IO[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; BenchmarkRecord\n</code></pre> <p>Reads a benchmark record from the given file path.</p> <p>The file driver is chosen based on the extension in the <code>file</code> pathname.</p> PARAMETER DESCRIPTION <code>file</code> <p>The file name to read from.</p> <p> TYPE: <code>str | PathLike[str] | IO[str]</code> </p> <code>mode</code> <p>File mode to use. Can be any of the modes used in builtin <code>open()</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'r'</code> </p> <code>driver</code> <p>File driver implementation to use. If None, the file driver inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Options to pass to the respective file driver implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The benchmark record contained in the file.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If no registered file driver matches the file extension and no other driver was explicitly specified.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(\n    self,\n    file: str | os.PathLike[str] | IO[str],\n    mode: str = \"r\",\n    driver: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Reads a benchmark record from the given file path.\n\n    The file driver is chosen based on the extension in the ``file`` pathname.\n\n    Parameters\n    ----------\n    file: str | os.PathLike[str] | IO[str]\n        The file name to read from.\n    mode: str\n        File mode to use. Can be any of the modes used in builtin ``open()``.\n    driver: str | None\n        File driver implementation to use. If None, the file driver inferred from the\n        given file path's extension will be used.\n    options: dict[str, Any] | None\n        Options to pass to the respective file driver implementation.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The benchmark record contained in the file.\n\n    Raises\n    ------\n    ValueError\n        If no registered file driver matches the file extension and no other driver\n        was explicitly specified.\n    \"\"\"\n    driver = driver or get_extension(file)\n\n    if not driver:\n        raise ValueError(\n            f\"could not infer a file driver to write file {str(file)!r}, \"\n            f\"and no file driver was specified (available drivers: \"\n            f\"{', '.join(repr(d) for d in _file_drivers)})\"\n        )\n    _, de = get_driver_implementation(driver)\n\n    if isinstance(file, str | os.PathLike):\n        protocol = get_protocol(file)\n        if protocol == \"file\":\n            fd = open(file, mode)\n        else:\n            try:\n                import fsspec\n            except ImportError:\n                raise RuntimeError(\"non-local URIs require the fsspec package\")\n            fs = fsspec.filesystem(protocol)\n            # NB(njunge): I sure hope this is standardized by fsspec\n            fd = fs.open(file, mode)\n    elif hasattr(file, \"read\"):\n        fd = file\n    else:\n        raise TypeError(\"filename must be a str, bytes, file or PathLike object\")\n\n    with fd as fp:\n        return de(fp, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter.write","title":"write","text":"<pre><code>write(\n    record: BenchmarkRecord,\n    file: str | PathLike[str] | IO[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Writes a benchmark record to the given file path.</p> <p>The file driver is chosen based on the extension found on the <code>file</code> path.</p> PARAMETER DESCRIPTION <code>record</code> <p>The record to write to the database.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> <code>file</code> <p>The file name to write to.</p> <p> TYPE: <code>str | PathLike[str] | IO[str]</code> </p> <code>mode</code> <p>File mode to use. Can be any of the modes used in builtin <code>open()</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'w'</code> </p> <code>driver</code> <p>File driver implementation to use. If None, the file driver inferred from the given file path's extension will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Options to pass to the respective file driver implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If no registered file driver matches the file extension and no other driver was explicitly specified.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self,\n    record: BenchmarkRecord,\n    file: str | os.PathLike[str] | IO[str],\n    mode: str = \"w\",\n    driver: str | None = None,\n    options: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Writes a benchmark record to the given file path.\n\n    The file driver is chosen based on the extension found on the ``file`` path.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The record to write to the database.\n    file: str | os.PathLike[str]\n        The file name to write to.\n    mode: str\n        File mode to use. Can be any of the modes used in builtin ``open()``.\n    driver: str | None\n        File driver implementation to use. If None, the file driver inferred from the\n        given file path's extension will be used.\n    options: dict[str, Any] | None\n        Options to pass to the respective file driver implementation.\n\n    Raises\n    ------\n    ValueError\n        If no registered file driver matches the file extension and no other driver\n        was explicitly specified.\n    \"\"\"\n    driver = driver or get_extension(file)\n\n    if not driver:\n        raise ValueError(\n            f\"could not infer a file driver to write file {str(file)!r}, \"\n            f\"and no file driver was specified (available drivers: \"\n            f\"{', '.join(repr(d) for d in _file_drivers)})\"\n        )\n    ser, _ = get_driver_implementation(driver)\n\n    if isinstance(file, str | os.PathLike):\n        protocol = get_protocol(file)\n        if protocol == \"file\":\n            fd = open(file, mode)\n        else:\n            try:\n                import fsspec\n            except ImportError:\n                raise RuntimeError(\"non-local URIs require the fsspec package\")\n            fs = fsspec.filesystem(protocol)\n            fd = fs.open(file, mode)\n    elif hasattr(file, \"write\"):\n        fd = file\n    else:\n        raise TypeError(\"filename must be a str, bytes, file or PathLike object\")\n\n    with fd as fp:\n        ser(record, fp, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.yaml_save","title":"yaml_save","text":"<pre><code>yaml_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def yaml_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None:\n    try:\n        import yaml\n    except ImportError:\n        raise ModuleNotFoundError(\"`pyyaml` is not installed\")\n\n    yaml.safe_dump(record.to_json(), fp, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.yaml_load","title":"yaml_load","text":"<pre><code>yaml_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def yaml_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord:\n    try:\n        import yaml\n    except ImportError:\n        raise ModuleNotFoundError(\"`pyyaml` is not installed\")\n\n    bms = yaml.safe_load(fp)\n    return BenchmarkRecord.expand(bms)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.json_save","title":"json_save","text":"<pre><code>json_save(record: BenchmarkRecord, fp: IO[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def json_save(record: BenchmarkRecord, fp: IO[str], options: dict[str, Any]) -&gt; None:\n    import json\n\n    json.dump(record.to_json(), fp, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.json_load","title":"json_load","text":"<pre><code>json_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def json_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord:\n    import json\n\n    benchmarks = json.load(fp, **options)\n    return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ndjson_save","title":"ndjson_save","text":"<pre><code>ndjson_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def ndjson_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None:\n    # mode is unused, since NDJSON requires every individual benchmark to be one line.\n    import json\n\n    bms = record.to_list()\n    fp.write(\"\\n\".join([json.dumps(b, **options) for b in bms]))\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ndjson_load","title":"ndjson_load","text":"<pre><code>ndjson_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def ndjson_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord:\n    import json\n\n    benchmarks: list[dict[str, Any]]\n    benchmarks = [json.loads(line, **options) for line in fp]\n    return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.csv_save","title":"csv_save","text":"<pre><code>csv_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def csv_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None:\n    # mode is unused, since NDJSON requires every individual benchmark to be one line.\n    import csv\n\n    bm = record.to_list()\n    writer = csv.DictWriter(fp, fieldnames=bm[0].keys(), **options)\n    writer.writeheader()\n\n    for b in bm:\n        writer.writerow(b)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.csv_load","title":"csv_load","text":"<pre><code>csv_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def csv_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord:\n    import csv\n    import json\n\n    reader = csv.DictReader(fp, **options)\n\n    benchmarks: list[dict[str, Any]] = []\n    # apparently csv.DictReader has no appropriate type hint for __next__,\n    # so we supply one ourselves.\n    bm: dict[str, Any]\n    for bm in reader:\n        benchmarks.append(bm)\n        # it can happen that the context is inlined as a stringified JSON object\n        # (e.g. in CSV), so we optionally JSON-load the context.\n        if \"context\" in bm:\n            strctx: str = bm[\"context\"]\n            # TODO: This does not play nicely with doublequote, maybe re.sub?\n            strctx = strctx.replace(\"'\", '\"')\n            bm[\"context\"] = json.loads(strctx)\n    return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.parquet_save","title":"parquet_save","text":"<pre><code>parquet_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def parquet_save(record: BenchmarkRecord, fp: IO, options: dict[str, Any]) -&gt; None:\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n\n    table = pa.Table.from_pylist(record.to_list())\n    pq.write_table(table, fp, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.parquet_load","title":"parquet_load","text":"<pre><code>parquet_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def parquet_load(fp: IO, options: dict[str, Any]) -&gt; BenchmarkRecord:\n    import pyarrow.parquet as pq\n\n    table = pq.read_table(fp, **options)\n    benchmarks: list[dict[str, Any]] = table.to_pylist()\n    return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_driver_implementation","title":"get_driver_implementation","text":"<pre><code>get_driver_implementation(name: str) -&gt; SerDe\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_driver_implementation(name: str) -&gt; SerDe:\n    try:\n        return _file_drivers[name]\n    except KeyError:\n        raise KeyError(f\"unsupported file format {name!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.register_driver_implementation","title":"register_driver_implementation","text":"<pre><code>register_driver_implementation(name: str, impl: SerDe, clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def register_driver_implementation(name: str, impl: SerDe, clobber: bool = False) -&gt; None:\n    if name in _file_drivers and not clobber:\n        raise RuntimeError(\n            f\"driver {name!r} is already registered \"\n            f\"(to force registration, rerun with clobber=True)\"\n        )\n\n    with _file_driver_lock:\n        _file_drivers[name] = impl\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.deregister_driver_implementation","title":"deregister_driver_implementation","text":"<pre><code>deregister_driver_implementation(name: str) -&gt; SerDe | None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def deregister_driver_implementation(name: str) -&gt; SerDe | None:\n    with _file_driver_lock:\n        return _file_drivers.pop(name, None)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_extension","title":"get_extension","text":"<pre><code>get_extension(f: str | PathLike[str] | IO) -&gt; str\n</code></pre> <p>Given a file path or file-like object, returns file extension (can be the empty string, if the file has no extension).</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_extension(f: str | os.PathLike[str] | IO) -&gt; str:\n    \"\"\"\n    Given a file path or file-like object, returns file extension\n    (can be the empty string, if the file has no extension).\n    \"\"\"\n    if isinstance(f, str | os.PathLike):\n        return Path(f).suffix\n    else:\n        return Path(f.name).suffix\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_protocol","title":"get_protocol","text":"<pre><code>get_protocol(url: str | PathLike[str]) -&gt; str\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_protocol(url: str | os.PathLike[str]) -&gt; str:\n    url = str(url)\n    parts = re.split(r\"(::|://)\", url, maxsplit=1)\n    if len(parts) &gt; 1:\n        return parts[0]\n    return \"file\"\n</code></pre>"},{"location":"reference/nnbench/reporter/util/","title":"util","text":""},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.nullcols","title":"nullcols","text":"<pre><code>nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]\n</code></pre> <p>Extracts columns that only contain false-ish data from a list of benchmarks.</p> <p>Since this data is most often not interesting, the result of this can be used to filter out these columns from the benchmark dictionaries.</p> PARAMETER DESCRIPTION <code>_benchmarks</code> <p>The benchmarks to filter.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>set[str]</code> <p>Set of the columns (key names) that only contain false-ish values across all benchmarks.</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]:\n    \"\"\"\n    Extracts columns that only contain false-ish data from a list of benchmarks.\n\n    Since this data is most often not interesting, the result of this\n    can be used to filter out these columns from the benchmark dictionaries.\n\n    Parameters\n    ----------\n    _benchmarks: list[dict[str, Any]]\n        The benchmarks to filter.\n\n    Returns\n    -------\n    set[str]\n        Set of the columns (key names) that only contain false-ish values\n        across all benchmarks.\n    \"\"\"\n    nulls: dict[str, bool] = collections.defaultdict(bool)\n    for bm in _benchmarks:\n        for k, v in bm.items():\n            nulls[k] = nulls[k] or bool(v)\n    return set(k for k, v in nulls.items() if not v)\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":""},{"location":"reference/nnbench/types/benchmark/","title":"benchmark","text":"<p>Type interfaces for benchmarks and benchmark collections.</p>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.State","title":"State  <code>dataclass</code>","text":"Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>@dataclass(frozen=True)\nclass State:\n    name: str\n    family: str\n    family_size: int\n    family_index: int\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.State.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.State.family","title":"family  <code>instance-attribute</code>","text":"<pre><code>family: str\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.State.family_size","title":"family_size  <code>instance-attribute</code>","text":"<pre><code>family_size: int\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.State.family_index","title":"family_index  <code>instance-attribute</code>","text":"<pre><code>family_index: int\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord","title":"BenchmarkRecord  <code>dataclass</code>","text":"Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>@dataclass(frozen=True)\nclass BenchmarkRecord:\n    name: str\n    context: dict[str, Any]\n    benchmarks: list[dict[str, Any]]\n\n    def to_json(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Export a benchmark record to JSON.\n\n        Returns\n        -------\n        dict[str, Any]\n            A JSON representation of the benchmark record.\n        \"\"\"\n        return asdict(self)\n\n    def to_list(self) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Export a benchmark record to a list of individual results,\n        each with the benchmark context inlined.\n        \"\"\"\n        results = []\n        for b in self.benchmarks:\n            bc = copy.deepcopy(b)\n            bc[\"context\"] = self.context\n            bc[\"run-name\"] = self.name\n            results.append(bc)\n        return results\n\n    @classmethod\n    def expand(cls, bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self:\n        \"\"\"\n        Expand a list of deserialized JSON-like objects into a benchmark record.\n        This is equivalent to extracting the context given by the method it was\n        serialized with, and then returning the rest of the data as is.\n\n        Parameters\n        ----------\n        bms: dict[str, Any] | list[dict[str, Any]]\n            The deserialized benchmark record or list of records to expand into a record.\n\n        Returns\n        -------\n        BenchmarkRecord\n            The resulting record with the context extracted.\n\n        \"\"\"\n        context: dict[str, Any]\n        if isinstance(bms, dict):\n            if \"benchmarks\" not in bms.keys():\n                raise ValueError(f\"no benchmark data found in struct {bms}\")\n\n            benchmarks = bms[\"benchmarks\"]\n            context = bms.get(\"context\", {})\n            name = bms.get(\"name\", \"\")\n        else:\n            name = \"\"\n            context = {}\n            benchmarks = bms\n            for b in benchmarks:\n                # TODO(nicholasjng): This does not do the right thing if the list contains\n                #  data from multiple benchmark runs, e.g. from a DB query.\n                if \"run-name\" in b:\n                    name = b.pop(\"run-name\")\n                if \"context\" in b:\n                    # TODO: Log context key/value disagreements\n                    context |= b.pop(\"context\", {})\n        return cls(name=name, benchmarks=benchmarks, context=context)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; dict[str, Any]\n</code></pre> <p>Export a benchmark record to JSON.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A JSON representation of the benchmark record.</p> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>def to_json(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Export a benchmark record to JSON.\n\n    Returns\n    -------\n    dict[str, Any]\n        A JSON representation of the benchmark record.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; list[dict[str, Any]]\n</code></pre> <p>Export a benchmark record to a list of individual results, each with the benchmark context inlined.</p> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>def to_list(self) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Export a benchmark record to a list of individual results,\n    each with the benchmark context inlined.\n    \"\"\"\n    results = []\n    for b in self.benchmarks:\n        bc = copy.deepcopy(b)\n        bc[\"context\"] = self.context\n        bc[\"run-name\"] = self.name\n        results.append(bc)\n    return results\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.BenchmarkRecord.expand","title":"expand  <code>classmethod</code>","text":"<pre><code>expand(bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self\n</code></pre> <p>Expand a list of deserialized JSON-like objects into a benchmark record. This is equivalent to extracting the context given by the method it was serialized with, and then returning the rest of the data as is.</p> PARAMETER DESCRIPTION <code>bms</code> <p>The deserialized benchmark record or list of records to expand into a record.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The resulting record with the context extracted.</p> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>@classmethod\ndef expand(cls, bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self:\n    \"\"\"\n    Expand a list of deserialized JSON-like objects into a benchmark record.\n    This is equivalent to extracting the context given by the method it was\n    serialized with, and then returning the rest of the data as is.\n\n    Parameters\n    ----------\n    bms: dict[str, Any] | list[dict[str, Any]]\n        The deserialized benchmark record or list of records to expand into a record.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The resulting record with the context extracted.\n\n    \"\"\"\n    context: dict[str, Any]\n    if isinstance(bms, dict):\n        if \"benchmarks\" not in bms.keys():\n            raise ValueError(f\"no benchmark data found in struct {bms}\")\n\n        benchmarks = bms[\"benchmarks\"]\n        context = bms.get(\"context\", {})\n        name = bms.get(\"name\", \"\")\n    else:\n        name = \"\"\n        context = {}\n        benchmarks = bms\n        for b in benchmarks:\n            # TODO(nicholasjng): This does not do the right thing if the list contains\n            #  data from multiple benchmark runs, e.g. from a DB query.\n            if \"run-name\" in b:\n                name = b.pop(\"run-name\")\n            if \"context\" in b:\n                # TODO: Log context key/value disagreements\n                context |= b.pop(\"context\", {})\n    return cls(name=name, benchmarks=benchmarks, context=context)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The function defining the benchmark.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>name</code> <p>A name to display for the given benchmark. If not given, will be constructed from the function name and given parameters.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>params</code> <p>A partial parametrization to apply to the benchmark function. Internal only, you should not need to set this yourself.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>dict()</code> </p> <code>setUp</code> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[[State, Mapping[str, Any]], None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p> <p> TYPE: <code>Callable[[State, Mapping[str, Any]], None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n\n    Parameters\n    ----------\n    fn: Callable[..., Any]\n        The function defining the benchmark.\n    name: str | None\n        A name to display for the given benchmark. If not given, will be constructed from the\n        function name and given parameters.\n    params: dict[str, Any]\n        A partial parametrization to apply to the benchmark function. Internal only,\n        you should not need to set this yourself.\n    setUp: Callable[..., None]\n        A setup hook run before the benchmark. Must take all members of `params` as inputs.\n    tearDown: Callable[..., None]\n        A teardown hook run after the benchmark. Must take all members of `params` as inputs.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    \"\"\"\n\n    fn: Callable[..., Any]\n    name: str = \"\"\n    params: dict[str, Any] = field(default_factory=dict)\n    setUp: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n    tearDown: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n    tags: tuple[str, ...] = field(repr=False, default=())\n    interface: Interface = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if not self.name:\n            super().__setattr__(\"name\", self.fn.__name__)\n        super().__setattr__(\"interface\", Interface.from_callable(self.fn, self.params))\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = ''\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.Parameters","title":"Parameters  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters. This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary is, of course, static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Parameters:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters. This class is not functional\n    on its own, and needs to be subclassed according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary is, of course,\n    static analysis and type safety for your benchmarking code.\n    \"\"\"\n</code></pre>"},{"location":"reference/nnbench/types/benchmark/#nnbench.types.benchmark.NoOp","title":"NoOp","text":"<pre><code>NoOp(state: State, params: Mapping[str, Any] = MappingProxyType({})) -&gt; None\n</code></pre> Source code in <code>src/nnbench/types/benchmark.py</code> <pre><code>def NoOp(state: State, params: Mapping[str, Any] = MappingProxyType({})) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/nnbench/types/interface/","title":"interface","text":"<p>Type interface for the function interface</p>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface. An instance of this class is created using the <code>from_callable</code> class method.</p> Parameters: <p>names : tuple[str, ...]     Names of the function parameters. types : tuple[type, ...]     Types of the function parameters. defaults : tuple     A tuple of the function parameters' default values. variables : tuple[Variable, ...]     A tuple of tuples, where each inner tuple contains the parameter name and type. returntype: type     The function's return type annotation, or NoneType if left untyped.</p> Source code in <code>src/nnbench/types/interface.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface. An instance of this class\n    is created using the `from_callable` class method.\n\n    Parameters:\n    ----------\n    names : tuple[str, ...]\n        Names of the function parameters.\n    types : tuple[type, ...]\n        Types of the function parameters.\n    defaults : tuple\n        A tuple of the function parameters' default values.\n    variables : tuple[Variable, ...]\n        A tuple of tuples, where each inner tuple contains the parameter name and type.\n    returntype: type\n        The function's return type annotation, or NoneType if left untyped.\n    \"\"\"\n\n    names: tuple[str, ...]\n    types: tuple[type, ...]\n    defaults: tuple\n    variables: tuple[Variable, ...]\n    returntype: type\n\n    @classmethod\n    def from_callable(cls, fn: Callable, defaults: dict[str, Any]) -&gt; Self:\n        \"\"\"\n        Creates an interface instance from the given callable.\n        \"\"\"\n        # Set `follow_wrapped=False` to get the partially filled interfaces.\n        # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n        sig = inspect.signature(fn, follow_wrapped=False)\n        ret = sig.return_annotation\n        _defaults = {k: defaults.get(k, v.default) for k, v in sig.parameters.items()}\n        # defaults are the signature parameters, then the partial parametrization.\n        return cls(\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(_defaults.values()),\n            tuple((k, v.annotation, _defaults[k]) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre>"},{"location":"reference/nnbench/types/interface/#nnbench.types.interface.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable, defaults: dict[str, Any]) -&gt; Self\n</code></pre> <p>Creates an interface instance from the given callable.</p> Source code in <code>src/nnbench/types/interface.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable, defaults: dict[str, Any]) -&gt; Self:\n    \"\"\"\n    Creates an interface instance from the given callable.\n    \"\"\"\n    # Set `follow_wrapped=False` to get the partially filled interfaces.\n    # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n    sig = inspect.signature(fn, follow_wrapped=False)\n    ret = sig.return_annotation\n    _defaults = {k: defaults.get(k, v.default) for k, v in sig.parameters.items()}\n    # defaults are the signature parameters, then the partial parametrization.\n    return cls(\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(_defaults.values()),\n        tuple((k, v.annotation, _defaults[k]) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/memo/","title":"memo","text":""},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.Memo","title":"Memo","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Abstract base class for memoized values in benchmark runs.</p> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>class Memo(Generic[T]):\n    \"\"\"Abstract base class for memoized values in benchmark runs.\"\"\"\n\n    # TODO: Make this better than the decorator application\n    #  -&gt; _Cached metaclass like in fsspec's AbstractFileSystem (maybe vendor with license)\n\n    @cached_memo\n    def __call__(self) -&gt; T:\n        \"\"\"Placeholder to override when subclassing. The call should return the to be cached object.\"\"\"\n        raise NotImplementedError\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete the cached object and clear it from the cache.\"\"\"\n        with _cache_lock:\n            sid = id(self)\n            if sid in _memo_cache:\n                logger.debug(f\"Deleting cached value for memo with ID {sid}\")\n                del _memo_cache[sid]\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.is_memo","title":"is_memo","text":"<pre><code>is_memo(v: Any) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def is_memo(v: Any) -&gt; bool:\n    return callable(v) and len(inspect.signature(v).parameters) == 0\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.is_memo_type","title":"is_memo_type","text":"<pre><code>is_memo_type(t: type) -&gt; bool\n</code></pre> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def is_memo_type(t: type) -&gt; bool:\n    return get_origin(t) is collections.abc.Callable and get_args(t)[0] == []\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.memo_cache_size","title":"memo_cache_size","text":"<pre><code>memo_cache_size() -&gt; int\n</code></pre> <p>Get the current size of the memo cache.</p> RETURNS DESCRIPTION <code>int</code> <p>The number of items currently stored in the memo cache.</p> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def memo_cache_size() -&gt; int:\n    \"\"\"\n    Get the current size of the memo cache.\n\n    Returns\n    -------\n    int\n        The number of items currently stored in the memo cache.\n    \"\"\"\n    return len(_memo_cache)\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.clear_memo_cache","title":"clear_memo_cache","text":"<pre><code>clear_memo_cache() -&gt; None\n</code></pre> <p>Clear all items from memo cache in a thread_safe manner.</p> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def clear_memo_cache() -&gt; None:\n    \"\"\"\n    Clear all items from memo cache in a thread_safe manner.\n    \"\"\"\n    with _cache_lock:\n        _memo_cache.clear()\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.evict_memo","title":"evict_memo","text":"<pre><code>evict_memo(_id: int) -&gt; Any\n</code></pre> <p>Pop cached item with key <code>_id</code> from the memo cache.</p> PARAMETER DESCRIPTION <code>_id</code> <p>The unique identifier (usually the id assigned by the Python interpreter) of the item to be evicted.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The value that was associated with the removed cache entry. If no item is found with the given <code>_id</code>, a KeyError is raised.</p> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def evict_memo(_id: int) -&gt; Any:\n    \"\"\"\n    Pop cached item with key `_id` from the memo cache.\n\n    Parameters\n    ----------\n    _id : int\n        The unique identifier (usually the id assigned by the Python interpreter) of the item to be evicted.\n\n    Returns\n    -------\n    Any\n        The value that was associated with the removed cache entry. If no item is found with the given `_id`, a KeyError is raised.\n    \"\"\"\n    with _cache_lock:\n        return _memo_cache.pop(_id)\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.get_memo_by_value","title":"get_memo_by_value","text":"<pre><code>get_memo_by_value(val: Any) -&gt; int | None\n</code></pre> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def get_memo_by_value(val: Any) -&gt; int | None:\n    for k, v in _memo_cache.items():\n        if v is val:\n            return k\n    return None\n</code></pre>"},{"location":"reference/nnbench/types/memo/#nnbench.types.memo.cached_memo","title":"cached_memo","text":"<pre><code>cached_memo(fn: Callable) -&gt; Callable\n</code></pre> <p>Decorator that caches the result of a method call based on the instance ID.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The method to memoize.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A wrapped version of the method that caches its result.</p> Source code in <code>src/nnbench/types/memo.py</code> <pre><code>def cached_memo(fn: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator that caches the result of a method call based on the instance ID.\n\n    Parameters\n    ----------\n    fn: Callable\n        The method to memoize.\n\n    Returns\n    -------\n    Callable\n        A wrapped version of the method that caches its result.\n    \"\"\"\n\n    @functools.wraps(fn)\n    def wrapper(self, *args, **kwargs):\n        _tid = id(self)\n        with _cache_lock:\n            if _tid in _memo_cache:\n                logger.debug(f\"Returning memoized value from cache with ID {_tid}\")\n                return _memo_cache[_tid]\n        logger.debug(f\"Computing value on memo with ID {_tid} (cache miss)\")\n        value = fn(self, *args, **kwargs)\n        with _cache_lock:\n            _memo_cache[_tid] = value\n        return value\n\n    return wrapper\n</code></pre>"},{"location":"tutorials/","title":"Examples","text":"<p>This page showcases some examples of applications for nnbench. Click any of the links below for inspiration on how to use nnbench in your projects.</p> <ul> <li>Integrating nnbench into an existing ML pipeline</li> <li>Integrating nnbench with workflow orchestrators</li> <li>Using a streamlit web app to dispatch benchmarks</li> <li>Analyzing benchmark results at scale with duckDB</li> <li>Streaming benchmark results to a cloud database (Google BigQuery)</li> <li>How to benchmark pre-trained HuggingFace models with memos</li> </ul>"},{"location":"tutorials/bq/","title":"Streaming benchmarks to a cloud database","text":"<p>Once you obtain the results of your benchmarks, you will most likely want to store them somewhere. Whether that is in storage as flat files, on a server, or in a database, <code>nnbench</code> allows you to write records anywhere, provided the destination supports JSON.</p> <p>This is a small guide containing a snippet on how to stream benchmark results to a Google Cloud BigQuery table.</p>"},{"location":"tutorials/bq/#the-benchmarks","title":"The benchmarks","text":"<p>Configure your benchmarks as normal, for example by separating them into a Python file. The following is a very simple example benchmark setup.</p> <pre><code>import nnbench\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\n@nnbench.benchmark\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre>"},{"location":"tutorials/bq/#setting-up-a-bigquery-client","title":"Setting up a BigQuery client","text":"<p>In order to authenticate with BigQuery, follow the official Google Cloud documentation. In this case, we rely on Application Default Credentials (ADC), which can be configured with the <code>gcloud</code> CLI.</p> <p>To interact with BigQuery from Python, the <code>google-cloud-bigquery</code> package has to be installed. You can do this e.g. using pip via <code>pip install --upgrade google-cloud-bigquery</code>.</p>"},{"location":"tutorials/bq/#creating-a-table","title":"Creating a table","text":"<p>Within your configured project, proceed by creating a destination table to write the benchmarks to. Consider the BigQuery Python documentation on tables for how to create a table programmatically.</p> <p>Note</p> <p>If the configured dataset does not exist, you will have to create it as well, either programmatically via the <code>bigquery.Client.create_dataset</code> API or in the Google Cloud console.</p>"},{"location":"tutorials/bq/#using-bigquerys-schema-auto-detection","title":"Using BigQuery's schema auto-detection","text":"<p>In order to skip tedious schema inference by hand, we can use BigQuery's schema auto-detection from JSON records. All we have to do is configure a BigQuery load job to auto-detect the schema from the Python dictionaries in memory:</p> <pre><code>    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n</code></pre> <p>After that, write and stream the compacted benchmark record directly to your destination table. In this example, we decide to flatten the benchmark context to be able to extract scalar context values directly from the result table using raw SQL queries. Note that you have to use a custom separator (an underscore <code>\"_\"</code> in this case) for the context data, since BigQuery does not allow dots in column names.</p> <pre><code>    load_job.result()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Tip</p> <p>If you would like to save the context dictionary as a struct instead, use <code>mode = \"inline\"</code> in the call to <code>BenchmarkRecord.compact()</code>.</p> <p>And that's all! To check that the records appear as expected, you can now query the data e.g. like so:</p> <pre><code># check that the insert worked.\nquery = f'SELECT name, value, time_ns, git_commit AS commit FROM {table_id}'\nr = client.query(query)\nfor row in r.result():\n    print(r)\n</code></pre>"},{"location":"tutorials/bq/#recap-and-the-full-source-code","title":"Recap and the full source code","text":"<p>In this tutorial, we</p> <p>1) defined and ran a benchmark workload using <code>nnbench</code>. 2) configured a Google Cloud BigQuery client and a load job to insert benchmark records into a table, and 3) inserted the records into the destination table.</p> <p>The full source code for this tutorial is included below, and also in the nnbench repository.</p> <pre><code>from google.cloud import bigquery\n\nimport nnbench\nfrom nnbench.context import GitEnvironmentInfo\n\n\ndef main():\n    client = bigquery.Client()\n\n    # TODO: Fill these out with your appropriate resource names.\n    table_id = \"&lt;PROJECT&gt;.&lt;DATASET&gt;.&lt;TABLE&gt;\"\n\n    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n\n    runner = nnbench.BenchmarkRunner()\n    res = runner.run(\"benchmarks.py\", params={\"a\": 1, \"b\": 1}, context=(GitEnvironmentInfo(),))\n\n    load_job = client.load_table_from_json(res.to_json(), table_id, job_config=job_config)\n    load_job.result()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/duckdb/","title":"Querying benchmark results at scale with duckDB","text":"<p>For a powerful way to query, filter, and visualize benchmark records, duckdb is a great choice. This page contains a quick tutorial for analyzing benchmark results with duckDB.</p>"},{"location":"tutorials/duckdb/#prerequisites-and-installation","title":"Prerequisites and installation","text":"<p>To use duckdb, install the duckdb Python package by running <code>pip install --upgrade duckdb</code>. In this tutorial, we are going to be using the in-memory database only, but you can easily persist SQL views of records on disk as well.</p>"},{"location":"tutorials/duckdb/#writing-and-ingesting-benchmark-records","title":"Writing and ingesting benchmark records","text":"<p>We consider the following easy benchmark example:</p> <pre><code>import nnbench\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\n@nnbench.benchmark\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Running both of these benchmarks produces a benchmark record, which we can save to disk using the <code>FileIO</code> class.</p> <pre><code>import nnbench\nfrom nnbench.context import GitEnvironmentInfo\nfrom nnbench.reporter.file import FileReporter\n\nrunner = nnbench.BenchmarkRunner()\nrecord = runner.run(\"benchmarks.py\", params={\"a\": 1, \"b\": 1}, context=(GitEnvironmentInfo(),))\n\nfile_reporter = FileReporter()\nfile_reporter.write(record, \"record.json\", driver=\"ndjson\")\n</code></pre> <p>This writes a newline-delimited JSON file as <code>record.json</code> into the current directory. We choose this format because it is ideal for duckdb to work with.</p> <p>Now, we can easily ingest the record into a duckDB database:</p> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n    SELECT name, value FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n\n# ----- prints: -----\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  name   \u2502 value \u2502\n# \u2502 varchar \u2502 int64 \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 prod    \u2502     1 \u2502\n# \u2502 sum     \u2502     2 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/duckdb/#querying-metadata-directly-in-sql-by-flattening-the-context-struct","title":"Querying metadata directly in SQL by flattening the context struct","text":"<p>By default, the benchmark context struct, which holds additional information about the benchmark runs, is inlined into the raw dictionary before saving it to a file. This is not ideal for some SQL implementations, where you might not be able to filter records easily by interpreting the serialized <code>context</code> struct.</p> <p>To improve, you can pass <code>ctxmode=\"flatten\"</code> to the <code>FileIO.write()</code> method to flatten the context and inline all nested values instead. This comes at the expense of an inflated schema, i.e. more columns in the database.</p> <pre><code>fio = FileIO()\nfio.write(record, \"record.json\", driver=\"ndjson\", ctxmode=\"flatten\")\n</code></pre> <p>In the example above, we used the <code>GitEnvironmentInfo</code> context provider to log some information on the git environment we ran our benchmarks in. In flat mode, this includes the <code>git.commit</code> and <code>git.repository</code> values, telling us at which commit and in which repository the benchmarks were run, respectively.</p> <p>To log this information in a duckDB view, we run the following on a flat-context NDJSON record:</p> <pre><code>duckdb.sql(\n    \"\"\"\n    SELECT name, value, \\\"git.commit\\\", \\\"git.repository\\\" FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n\n#   ---------------------------------- prints ------------------------------------------\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  name   \u2502 value \u2502                git.commit                \u2502    git.repository     \u2502\n# \u2502 varchar \u2502 int64 \u2502                 varchar                  \u2502        varchar        \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 prod    \u2502     1 \u2502 0d47d7bcd2d2c13b69796355fe9d4ef5f50b1edb \u2502 aai-institute/nnbench \u2502\n# \u2502 sum     \u2502     2 \u2502 0d47d7bcd2d2c13b69796355fe9d4ef5f50b1edb \u2502 aai-institute/nnbench \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This method is great to select a subset of context values when the full context metadata structure is not required.</p> <p>Tip</p> <p>In duckDB specifically, this is equivalent to dotted access of the \"context\" column if <code>ctxmode=\"inline\"</code>. This means that the following also works to obtain the git commit mentioned above: <pre><code>duckdb.sql(\n    \"\"\"\n    SELECT name, value, context.git.commit AS \\\"git.commit\\\" FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n</code></pre></p>"},{"location":"tutorials/huggingface/","title":"Benchmarking HuggingFace models on a dataset","text":"<p>There is a high likelihood that you, at some point, find yourself wanting to benchmark previously trained models. This guide shows you how to do it for a HuggingFace model with nnbench.</p>"},{"location":"tutorials/huggingface/#example-named-entity-recognition","title":"Example: Named Entity Recognition","text":"<p>We start with a small tangent about the example setup that we will use in this guide. If you are only interested in the application of nnbench, you can skip this section.</p> <p>There are lots of reasons why you could want to retrieve saved models for benchmarking.  Among them these are reviewing the work of colleagues, comparing model performance to an existing benchmark, or dealing with models that require significant compute such that in-place retraining is impractical.</p> <p>For this example, we look at a named entity recognition (NER) model that is based on the pre-trained encoder-decoder transformer BERT from HuggingFace. The model is trained on the CoNLLpp dataset which consists of sentences from news stories where words were tagged with Person, Organization, Location, or Miscellaneous if they referred to entities.  Words are assigned an out-of-entity label if they do not represent an entity.</p>"},{"location":"tutorials/huggingface/#model-training","title":"Model Training","text":"<p>You find the code to train the model in the nnbench repository. If you want to skip running the training script but still want to reproduce this example, you can take any BERT model fine tuned for NER with the CoNLL dataset family. You find many on the Huggingface model hub, for example this one. You need to download the <code>model.safetensors</code>, <code>config.json</code>, <code>tokenizer_config.json</code>, and <code>tokenizer.json</code> files. If you want to train your own model, continue below.</p> <p>There is some necessary preprocessing and data wrangling to train the model.  We will not go into the details here, but if you are interested in a more thorough walkthrough, look into this resource by Huggingface which served as the basis for this example.</p> <p>It is not feasible to train the model on a CPU. If you do not have access to a GPU, you can use free GPU instances on Google Colab. When opening a new Colab notebook, make sure to select a GPU instance in the upper right corner. Then, you can upload the <code>training.py</code>. You can ignore any warnings about the data not being persisted.</p> <p>Next, install the necessary dependencies: <code>!pip install datasets transformers[torch]</code>.  Google Colab comes with some dependencies already installed in the environment. Hence, if you are working with a different GPU instance, make sure to install everything from the <code>pyproject.toml</code> in the <code>examples/artifact_benchmarking</code> folder.</p> <p>Finally, you can execute the <code>training.py</code> with <code>!python training.py</code>. This will train two BERT models (\"bert-base-uncased\" and \"distilbert-base-uncased\") which we can compare using nnbench.  If you want, you can adapt the training script to train other models by editing the tuples in the <code>tokenizers_and_models</code> list at the bottom of the training script.  The training of the models takes around 10 minutes.</p> <p>Once it is done, download the respective files and save them to your disk. They should be the same mentioned above. We will need the paths to the files for benchmarking later.</p>"},{"location":"tutorials/huggingface/#the-benchmarks","title":"The benchmarks","text":"<p>The benchmarking code is found in the <code>examples/huggingface/benchmark.py</code>. We calculate precision, recall, accuracy, and f1 scores for the whole test set and specific labels. Additionally, we obtain information about the model such as its memory footprint and inference time.</p> <p>We are not walking through the whole file but instead point out certain design choices as an inspiration to you.  If you are interested in a more detailed walkthrough on how to set up benchmarks, you can find it here.</p> <p>Notable design choices in this benchmark are that we factored out the evaluation loop as it is necessary for all evaluation metrics. We cache it using the <code>functools.cache</code> decorator so the evaluation loop runs only once per benchmark run instead of once per metric which greatly reduces runtime.</p> <p>We also use <code>nnbench.parametrize</code> to get the per-class metrics. As the parametrization method needs the same arguments for each benchmark, we use Python's builtin <code>functools.partial</code> to fill the arguments.</p> <pre><code>parametrize_label = partial(\n    nnbench.product,\n    model=[TokenClassificationModelMemo(\"dslim/distilbert-NER\")],\n    tokenizer=[TokenizerMemo(\"dslim/distilbert-NER\")],\n    valdata=[ConllValidationMemo(path=\"conllpp\", split=\"validation\")],\n    index_label_mapping=[IndexLabelMapMemo(path=\"conllpp\", split=\"validation\")],\n    class_label=[\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"],\n    tags=(\"metric\", \"per-class\"),\n)()\n</code></pre> <p>Tip</p> <p>In this parametrization, the model path is hardcoded to \"dslim/distilbert-NER\" on the HuggingFace hub. When benchmarking other models, be sure to change this path to the actual model you want to benchmark.</p> <p>After this, the benchmarking code is actually very simple, as in most of the other examples. You find it in the nnbench repository in <code>examples/huggingface/runner.py</code>.</p>"},{"location":"tutorials/huggingface/#custom-memo-classes","title":"Custom memo classes","text":"<p>The parametrization contains a list of models, which are each instances of a <code>TokenClassificationModelMemo</code> a custom class we implemented which inherits from the <code>nnbench.Memo</code> class. A big advantage of a memo in this case is its ability to lazy-load models and later evict the loaded models again from a cache.</p> <pre><code>class TokenClassificationModelMemo(Memo[Module]):\n    def __init__(self, path: str):\n        self.path = path\n\n    @cached_memo\n    def __call__(self) -&gt; Module:\n        model: Module = AutoModelForTokenClassification.from_pretrained(\n            self.path, use_safetensors=True\n        )\n        return model\n\n    def __str__(self):\n        return self.path\n</code></pre> <p>The <code>Memo</code> class is a generic wrapper around serialized data of any kind. It allows for lazy deserialization of artifacts from uniquely identifying metadata like storage paths, checksums, or model names on HuggingFace Hub in our case. In our derived class, we have to override the <code>Memo.__call__()</code> method to properly load the memoized value into memory.</p> <p>We do similar with the CoNLLpp dataset.</p> <pre><code>class ConllValidationMemo(Memo[Dataset]):\n    def __init__(self, path: str, split: str):\n        self.path = path\n        self.split = split\n\n    @cached_memo\n    def __call__(self) -&gt; Dataset:\n        dataset = load_dataset(self.path)\n        path = dataset.cache_files[self.split][0][\"filename\"]\n        dataset = Dataset.from_file(path)\n        return dataset\n\n    def __str__(self):\n        return self.path + \"/\" + self.split\n</code></pre> <p>In this case, we lazy-load the <code>datasets.Dataset</code> object. In the following <code>IndexLabelMapMemo</code> class, we store a dictionary mapping the label ID to a semantic string.</p> <pre><code>class IndexLabelMapMemo(Memo[dict[int, str]]):\n    def __init__(self, path: str, split: str):\n        self.path = path\n        self.split = split\n\n    @cached_memo\n    def __call__(self) -&gt; dict[int, str]:\n        dataset = load_dataset(self.path)\n        path = dataset.cache_files[self.split][0][\"filename\"]\n        dataset = Dataset.from_file(path)\n        label_names: Sequence[str] = dataset.features[\"ner_tags\"].feature.names\n        id2label = {i: label for i, label in enumerate(label_names)}\n        return id2label\n\n    def __str__(self):\n        return self.path + \"/\" + self.split\n</code></pre> <p>Info</p> <p>There is no need to type-hint <code>TokenClassificationModelMemo</code>s in the corresponding benchmarks - the benchmark runner takes care of filling in the memoized values for the memos themselves.</p> <p>Because we implemented our memoized values as four different memo class types, this modularizes the benchmark input parameters - we only need to reference memos when they are actually used. Considering the recall benchmarks:</p> <pre><code>@nnbench.benchmark(tags=(\"metric\", \"aggregate\"))\ndef recall(\n    model: Module,\n    tokenizer: PreTrainedTokenizerBase,\n    valdata: Dataset,\n    padding_id: int = -100,\n) -&gt; float:\n    dataloader = make_dataloader(tokenizer, valdata)\n\n    tp, fp, tn, fn = run_eval_loop(model, dataloader, padding_id)\n    recall = tp / (tp + fn + 1e-6)\n    return torch.mean(recall).item()\n\n\n@parametrize_label\ndef recall_per_class(\n    class_label: str,\n    model: Module,\n    tokenizer: PreTrainedTokenizerBase,\n    valdata: Dataset,\n    index_label_mapping: dict[int, str],\n    padding_id: int = -100,\n) -&gt; float:\n    dataloader = make_dataloader(tokenizer, valdata)\n\n    tp, fp, tn, fn = run_eval_loop(model, dataloader, padding_id)\n    recall_values = tp / (tp + fn + 1e-6)\n    for idx, lbl in index_label_mapping.items():\n        if lbl == class_label:\n            return recall_values[idx]\n    raise ValueError(f\" Key {class_label} not in test labels\")\n</code></pre> <p>we see that the memoized <code>index_label_mapping</code> argument is only necessary in the per-class benchmark, so it is never passed to the main computation.</p> <p>Tip</p> <p>When implementing memos for a benchmark workload, using only one value per memo at the cost of another class definition is often worth it, since you have more direct control over what goes into your benchmarks, and you can avoid having unused parameters altogether with this approach.</p>"},{"location":"tutorials/mnist/","title":"Integrating nnbench into an existing ML pipeline","text":"<p>Thanks to nnbench's modularity, we can easily integrate it into existing ML experiment code.</p> <p>As an example, we use an MNIST pipeline written for the popular ML framework JAX. While the actual data sourcing and training code is interesting on its own, we focus solely on the nnbench application part. You can find the full example code in the nnbench repository.</p>"},{"location":"tutorials/mnist/#defining-and-organizing-benchmarks","title":"Defining and organizing benchmarks","text":"<p>To properly structure our project, we avoid mixing training pipeline code and benchmark code by placing all benchmarks in a standalone file, similarly to how you might structure unit tests for your code.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom mnist import ArrayMapping, ConvNet\n\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(params: ArrayMapping, data: ArrayMapping) -&gt; float:\n    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n    cn = ConvNet()\n    y_pred = cn.apply({\"params\": params}, x_test)\n    return jnp.mean(jnp.argmax(y_pred, -1) == y_test).item()\n\n\n@nnbench.benchmark(name=\"Model size (MB)\")\ndef modelsize(params: ArrayMapping) -&gt; float:\n    nbytes = sum(x.size * x.dtype.itemsize for x in jax.tree_util.tree_leaves(params))\n    return nbytes / 1e6\n</code></pre> <p>This definition is short and sweet, and contains a few important details:</p> <ul> <li>Both functions are given the <code>@nnbench.benchmark</code> decorator - this enables our runner to find and collect them before starting the benchmark run.</li> <li>The <code>modelsize</code> benchmark is given a custom name (<code>\"Model size (MB)\"</code>), indicating that the resulting number is the combined size of the model weights in megabytes. This is done for display purposes, to improve interpretability when reporting results.</li> <li>The <code>params</code> argument is the same in both benchmarks, both in name and type. This is important, since it ensures that both benchmarks will be run with the same model weights.</li> </ul> <p>That's all - now we can shift over to our main pipeline code and see what is necessary to execute the benchmarks and visualize the results.</p>"},{"location":"tutorials/mnist/#setting-up-a-benchmark-runner-and-parameters","title":"Setting up a benchmark runner and parameters","text":"<p>After finishing the benchmark setup, we only need a few more lines to augment our pipeline.</p> <p>We assume that the benchmark file is located in the same folder as the training pipeline - thus, we can specify our parent directory as the place in which to search for benchmarks:</p> <p>Next, we can define a custom subclass of <code>nnbench.Parameters</code> to hold our benchmark parameters. Benchmark parameters are a set of variables used as inputs to the benchmark functions collected during the benchmark run.</p> <p>Since our benchmarks above are parametrized by the model weights (named <code>params</code> in the function signatures) and the MNIST data split (called <code>data</code>), we define our parameters to take exactly these two values.</p> <pre><code>class MNISTTestParameters(nnbench.Parameters):\n    params: Mapping[str, jax.Array]\n    data: ArrayMapping\n</code></pre> <p>And that's it! After we implement all training code, we just run nnbench directly after training in our top-level pipeline function:</p> <pre><code>    \"\"\"Load MNIST data and train a simple ConvNet model.\"\"\"\n    mnist = load_mnist()\n    mnist = preprocess(mnist)\n    state, data = train(mnist)\n\n    # the nnbench portion.\n    runner = nnbench.BenchmarkRunner()\n    reporter = nnbench.reporter.FileReporter()\n    params = MNISTTestParameters(params=state.params, data=data)\n    result = runner.run(HERE, params=params)\n    reporter.write(result, \"result.json\")\n</code></pre> <p>We use the <code>BenchmarkReporter</code> to print the results directly to the terminal in a table. Notice how by we can reuse the training artifacts in nnbench as parameters to obtain results right after training!</p> <p>The output might look like this:</p> <pre><code>name               value\n---------------  -------\naccuracy         0.9712\nModel size (MB)  3.29783\n</code></pre> <p>This can be improved in a number of ways - for example by enriching it with metadata about the model architecture, the used GPU, etc. For more information on how to supply context to benchmarks, check the user guide section.</p>"},{"location":"tutorials/prefect/","title":"Integrating nnbench with Prefect","text":"<p>If you have more complex workflows it is sensible to use a workflow orchestration tool to manage them.  Benchmarking with nnbench can be integrated with orchestrators. We will present an example integration with Prefect. We will explain the orchestration concepts in a high level and link to the corresponding parts of the  Prefect docs. The full example code can be found in the nnbench repository.</p> <p>In this example we want to orchestrate the training and benchmarking of a linear regression model.</p>"},{"location":"tutorials/prefect/#project-structure","title":"Project Structure","text":""},{"location":"tutorials/prefect/#defining-the-training-tasks-and-workflows","title":"Defining the training tasks and workflows","text":"<p>We recommend to separate the training and benchmarking logic. </p> <pre><code>import numpy as np\nfrom prefect import flow, task\nfrom sklearn import base\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\n@task\ndef make_regression_data(\n    random_state: int, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    X, y = make_regression(\n        n_samples=n_samples, n_features=n_features, noise=noise, random_state=random_state\n    )\n    return X, y\n\n\n@task\ndef make_train_test_split(\n    X: np.ndarray, y: np.ndarray, random_state: int, test_size: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    return X_train, y_train, X_test, y_test\n\n\n@task\ndef train_linear_regression(X: np.ndarray, y: np.ndarray) -&gt; base.BaseEstimator:\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\n\n@flow\ndef prepare_regression_data(\n    random_state: int = 42, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, ...]:\n    X, y = make_regression_data(\n        random_state=random_state, n_samples=n_samples, n_features=n_features, noise=noise\n    )\n    X_train, y_train, X_test, y_test = make_train_test_split(X=X, y=y, random_state=random_state)\n    return X_train, y_train, X_test, y_test\n\n\n@flow\nasync def prepare_regressor_and_test_data(\n    data_params: dict[str, int | float] | None = None,\n) -&gt; tuple[base.BaseEstimator, np.ndarray, np.ndarray]:\n    if data_params is None:\n        data_params = {}\n    X_train, y_train, X_test, y_test = prepare_regression_data(**data_params)\n    model = train_linear_regression(X=X_train, y=y_train)\n    return model, X_test, y_test\n</code></pre> <p>The <code>training.py</code> file contains functions to generate synthetic data for our regression model, facilitate a train-test-split, and finally train the regression model. We have applied Prefect's <code>@task</code> decorator.. which marks the contained logic as a discrete unit of work for Prefect.  Two other functions prepare the regression data and train the estimator.  They are labeled with the <code>@flow</code> decorator. that labels the function as a workflow that can depend on other flows or tasks. The <code>prepare_regressor_and_test_data</code> function returns the model and test data so that we can use it in our benchmarks.</p>"},{"location":"tutorials/prefect/#defining-benchmarks","title":"Defining Benchmarks","text":"<p>The benchmarks are in the <code>benchmark.py</code> file. We have two functions to calculate the mean absolute error and the mean squared error. These benchmarks are tagged to indicate they are metrics. Another two benchmarks calculate calculate information about the model, namely the inference time and size of the model. The last two functions serve to investigate the test dataset.</p> <pre><code>import pickle\nimport sys\nimport time\n\nimport numpy as np\nfrom sklearn import base, metrics\n\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mae(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_absolute_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mse(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_squared_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(name=\"Model size (bytes)\", tags=(\"model-meta\",))\ndef modelsize(model: base.BaseEstimator) -&gt; int:\n    model_bytes = pickle.dumps(model)\n    return sys.getsizeof(model_bytes)\n\n\n@nnbench.benchmark(name=\"Inference time (s)\", tags=(\"model-meta\",))\ndef inference_time(model: base.BaseEstimator, X: np.ndarray, n_iter: int = 100) -&gt; float:\n    start = time.perf_counter()\n    for i in range(n_iter):\n        _ = model.predict(X)\n    end = time.perf_counter()\n    return (end - start) / n_iter\n</code></pre> <p>We did not apply any Prefect decorators here, as we will assign <code>@task</code>s - Prefects smallest unit of work - to run a benchmark family.</p>"},{"location":"tutorials/prefect/#defining-benchmark-runners","title":"Defining Benchmark runners.","text":"<p>In the <code>runners.py</code> file, we define the logic to run our benchmarks. The runner collects the benchmarks from the specified file.  We can filter by tags and use this to define two separate tasks, one to run the metrics and the other to run the metadata benchmarks. We have applied the <code>@task</code> decorator to these functions.</p> <pre><code>    model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray\n) -&gt; nnbench.types.BenchmarkRecord:\n    runner = nnbench.BenchmarkRunner()\n    results = runner.run(\n        os.path.join(dir_path, \"benchmark.py\"),\n        tags=(\"metric\",),\n        params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test},\n    )\n    return results\n\n\n@task\ndef run_metadata_benchmarks(\n    model: base.BaseEstimator, X: np.ndarray\n) -&gt; nnbench.types.BenchmarkRecord:\n    runner = nnbench.BenchmarkRunner()\n    result = runner.run(\n        os.path.join(dir_path, \"benchmark.py\"),\n        tags=(\"model-meta\",),\n        params={\"model\": model, \"X\": X},\n    )\n    return result\n</code></pre> <p>We have also defined a basic reporter that we will use to save the benchmark results with Prefect's artifact storage machinery.</p> <p><pre><code>        self.logger = get_run_logger()\n\n    async def write(\n        self, record: types.BenchmarkRecord, key: str, description: str = \"Benchmark and Context\"\n    ) -&gt; None:\n        await create_table_artifact(\n            key=key,\n            table=record.to_json(),\n            description=description,\n        )\n</code></pre> In a real-world scenario, we would report to a database and use a dedicated frontend to look at the benchmark results. But logging will suffice as we are only discussing integration with orchestrators here.</p> <p>A final compound flow executes the model training, obtains the test set and supplies it to the benchmarks we defined earlier.</p> <pre><code>    data_params: dict[str, int | float] | None = None,\n) -&gt; tuple[types.BenchmarkRecord, ...]:\n    if data_params is None:\n        data_params = {}\n\n    reporter = PrefectReporter()\n\n    regressor_and_test_data: tuple[\n        base.BaseEstimator, np.ndarray, np.ndarray\n    ] = await training.prepare_regressor_and_test_data(data_params=data_params)  # type: ignore\n\n    model = regressor_and_test_data[0]\n    X_test = regressor_and_test_data[1]\n    y_test = regressor_and_test_data[2]\n\n    metadata_results: types.BenchmarkRecord = run_metadata_benchmarks(model=model, X=X_test)\n\n    metadata_results.context.update(data_params)\n    metadata_results.context.update(context.PythonInfo())\n\n    await reporter.write(\n        record=metadata_results, key=\"model-attributes\", description=\"Model Attributes\"\n    )\n\n    metric_results: types.BenchmarkRecord = run_metric_benchmarks(\n        model=model, X_test=X_test, y_test=y_test\n    )\n\n    metric_results.context.update(data_params)\n    metric_results.context.update(context.PythonInfo())\n    await reporter.write(metric_results, key=\"model-performance\", description=\"Model Performance\")\n    return metadata_results, metric_results\n</code></pre> <p>The final lines in the <code>runner.py</code> serve the <code>train_and_benchmark</code> function to make it available to Prefect for execution.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/prefect/#running-prefect","title":"Running Prefect","text":"<p>To run Prefect we have to do several things. First, we have to make sure it is installed. You can use <code>pip install -U prefect</code>. Then we have to run a Prefect server using <code>prefect server start</code>. We make our benchmark flows available by executing it, <code>python runner.py</code>. This enables us to now order an execution with the following command: <code>prefect deployment run 'train-and-benchmark/benchmark-runner'</code>. The command should also be displayed in the output of the <code>runner.py</code> execution.</p> <p>Now we can visit the local Prefect dashboard. By default it is on <code>localhost:4200</code>.  Here we see the executed tasks and workflows.</p> <p></p> <p>If we navigate to the \"Flow Runs\" tab we see more details of the flow runs.</p> <p></p> <p>In the \"Deployments\" tab you see all deployed flows. Currently, there is only our <code>train_and_benchmark</code> flow under the <code>benchmark-runner</code> name. We can trigger a custom execution of workflows in the menu behind the three dots.</p> <p></p> <p>You find the results of the benchmarks when visiting the \"Artifacts\" tab or by navigating to the \"Artifacts\" section of a specific flow execution.</p> <p>As you can see, the nnbench is easily integrated with workflow orchestrators by simply registering the execution of a benchmark runner as a task in the orchestrator.</p> <p>For more functionality of Prefect, you can check out their documentation. </p>"},{"location":"tutorials/streamlit/","title":"Streamlit","text":""},{"location":"tutorials/streamlit/#integrating-nnbench-with-streamlit-and-prefect","title":"Integrating nnbench with Streamlit and Prefect","text":"<p>In a project you may want to execute benchmarks or investigate their results with a dedicated frontend. There exist several frameworks that can help you setting up a frontend quickly. For example Streamlit, Gradio, Dash, or you could roll your own implementation using a backend framework such as Flask. In this guide we will use Streamlit and integrate it with the orchestration setup we've developed with Prefect. That guide is a prerequisite for this one.  The full example code can be found in the nnbench repository.</p>"},{"location":"tutorials/streamlit/#the-streamlit-ui","title":"The Streamlit UI","text":"<p>The Streamlit UI is launched by executing  <pre><code>streamlit run streamlit_example.py\n</code></pre> and initially looks like this: </p> <p>The user interface is assembled in the final part of <code>streamlit_example.py</code>.</p> <pre><code>\n</code></pre> <p>The user inputs are generated via the custom <code>setup_ui()</code> function which then processes the input values once the \"Run Benchmark\" button is clicked.</p> <pre><code>\n</code></pre> <p>We use a session state to keep track of all the benchmarks we ran in the current session which then are displayed within expander elements at the bottom.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#integrating-prefect-with-streamlit","title":"Integrating Prefect with Streamlit","text":"<p>To integrate Streamlit with Prefect, we have to do some initial housekeeping. Namely, we specify the URL for the <code>PreFectClient</code> as well as the storage location of run artifacts where we retrieve the benchmark results from.</p> <pre><code>\n</code></pre> <p>In this example there is no direct integration of Streamlit with nnbench, but all interactions are passing through Prefect to make use of its orchestration benefits such as caching of tasks. Another thing to note is that we are working with local instances for easier reproducibility of this example. Adapting it to work with a remote orchestration server and object storage should be straightforward.</p> <p>The main interaction of the Streamlit frontend with Prefect takes place in the <code>run_bms</code> and <code>get_bm_artifacts</code> functions.</p> <p>The former searches for a Prefect deployment <code>\"train-and-benchmark/benchmark-runner\"</code> and executes it with the benchmark parameters specified by the user. It returns the <code>storage_key</code>, which we use to retrieve the persisted benchmark results.</p> <pre><code>\n</code></pre> <p>The <code>get_bm_artifacts</code> function gets a storage key and retrieves the corresponding results. As the results are stored in raw bytes, we have some logic to reconstruct the <code>nnbench.types.BenchmarkRecord</code> object. We transform the data into Pandas <code>DataFrame</code>s, which are later processed by Streamlit to display the results in tables.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#running-the-example","title":"Running the example","text":"<p>To run the example, we have to do several things.  First, we need to start Prefect using <code>prefect server start</code> in the command line.  Next, we need to make the <code>\"train-and-benchmark/benchmark-runner\"</code> deployment available. We do so by running the corresponding Python file, <code>python runner.py</code>. You find that file in the <code>examples/prefect/src</code> directory.  If you are recreating this example on your machine, make sure you have the full contents of the <code>prefect</code> directory available in addition to the <code>streamlit_example.py</code>. For more information, you can look into the Prefect Guide.</p> <p>Now that Prefect is set up, you can launch a local instance of Streamlit with <code>streamlit run streamlit_example.py</code>.</p> <p>For more information on how to work with Streamlit, visit their docs.</p>"}]}